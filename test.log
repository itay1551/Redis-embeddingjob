[
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 0
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\nGetting started with Red Hat OpenShift AI\nSelf-Managed\nLearn how to work in an OpenShift AI environment\nLast Updated: 2024-04-05",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 1
      },
      "page_content": "",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 2
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\n \nGetting started with Red Hat\nOpenShift AI Self-Managed\nLearn how to work in an OpenShift AI environment",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 3
      },
      "page_content": "Legal Notice\nCopyright \n©\n 2024 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nLearn how to work in an OpenShift AI environment.",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 4
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. LOGGING IN TO OPENSHIFT AI\nCHAPTER 2. THE OPENSHIFT AI USER INTERFACE\n2.1. GLOBAL NAVIGATION\n2.2. SIDE NAVIGATION\nCHAPTER 3. NOTIFICATIONS IN OPENSHIFT AI\nCHAPTER 4. CREATING A DATA SCIENCE PROJECT\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n5.1. LAUNCHING JUPYTER AND STARTING A NOTEBOOK SERVER\n5.2. OPTIONS FOR NOTEBOOK SERVER ENVIRONMENTS\nCHAPTER 6. TUTORIALS FOR DATA SCIENTISTS\n6.1. ACCESSING TUTORIALS\nCHAPTER 7. CONFIGURING YOUR IDE\n7.1. CONFIGURING YOUR CODE-SERVER WORKBENCH\n7.1.1. Installing extensions with code-server\n7.1.1.1. Extensions\nCHAPTER 8. ENABLING SERVICES CONNECTED TO OPENSHIFT AI\nCHAPTER 9. DISABLING APPLICATIONS CONNECTED TO OPENSHIFT AI\n9.1. REMOVING DISABLED APPLICATIONS FROM OPENSHIFT AI\nCHAPTER 10. SUPPORT REQUIREMENTS AND LIMITATIONS\n10.1. SUPPORTED BROWSERS\n10.2. SUPPORTED SERVICES\n10.3. SUPPORTED PACKAGES\nCHAPTER 11. COMMON QUESTIONS\nCHAPTER 12. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR ADMINISTRATORS\n12.1. A USER RECEIVES A 404: PAGE NOT FOUND ERROR WHEN LOGGING IN TO JUPYTER\n12.2. A USER’S NOTEBOOK SERVER DOES NOT START\n12.3. THE USER RECEIVES A DATABASE OR DISK IS FULL ERROR OR A NO SPACE LEFT ON DEVICE ERROR\nWHEN THEY RUN NOTEBOOK CELLS\nCHAPTER 13. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR USERS\n13.1. I SEE A 403: FORBIDDEN ERROR WHEN I LOG IN TO JUPYTER\n13.2. MY NOTEBOOK SERVER DOES NOT START\n13.3. I SEE A DATABASE OR DISK IS FULL ERROR OR A NO SPACE LEFT ON DEVICE ERROR WHEN I RUN MY\nNOTEBOOK CELLS\n3\n4\n5\n5\n8\n9\n10\n11\n13\n26\n29\n30\n30\n30\n31\n32\n34\n35\n36\n36\n36\n37\n38\n39\n39\n39\n40\n42\n42\n42\n42\nTable of Contents\n1",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 5
      },
      "page_content": "Red Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n2",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 6
      },
      "page_content": "CHAPTER 1. LOGGING IN TO OPENSHIFT AI\nLog in to OpenShift AI from a browser for easy access to Jupyter and your data science projects.\nProcedure\n1\n. \nBrowse to the OpenShift AI instance URL and click \nLog in with OpenShift\n.\nIf you are a data scientist user, your administrator must provide you with the OpenShift AI\ninstance URL, for example, \nhttps://rhods-dashboard-redhat-oai-\napplications.apps.example.abc1.p1.openshiftapps.com/\nIf you have access to OpenShift Container Platform, you can browse to the OpenShift\nContainer Platform web console and click the \nApplication Launcher\n ( \n \n) \n→\n \nRed Hat\nOpenShift AI\n.\n2\n. \nClick the name of your identity provider, for example, \nGitHub\n.\n3\n. \nEnter your credentials and click \nLog in\n (or equivalent for your identity provider).\nVerification\nOpenShift AI opens on the \nEnabled applications\n page.\nTroubleshooting\nIf you see \nAn authentication error occurred\n or \nCould not create user\n when you try to log in:\nYou might have entered your credentials incorrectly. Confirm that your credentials are\ncorrect.\nYou might have an account in more than one configured identity provider. If you have\nlogged in with a different identity provider previously, try again with that identity provider.\nAdditional resources\nLaunching Jupyter and starting a notebook server\nCHAPTER 1. LOGGING IN TO OPENSHIFT AI\n3",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 7
      },
      "page_content": "CHAPTER 2. THE OPENSHIFT AI USER INTERFACE\nThe Red Hat OpenShift AI interface is based on the OpenShift web console user interface.\nThe Red Hat OpenShift AI user interface is divided into several areas:\nThe global navigation bar, which provides access to useful controls, such as \nHelp\n and\nNotifications\n.\nFigure 2.1. The global navigation bar\nThe side navigation menu, which contains different categories of pages available in OpenShift\nAI.\nFigure 2.2. The side navigation menu\nThe main display area, which displays the current page and shares space with any drawers\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n4",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 8
      },
      "page_content": "currently displaying information, such as notifications or quick start guides. The main display\narea also displays the \nNotebook server control panel\n where you can launch Jupyter by starting\nand configuring a notebook server. Administrators can also use the \nNotebook server control\npanel\n to manage other users' notebook servers.\nFigure 2.3. The main display area\n2.1. GLOBAL NAVIGATION\nThere are four items in the top navigation:\nThe \nToggle side navigation menu\n button ( \n \n) toggles whether or not the side navigation is\ndisplayed.\nThe \nNotifications\n button ( \n \n) opens and closes the \nNotifications\n drawer, letting you read\ncurrent and previous notifications in more detail.\nThe \nHelp\n menu ( \n \n) provides a link to create a ticket with Red Hat Support and access the\nOpenShift AI documentation.\nThe \nUser\n menu displays the name of the currently logged-in user and provides access to the\nLog out\n button.\n2.2. SIDE NAVIGATION\nThere are several different pages in the side navigation:\nApplications \n→\n Enabled\nThe \nEnabled\n page displays applications that are enabled and ready to use on OpenShift AI. This\npage is the default landing page for OpenShift AI.\nClick the \nLaunch application\n button on an application tile to open the application interface in a new\ntab. If an application has an associated quick start tour, click the drop-down menu on the application\ntile and select \nOpen quick start\n to access it. This page also displays applications and components\nCHAPTER 2. THE OPENSHIFT AI USER INTERFACE\n5",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 9
      },
      "page_content": "that have been disabled by your administrator. Disabled applications are denoted with \nDisabled\n on\nthe application tile. Click \nDisabled\n on the application tile to access links allowing you to remove the\ntile itself, and to revalidate its license, if the license had previously expired.\nApplications \n→\n Explore\nThe \nExplore\n page displays applications that are available for use with OpenShift AI. Click a tile for\nmore information about the application or to access the \nEnable\n button. The \nEnable\n button is visible\nonly if an application does not require an OpenShift Operator installation. \nData Science Projects\nThe \nData science projects\n page allows you to organize your data science work into a single project.\nFrom this page, you can create and manage data science projects. You can also enhance the\ncapabilities of your data science project by adding workbenches, adding storage to your project’s\ncluster, adding data connections, and adding model servers.\nData Science Pipelines \n→\n Pipelines\nThe \nPipelines\n page allows you to import, manage, track, and view data science pipelines. Using\nRed Hat OpenShift AI pipelines, you can standardize and automate machine learning workflows to\nenable you to develop and deploy your data science models.\nData Science Pipelines \n→\n Runs\nThe \nRuns\n page allows you to define, manage, and track executions of a data science pipeline. A\npipeline run is a single execution of a data science pipeline. You can also view a record of previously\nexecuted and scheduled runs for your data science project.\nModel Serving\nThe \nModel Serving\n page allows you to manage and view the status of your deployed models. You\ncan use this page to deploy data science models to serve intelligent applications, or to view existing\ndeployed models. You can also determine the inference endpoint of a deployed model.\nResources\nThe \nResources\n page displays learning resources such as documentation, how-to material, and quick\nstart tours. You can filter visible resources using the options displayed on the left, or enter terms into\nthe search bar.\nSettings \n→\n Notebook images\nThe \nNotebook images\n page allows you to configure custom notebook images that cater to your\nproject’s specific requirements. After you have added custom notebook images to your deployment\nof OpenShift AI, they are available for selection when creating a notebook server.\nSettings \n→\n Cluster settings\nThe \nCluster settings\n page allows you to perform the following administrative tasks on your cluster:\nEnable or disable Red Hat’s ability to collect data about OpenShift AI usage on your cluster.\nConfigure how resources are claimed within your cluster by changing the default size of the\ncluster’s persistent volume claim (PVC).\nReduce resource usage in your OpenShift AI deployment by stopping notebook servers that\nhave been idle.\nSchedule notebook pods on tainted nodes by adding tolerations.\nSettings \n→\n Accelerator profiles\nThe \nAccelerator profiles\n page allows you to perform the following administrative tasks on your\naccelerator profiles:\nEnable or disable an existing accelerator profile.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n6",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 10
      },
      "page_content": "Enable or disable an existing accelerator profile.\nCreate, update, or delete accelerator profiles.\nSchedule pods on tainted nodes by adding tolerations.\nSettings \n→\n Serving runtimes\nThe \nServing runtimes\n page allows you to manage the model-serving runtimes in your OpenShift AI\ndeployment. You can use this page to add, edit, and enable or disable model-serving runtimes. You\nspecify a model-serving runtime when you configure a model server on the \nData Science Projects\npage.\nSettings \n→\n User management\nThe \nUser management\n page allows you to define OpenShift AI user group and admin group\nmembership.\nCHAPTER 2. THE OPENSHIFT AI USER INTERFACE\n7",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 11
      },
      "page_content": "CHAPTER 3. NOTIFICATIONS IN OPENSHIFT AI\nRed Hat OpenShift AI displays notifications when important events happen in the cluster.\nIf you miss a notification message, click the \nNotifications\n button ( \n \n) to open the \nNotifications\ndrawer and view unread messages.\nFigure 3.1. The Notifications drawer\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n8",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 12
      },
      "page_content": "CHAPTER 4. CREATING A DATA SCIENCE PROJECT\nTo start your data science work, create a data science project. Creating a project helps you organize your\nwork in one place. You can also enhance your data science project by adding the following functionality:\nWorkbenches\nStorage for your project’s cluster\nData connections\nModel servers\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick \nCreate data science project\n.\nThe \nCreate a data science project\n dialog opens.\n3\n. \nEnter a \nname\n for your data science project.\n4\n. \nOptional: Edit the \nresource name\n for your data science project. The resource name must\nconsist of lowercase alphanumeric characters, \n-\n, and must start and end with an alphanumeric\ncharacter.\n5\n. \nEnter a \ndescription\n for your data science project.\n6\n. \nClick \nCreate\n.\nA project details page opens. From this page, you can create workbenches, add cluster storage\nand data connections, import pipelines, and deploy models.\nVerification\nThe project that you created is displayed on the \nData science projects\n page.\nCHAPTER 4. CREATING A DATA SCIENCE PROJECT\n9",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 13
      },
      "page_content": "CHAPTER 5. CREATING A PROJECT WORKBENCH\nTo examine and work with models in an isolated area, you can create a workbench. You can use this\nworkbench to create a Jupyter notebook from an existing notebook container image to access its\nresources and properties. For data science projects that require data retention, you can add container\nstorage to the workbench you are creating. If you require extra power for use with large datasets, you\ncan assign accelerators to your workbench to optimize performance.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use specialized OpenShift AI groups, you are part of the user group or admin group (for\nexample, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that you can add a workbench to.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to add the workbench to.\nThe \nDetails\n page for the project opens.\n3\n. \nIn the \nWorkbenches\n section, click \nCreate workbench\n.\nThe \nCreate workbench\n page opens.\n4\n. \nConfigure the properties of the workbench you are creating.\na\n. \nIn the \nName\n field, enter a name for your workbench.\nb\n. \nOptional: In the \nDescription\n field, enter a description to define your workbench.\nc\n. \nIn the \nNotebook image\n section, complete the fields to specify the notebook image to use\nwith your workbench.\ni\n. \nFrom the \nImage selection\n list, select a notebook image.\nd\n. \nIn the \nDeployment size\n section, specify the size of your deployment instance.\ni\n. \nFrom the \nContainer size\n list, select a container size for your server.\nii\n. \nOptional: From the \nAccelerator\n list, select an accelerator.\niii\n. \nIf you selected an accelerator in the preceding step, specify the number of accelerators\nto use.\ne\n. \nOptional: Select and specify values for any new \nenvironment variables\n.\na\n. \nConfigure the storage for your OpenShift AI cluster.\ni\n. \nSelect \nCreate new persistent storage\n to create storage that is retained after you log out\nof OpenShift AI. Complete the relevant fields to define the storage.\nii\n. \nSelect \nUse existing persistent storage\n to reuse existing storage and select the storage\nfrom the \nPersistent storage\n list.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n10",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 14
      },
      "page_content": "b\n. \nTo use a data connection, in the \nData connections\n section, select the \nUse a data connection\ncheckbox.\nCreate a new data connection as follows:\ni\n. \nSelect \nCreate new data connection\n.\nii\n. \nIn the \nName\n field, enter a unique name for the data connection.\niii\n. \nIn the \nAccess key\n field, enter the access key ID for the S3-compatible object storage\nprovider.\niv\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object\nstorage account that you specified.\nv\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage bucket.\nvi\n. \nIn the \nRegion\n field, enter the default region of your S3-compatible object storage\naccount.\nvii\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nUse an existing data connection as follows:\ni\n. \nSelect \nUse existing data connection\n.\nii\n. \nFrom the \nData connection\n list, select a data connection that you previously defined.\n1\n. \nClick \nCreate workbench\n.\nVerification\nThe workbench that you created appears on the \nDetails\n page for the project.\nAny cluster storage that you associated with the workbench during the creation process\nappears on the \nDetails\n page for the project.\nThe \nStatus\n column, located in the \nWorkbenches\n section of the \nDetails\n page, displays a status\nof \nStarting\n when the workbench server is starting, and \nRunning\n when the workbench has\nsuccessfully started.\n5.1. LAUNCHING JUPYTER AND STARTING A NOTEBOOK SERVER\nLaunch Jupyter and start a notebook server to start working with your notebooks. If you require extra\npower for use with large datasets, you can assign accelerators to your notebook server to optimize\nperformance.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou know the names and values you want to use for any environment variables in your notebook\nserver environment, for example, \nAWS_SECRET_ACCESS_KEY\n.\nIf you want to work with a large data set, work with your administrator to proactively increase the\nstorage capacity of your notebook server. If applicable, also consider assigning accelerators to\nyour notebook server.\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n11",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 15
      },
      "page_content": "Procedure\n1\n. \nLocate the \nJupyter\n tile on the \nEnabled applications\n page.\n2\n. \nClick \nLaunch application\n.\nIf you see an \nAccess permission needed\n message, you are not in the default user group or the\ndefault administrator group for OpenShift AI. Ask your administrator to add you to the correct\ngroup by using \nAdding users\n.\nIf you have not previously authorized the \njupyter-nb-<username>\n service account to access\nyour account, the \nAuthorize Access\n page appears prompting you to provide authorization.\nInspect the permissions selected by default, and click the \nAllow selected permissions\n button.\nIf you credentials are accepted, the \nNotebook server control panel\n opens displaying the \nStart\na notebook server\n page.\n3\n. \nStart a notebook server.\nThis is not required if you have previously opened Jupyter.\na\n. \nIn the \nNotebook image\n section, select the notebook image to use for your server.\nb\n. \nIf the notebook image contains multiple versions, select the version of the notebook image\nfrom the \nVersions\n section.\nNOTE\nWhen a new version of a notebook image is released, the previous version\nremains available and supported on the cluster. This gives you time to\nmigrate your work to the latest version of the notebook image.\nc\n. \nFrom the \nContainer size\n list, select a suitable container size for your server.\nd\n. \nOptional: From the \nAccelerator\n list, select an accelerator.\ne\n. \nIf you selected an accelerator in the preceding step, specify the number of accelerators to\nuse.\nIMPORTANT\nUsing accelerators is only supported with specific notebook images. For\nGPUs, only the PyTorch, TensorFlow, and CUDA notebook images are\nsupported. For Habana Gaudi devices, only the HabanaAI notebook image is\nsupported. In addition, you can only specify the number of accelerators\nrequired for your notebook server if accelerators are enabled on your cluster.\nTo learn how to enable GPU support, see \nEnabling GPU support in\nOpenShift AI\n.\nf\n. \nOptional: Select and specify values for any new \nEnvironment variables\n.\nThe interface stores these variables so that you only need to enter them once. Example\nvariable names for common environment variables are automatically provided for frequently\nintegrated environments and frameworks, such as Amazon Web Services (AWS).\nIMPORTANT\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n12",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 16
      },
      "page_content": "IMPORTANT\nSelect the \nSecret\n checkbox for variables with sensitive values that must\nremain private, such as passwords.\ng\n. \nOptional: Select the \nStart server in current tab\n checkbox if necessary.\nh\n. \nClick \nStart server\n.\nThe \nStarting server\n progress indicator appears. Click \nExpand event log\n to view additional\ninformation about the server creation process. Depending on the deployment size and\nresources you requested, starting the server can take up to several minutes. Click \nCancel\n to\ncancel the server creation.\nAfter the server starts, you see one of the following behaviors:\nIf you previously selected the \nStart server in current tab\n checkbox, the JupyterLab\ninterface opens in the current tab of your web browser.\nIf you did not previously select the \nStart server in current tab\n checkbox, the \nStarting\nserver\n dialog box prompts you to open the server in a new browser tab or in the current\nbrowser tab.\nThe JupyterLab interface opens according to your selection.\nVerification\nThe JupyterLab interface opens.\nAdditional resources\nOptions for notebook server environments\n.\nTroubleshooting\nIf you see the \"Unable to load notebook server configuration options\" error message, contact\nyour administrator so that they can review the logs associated with your Jupyter pod and\ndetermine further details about the problem.\n5.2. OPTIONS FOR NOTEBOOK SERVER ENVIRONMENTS\nWhen you start Jupyter for the first time, or after stopping your notebook server, you must select server\noptions in the \nStart a notebook server\n wizard so that the software and variables that you expect are\navailable on your server. This section explains the options available in the \nStart a notebook server\nwizard in detail.\nThe \nStart a notebook server\n page consists of the following sections:\nNotebook image\nSpecifies the container image that your notebook server is based on. Different notebook images\nhave different packages installed by default. If the notebook image has multiple versions available,\nyou can select the notebook image version to use from the \nVersions\n section.\nNOTE\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n13",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 17
      },
      "page_content": "NOTE\nNotebook images are supported for a minimum of one year. Major updates to\npreconfigured notebook images occur about every six months. Therefore, two\nsupported notebook image versions are typically available at any given time. Legacy\nnotebook image versions, that is, not the two most recent versions, might still be\navailable for selection. Legacy image versions include a label that indicates the image\nis out-of-date.\nFrom OpenShift AI 2.5, version 1.2 of notebook images is no longer supported.\nNotebooks that are already running on version 1.2 of an image will continue to work\nnormally, but it is not available to select for new users or notebooks.\nTo use the latest package versions, Red Hat recommends that you use the most\nrecently added notebook image.\nAfter you start a notebook image, you can check which Python packages are installed on your\nnotebook server and which version of the package you have by running the \npip\n tool in a notebook\ncell.\nThe following table shows the package versions used in the available notebook images.\nIMPORTANT\nNotebook images denoted with \n(Technology Preview)\n in this table are not supported\nwith Red Hat production service level agreements (SLAs) and might not be functionally\ncomplete. Red Hat does not recommend using Technology Preview features in\nproduction. These features provide early access to upcoming product features, enabling\ncustomers to test functionality and provide feedback during the development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee Technology Preview Features Support Scope.\nTable 5.1. Notebook image options\nImage name\nImage version\nPreinstalled packages\nCUDA\n2023.2 (Recommended)\nCUDA 11.8\nPython 3.9\nJupyterLab 3.6\nNotebook 6.5\n2023.1\nCUDA 11.8\nPython 3.9\nJupyterLab 3.5\nNotebook 6.5\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n14",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 18
      },
      "page_content": "1.2\nCUDA 11.4\nPython 3.8\nJupyterLab 3.2\nNotebook 6.4\nMinimal Python (default)\n2023.2 (Recommended)\nPython 3.9\nJupyterLab 3.6\nNotebook 6.5\n2023.1\nPython 3.9\nJupyterLab 3.5\nNotebook 6.5\n1.2\nPython 3.8\nJupyterLab 3.2\nNotebook 6.4\nImage name\nImage version\nPreinstalled packages\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n15",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 19
      },
      "page_content": "PyTorch\n2023.2 (Recommended)\nCUDA 11.8\nPython 3.9\nPyTorch 2.0\nJupyterLab 3.6\nNotebook 6.5\nTensorBoard 2.13\nBoto3 1.28\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.3\nSciPy 1.11\nElyra 3.15\nPyMongo 4.5\nPyodbc 4.0\nCodeflare-SDK 0.12\nSklearn-onnx 1.15\nPsycopg 3.1\nMySQL\nConnector/Python 8.0\nImage name\nImage version\nPreinstalled packages\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n16",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 20
      },
      "page_content": "2023.1\nCUDA 11.8\nPython 3.9\nPyTorch 1.13\nJupyterLab 3.5\nNotebook 6.5\nTensorBoard 2.11\nBoto3 1.26\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.2\nSciPy 1.10\nElyra 3.15\n1.2\nCUDA 11.4\nPython 3.8\nPyTorch 1.8\nJupyterLab 3.2\nNotebook 6.4\nTensorBoard 2.6\nBoto3 1.17\nKafka-Python 2.0\nMatplotlib 3.4\nNumpy 1.19\nPandas 1.2\nScikit-learn 0.24\nSciPy 1.6\nImage name\nImage version\nPreinstalled packages\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n17",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 21
      },
      "page_content": "Standard Data Science\n2023.2 (Recommended)\nPython 3.9\nJupyterLab 3.6\nNotebook 6.5\nBoto3 1.28\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nPandas 1.5\nNumpy 1.24\nScikit-learn 1.3\nSciPy 1.11\nElyra 3.15\nPyMongo 4.5\nPyodbc 4.0\nCodeflare-SDK 0.12\nSklearn-onnx 1.15\nPsycopg 3.1\nMySQL\nConnector/Python 8.0\nImage name\nImage version\nPreinstalled packages\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n18",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 22
      },
      "page_content": "2023.1\nPython 3.9\nJupyterLab 3.5\nNotebook 6.5\nBoto3 1.26\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.2\nSciPy 1.10\nElyra 3.15\n1.2\nPython 3.8\nJupyterLab 3.2\nNotebook 6.4\nBoto3 1.17\nKafka-Python 2.0\nMatplotlib 3.4\nPandas 1.2\nNumpy 1.19\nScikit-learn 0.24\nSciPy 1.6\nImage name\nImage version\nPreinstalled packages\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n19",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 23
      },
      "page_content": "TensorFlow\n2023.2 (Recommended)\nCUDA 11.8\nPython 3.9\nJupyterLab 3.6\nNotebook 6.5\nTensorFlow 2.13\nTensorBoard 2.13\nBoto3 1.28\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.3\nSciPy 1.11\nElyra 3.15\nPyMongo 4.5\nPyodbc 4.0\nCodeflare-SDK 0.12\nSklearn-onnx 1.15\nPsycopg 3.1\nMySQL\nConnector/Python 8.0\nImage name\nImage version\nPreinstalled packages\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n20",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 24
      },
      "page_content": "2023.1\nCUDA 11.8\nPython 3.9\nJupyterLab 3.5\nNotebook 6.5\nTensorFlow 2.11\nTensorBoard 2.11\nBoto3 1.26\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.2\nSciPy 1.10\nElyra 3.15\n1.2\nCUDA 11.4\nPython 3.8\nJupyterLab 3.2\nNotebook 6.4\nTensorFlow 2.7\nTensorBoard 2.6\nBoto3 1.17\nKafka-Python 2.0\nMatplotlib 3.4\nNumpy 1.19\nPandas 1.2\nScikit-learn 0.24\nSciPy 1.6\nImage name\nImage version\nPreinstalled packages\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n21",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 25
      },
      "page_content": "TrustyAI\n2023.2 (Recommended)\nPython 3.9\nJupyterLab 3.6\nNotebook 6.5\nTrustyAI 0.3\nBoto3 1.28\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.3\nSciPy 1.11\nElyra 3.15\nPyMongo 4.5\nPyodbc 4.0\nCodeflare-SDK 0.12\nSklearn-onnx 1.15\nPsycopg 3.1\nMySQL\nConnector/Python 8.0\nImage name\nImage version\nPreinstalled packages\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n22",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 26
      },
      "page_content": "2023.1\nPython 3.9\nJupyterLab 3.5\nNotebook 6.5\nTrustyAI 0.3\nBoto3 1.26\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.24\nPandas 1.5\nScikit-learn 1.2\nSciPy 1.10\nElyra 3.15\nHabanaAI\n2023.2 (Recommended)\nPython 3.8\nHabana 1.10\nJupyterLab 3.5\nTensorFlow 2.12\nBoto3 1.26\nKafka-Python 2.0\nKfp-tekton 1.5\nMatplotlib 3.6\nNumpy 1.23\nPandas 1.5\nScikit-learn 1.2\nSciPy 1.10\nPyTorch 2.0\nElyra 3.15\nImage name\nImage version\nPreinstalled packages\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n23",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 27
      },
      "page_content": "code-server (Technology\nPreview)\n2023.2 (Recommended)\nPython 3.9\nBoto3 1.29\nKafka-Python 2.0\nMatplotlib 3.8\nNumpy 1.26\nPandas 2.1\nPlotly 5.18\nScikit-learn 1.3\nScipy 1.11\nSklearn-onnx 1.15\nIpykernel 6.26\n(code-server plugin)\nPython 2023.14.0\n(code-server plugin)\nJupyter 2023.3.100\nImage name\nImage version\nPreinstalled packages\nDeployment size\nspecifies the compute resources available on your notebook server.\nContainer size\n controls the number of CPUs, the amount of memory, and the minimum and\nmaximum request capacity of the container.\nAccelerators\n specifies the accelerators available on your notebook server.\nNumber of accelerators\n specifies the number of accelerators to use.\nIMPORTANT\nUsing accelerators is only supported with specific notebook images. For GPUs, only\nthe PyTorch, TensorFlow, and CUDA notebook images are supported. For Habana\nGaudi devices, only the HabanaAI notebook image is supported. In addition, you can\nonly specify the number of accelerators required for your notebook server if\naccelerators are enabled on your cluster. To learn how to enable GPU support, see\nEnabling GPU support in OpenShift AI\n.\nEnvironment variables\nSpecifies the name and value of variables to be set on the notebook server. Setting environment\nvariables during server startup means that you do not need to define them in the body of your\nnotebooks, or with the Jupyter command line interface. Some recommended environment variables\nare shown in the table.\nTable 5.2. Recommended environment variables\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n24",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 28
      },
      "page_content": "Environment\nvariable option\nRecommended variable names\nAWS\nAWS_ACCESS_KEY_ID\n specifies your Access Key ID for Amazon Web\nServices.\nAWS_SECRET_ACCESS_KEY\n specifies your Secret access key for the\naccount specified in \nAWS_ACCESS_KEY_ID\n.\nAdditional resources\nLaunching Jupyter and starting a notebook server\nCHAPTER 5. CREATING A PROJECT WORKBENCH\n25",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 29
      },
      "page_content": "CHAPTER 6. TUTORIALS FOR DATA SCIENTISTS\nTo help you get started quickly, you can access learning resources for Red Hat OpenShift AI and its\nsupported applications.\nThe \nOpenShift AI tutorial: Fraud detection example\n provides step-by-step guidance for using RHOAI to\ndevelop and train an example model in Jupyter notebooks, deploy the model, integrate the model into a\nfraud detection application, and refine the model by using automated pipelines.\nAdditonal resources are available on the \nResources\n tab of the Red Hat OpenShift AI user interface.\nTable 6.1. Tutorials\nResource Name\nDescription\nAccelerating scientific workloads in Python with\nNumba\nWatch a video about how to make your Python code\nrun faster.\nBuilding interactive visualizations and dashboards in\nPython\nExplore a variety of data across multiple notebooks\nand learn how to deploy full dashboards and\napplications.\nBuilding machine learning models with scikit-learn\nLearn how to build machine learning models with\nscikit-learn for supervised learning, unsupervised\nlearning, and classification problems.\nBuilding a binary classification model\nTrain a model to predict if a customer is likely to\nsubscribe to a bank promotion.\nChoosing Python tools for data visualization\nUse the PyViz.org website to help you decide on the\nbest open source Python data visualization tools for\nyou.\nExploring Anaconda for data science\nLearn about Anaconda, a freemium open source\ndistribution of the Python and R programming\nlanguages.\nGetting started with Pachyderm concepts\nLearn Pachyderm’s main concepts by creating\npipelines that perform edge detection on a few\nimages.\nGPU Computing in Python with Numba\nLearn how to create GPU accelerated functions\nusing Numba.\nRun a Python notebook to generate results in IBM\nWatson OpenScale\nRun a Python notebook to create, train, and deploy a\nmachine learning model.\nRunning an AutoAI experiment to build a model\nWatch a video about building a binary classification\nmodel for a marketing campaign.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n26",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 30
      },
      "page_content": "Training a regression model in Pachyderm\nLearn how to create a sample housing data\nrepository using a Pachyderm cluster to run\nexperiments, analyze data, and set up regression.\nUsing Dask for parallel data analysis\nAnalyze medium-sized datasets in parallel locally\nusing Dask, a parallel computing library that scales\nthe existing Python ecosystem.\nUsing Jupyter notebooks in Watson Studio\nWatch a video about working with Jupyter notebooks\nin Watson Studio.\nUsing Pandas for data analysis in Python\nLearn how to use pandas, a data analysis library for\nthe Python programming language.\nResource Name\nDescription\nTable 6.2. Quick start guides\nResource Name\nDescription\nCreating a Jupyter notebook\nCreate a Jupyter notebook in JupyterLab.\nCreating an Anaconda-enabled Jupyter notebook\nCreate an Anaconda-enabled Jupyter notebook and\naccess Anaconda packages that are curated for\nsecurity and compatibility.\nDeploying a model with Watson Studio\nImport a notebook in Watson Studio and use AutoAI\nto build and deploy a model.\nDeploying a sample Python application using Flask\nand OpenShift\nDeploy your data science model out of a Jupyter\nnotebook and into a Flask application to use as a\ndevelopment sandbox.\nImporting Pachyderm Beginner Tutorial Notebook\nLoad Pachyderm’s beginner tutorial notebook and\nlearn about Pachyderm’s main concepts such as data\nrepositories, pipelines, and using the pachctl CLI\nfrom your cells.\nQuerying data with Starburst Enterprise\nLearn to query data using Starburst Enterprise from\na Jupyter notebook.\nUsing the Intel® oneAPI AI Analytics Toolkit (AI Kit)\nNotebook\nRun a data science notebook sample with the Intel®\noneAPI AI Analytics Toolkit.\nUsing the OpenVINO toolkit\nQuantize an ONNX computer vision model using the\nOpenVINO model optimizer and use the result for\ninference from a notebook.\nCHAPTER 6. TUTORIALS FOR DATA SCIENTISTS\n27",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 31
      },
      "page_content": "Table 6.3. How to guides\nResource Name\nDescription\nHow to choose between notebook runtime\nenvironment options\nExplore available options for configuring your\nnotebook runtime environment.\nHow to clean, shape, and visualize data\nLearn how to clean and shape tabular data using IBM\nWatson Studio data refinery.\nHow to create a connection to access data\nLearn how to create connections to various data\nsources across the platform.\nHow to create a deployment space\nLearn how to create a deployment space for\nmachine learning.\nHow to create a notebook in Watson Studio\nLearn how to create a basic Jupyter notebook in\nWatson Studio.\nHow to create a project in Watson Studio\nLearn how to create an analytics project in Watson\nStudio.\nHow to create a project that integrates with Git\nLearn how to add assets from a Git repository into a\nproject.\nHow to install Python packages on your notebook\nserver\nLearn how to install additional Python packages on\nyour notebook server.\nHow to load data into a Jupyter notebook\nLearn how to integrate data sources into a Jupyter\nnotebook by loading data.\nHow to serve a model using OpenVINO Model Server\nLearn how to deploy optimized models with the\nOpenVINO Model Server using OpenVINO custom\nresources.\nHow to set up Watson OpenScale\nLearn how to track and measure outcomes from\nmodels with OpenScale.\nHow to update notebook server settings\nLearn how to update the settings or the notebook\nimage on your notebook server.\nHow to use data from Amazon S3 buckets\nLearn how to connect to data in S3 Storage using\nenvironment variables.\nHow to view installed packages on your notebook\nserver\nLearn how to see which packages are installed on\nyour running notebook server.\nInstallation Requirements for Starburst Enterprise\nExplore hardware and software requirements for\ninstalling Starburst Enterprise on Kubernetes.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n28",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 32
      },
      "page_content": "Overview of Starburst Enterprise on OpenShift\nExplore the available options for deploying Starburst\non OpenShift.\nStarburst Enterprise Deployment Guide for\nOpenShift\nLearn how to deploy Starburst Enterprise on\nOpenShift.\nResource Name\nDescription\n6.1. ACCESSING TUTORIALS\nYou can access learning resources for Red Hat OpenShift AI and supported applications.\nPrerequisites\nEnsure that you have logged in to Red Hat OpenShift AI.\nYou have logged in to the OpenShift Container Platform web console.\nProcedure\n1\n. \nOn the Red Hat OpenShift AI home page, click \nResources\n.\nThe \nResources\n page opens.\n2\n. \nClick \nAccess tutorial\n on the relevant tile.\nVerification\nYou can view and access the learning resources for Red Hat OpenShift AI and supported\napplications.\nAdditional resources\nCommon questions\n.\nCHAPTER 6. TUTORIALS FOR DATA SCIENTISTS\n29",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 33
      },
      "page_content": "CHAPTER 7. CONFIGURING YOUR IDE\nYou can configure some notebook workbenches to get the most out of your data science work.\n7.1. CONFIGURING YOUR CODE-SERVER WORKBENCH\nYou can use extensions to streamline your workflow, add new languages, themes, debuggers, and\nconnect to additional services.\nFor more information on code-server, see \ncode-server in GitHub\n.\nIMPORTANT\nThe code-server notebook image is currently available in Red Hat OpenShift AI 2.8 as a\nTechnology Preview feature. Technology Preview features are not supported with\nRed Hat production service level agreements (SLAs) and might not be functionally\ncomplete. Red Hat does not recommend using them in production. These features\nprovide early access to upcoming product features, enabling customers to test\nfunctionality and provide feedback during the development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee \nTechnology Preview Features Support Scope\n.\n7.1.1. Installing extensions with code-server\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use specialized OpenShift AI groups, you are part of the user group or admin group (for\nexample, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project that has a code-server workbench.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project containing the code-server workbench you want to start.\nThe \nDetails\n page for the project opens.\n3\n. \nClick the toggle in the \nStatus\n column for the relevant workbench to start a workbench that is\nnot running.\nThe status of the workbench that you started changes from \nStopped\n to \nRunning\n.\n4\n. \nAfter the workbench has started, click \nOpen\n to open the workbench notebook.\n5\n. \nIn the Activity Bar, click the Extensions icon. ( \n \n)\n6\n. \nSearch for the name of the extension you want to install.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n30",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 34
      },
      "page_content": "7\n. \nClick \nInstall\n to add the extension to your code-server environment.\nThe extension you installed appears in the \nBrowser - Installed\n list on the \nExtensions\n panel.\n7.1.1.1. Extensions\nSee \nOpen VSX Registry\n for available third-party extensions that you can consider installing.\nCHAPTER 7. CONFIGURING YOUR IDE\n31",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 35
      },
      "page_content": "CHAPTER 8. ENABLING SERVICES CONNECTED TO\nOPENSHIFT AI\nYou must enable SaaS-based services, such as Anaconda Professional Edition, before using them with\nRed Hat OpenShift AI. On-cluster services are enabled automatically.\nTypically, you can install services, or enable services connected to OpenShift AI using one of the\nfollowing methods:\nEnabling the service from the \nExplore\n page on the OpenShift AI dashboard, as documented in\nthe following procedure.\nInstalling the Operator for the service from OperatorHub. OperatorHub is a web console for\ncluster administrators to discover and select Operators to install on their cluster. It is deployed\nby default in OpenShift Container Platform (\nInstalling from OperatorHub using the web\nconsole\n).\nNOTE\nDeployments containing Operators installed from OperatorHub may not be fully\nsupported by Red Hat.\nInstalling the Operator for the service from Red Hat Marketplace (\nInstall Operators\n).\nInstalling the service as an Operator to your OpenShift Container Platform cluster (\nAdding\nOperators to a cluster\n).\nFor some services (such as Jupyter), the service endpoint is available on the tile for the service on the\nEnabled\n page of OpenShift AI. Certain services cannot be accessed directly from their tiles, for\nexample, OpenVINO and Anaconda provide notebook images for use in Jupyter and do not provide an\nendpoint link from their tile. Additionally, it may be useful to store these endpoint URLs as environment\nvariables for easy reference in a notebook environment.\nSome independent software vendor (ISV) applications must be installed in specific namespaces. In these\ncases, the tile for the application in the OpenShift AI dashboard specifies the required namespace.\nTo help you get started quickly, you can access the service’s learning resources and documentation on\nthe \nResources\n page, or by clicking the relevant link on the tile for the service on the \nEnabled\n page.\nPrerequisites\nYou have logged in to OpenShift AI.\nYour administrator has installed or configured the service on your OpenShift Container Platform\ncluster.\nProcedure\n1\n. \nOn the OpenShift AI home page, click \nExplore\n.\nThe \nExplore\n page opens.\n2\n. \nClick the tile of the service that you want to enable.\n3\n. \nClick \nEnable\n on the drawer for the service.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n32",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 36
      },
      "page_content": "4\n. \nIf prompted, enter the service’s key and click \nConnect\n.\n5\n. \nClick \nEnable\n to confirm that you are enabling the service.\nVerification\nThe service that you enabled appears on the \nEnabled\n page.\nThe service endpoint is displayed on the tile for the service on the \nEnabled\n page.\nCHAPTER 8. ENABLING SERVICES CONNECTED TO OPENSHIFT AI\n33",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 37
      },
      "page_content": "CHAPTER 9. DISABLING APPLICATIONS CONNECTED TO\nOPENSHIFT AI\nYou can disable applications and components so that they do not appear on the OpenShift AI\ndashboard when you no longer want to use them, for example, when data scientists no longer use an\napplication or when the application license expires.\nDisabling unused applications allows your data scientists to manually remove these application tiles from\ntheir OpenShift AI dashboard so that they can focus on the applications that they are most likely to use.\nSee \nRemoving disabled applications from OpenShift AI\n for more information about manually removing\napplication tiles.\nIMPORTANT\nDo not follow this procedure when disabling the following applications:\nAnaconda Professional Edition. You cannot manually disable Anaconda\nProfessional Edition. It is automatically disabled only when its license expires.\nPrerequisites\nYou have logged in to the OpenShift Container Platform web console.\nYou are part of the \ncluster-admins\n user group in OpenShift Container Platform.\nYou have installed or configured the service on your OpenShift Container Platform cluster.\nThe application or component that you want to disable is enabled and appears on the \nEnabled\npage.\nProcedure\n1\n. \nIn the OpenShift Container Platform web console, switch to the \nAdministrator\n perspective.\n2\n. \nSwitch to the \nredhat-ods-applications\n project.\n3\n. \nClick \nOperators\n \n→\n \nInstalled Operators\n.\n4\n. \nClick on the Operator that you want to uninstall. You can enter a keyword into the \nFilter by\nname\n field to help you find the Operator faster.\n5\n. \nDelete any Operator resources or instances by using the tabs in the Operator interface.\nDuring installation, some Operators require the administrator to create resources or start\nprocess instances using tabs in the Operator interface. These must be deleted before the\nOperator can uninstall correctly.\n6\n. \nOn the \nOperator Details\n page, click the \nActions\n drop-down menu and select \nUninstall\nOperator\n.\nAn \nUninstall Operator?\n dialog box is displayed.\n7\n. \nSelect \nUninstall\n to uninstall the Operator, Operator deployments, and pods. After this is\ncomplete, the Operator stops running and no longer receives updates.\nIMPORTANT\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n34",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 38
      },
      "page_content": "IMPORTANT\nRemoving an Operator does not remove any custom resource definitions or managed\nresources for the Operator. Custom resource definitions and managed resources still\nexist and must be cleaned up manually. Any applications deployed by your Operator and\nany configured off-cluster resources continue to run and must be cleaned up manually.\nVerification\nThe Operator is uninstalled from its target clusters.\nThe Operator no longer appears on the \nInstalled Operators\n page.\nThe disabled application is no longer available for your data scientists to use, and is marked as \nDisabled\n on the \nEnabled\n page of the OpenShift AI dashboard. This action may take a few\nminutes to occur following the removal of the Operator.\n9.1. REMOVING DISABLED APPLICATIONS FROM OPENSHIFT AI\nAfter your administrator has disabled your unused applications, you can manually remove them from the\nRed Hat OpenShift AI dashboard. Disabling and removing unused applications allows you to focus on the\napplications that you are most likely to use.\nPrerequisites\nEnsure that you have logged in to Red Hat OpenShift AI.\nYou have logged in to the OpenShift Container Platform web console.\nYour administrator has previously disabled the application that you want to remove.\nProcedure\n1\n. \nIn the OpenShift AI interface, click \nEnabled\n.\nThe \nEnabled\n page opens. Disabled applications are denoted with \nDisabled\n on the tile for the\napplication.\n2\n. \nClick \nDisabled\n on the tile for the application that you want to remove.\n3\n. \nClick the link to remove the application tile.\nVerification\nThe tile for the disabled application no longer appears on the \nEnabled\n page.\nCHAPTER 9. DISABLING APPLICATIONS CONNECTED TO OPENSHIFT AI\n35",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 39
      },
      "page_content": "CHAPTER 10. SUPPORT REQUIREMENTS AND LIMITATIONS\nReview this section to understand the requirements for Red Hat support and any limitations to Red Hat\nsupport of Red Hat OpenShift AI.\n10.1. SUPPORTED BROWSERS\nRed Hat OpenShift AI supports the latest version of the following browsers:\nGoogle Chrome\nMozilla Firefox\nSafari\n10.2. SUPPORTED SERVICES\nRed Hat OpenShift AI supports the following services:\nTable 10.1. Supported services\nService Name\nDescription\nAnaconda\nProfessional\nAnaconda Professional is a popular open source package distribution and management\nexperience that is optimized for commercial use.\nIBM Watson\nStudio\nIBM Watson Studio is a platform for embedding AI and machine learning into your business\nand creating custom models with your own data.\nIntel® oneAPI\nAI Analytics\nToolkit\nContainer\nThe AI Kit is a set of AI software tools to accelerate end-to-end data science and analytics\npipelines on Intel® architectures.\nJupyter\nJupyter is a multi-user version of the notebook designed for companies, classrooms, and\nresearch labs.\nIMPORTANT\nWhile every effort is made to make Red Hat OpenShift AI resilient to\nOpenShift node failure, upgrades, and similarly disruptive operations,\nindividual users' notebook environments can be interrupted during these\nevents. If an OpenShift node restarts or becomes unavailable, any user\nnotebook environment on that node is restarted on a different node.\nWhen this occurs, any ongoing process executing in the user’s notebook\nenvironment is interrupted, and the user needs to re-execute it when their\nenvironment becomes available again.\nDue to this limitation, Red Hat recommends that processes for which\ninterruption is unacceptable are not executed in the Jupyter notebook\nserver environment on OpenShift AI.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n36",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 40
      },
      "page_content": "OpenVINO\nOpenVINO is an open source toolkit to help optimize deep learning performance and\ndeploy using an inference engine onto Intel hardware.\nPachyderm\nUse Pachyderm’s data versioning, pipeline and lineage capabilities to automate the\nmachine learning life cycle and optimize machine learning operations.\nNOTE\nThe \npachd\n pod that Pachyderm creates \nintentionally\n does not provide\nterminal access from the OpenShift web console. If you try to access the\nterminal view for the \npachd\n pod, you see an error. This behavior is\nexpected.\nStarburst\nEnterprise\nStarburst Enterprise platform (SEP) is the commercial distribution of Trino, which is an\nopen-source, Massively Parallel Processing (MPP) ANSI SQL query engine. Starburst\nsimplifies data access for your Red Hat OpenShift AI workloads by providing fast access to\nall of your data. Starburst does this by connecting directly to each data source and pulling\nthe data back into memory for processing, alleviating the need to copy or move the data\ninto a single location first.\nService Name\nDescription\n10.3. SUPPORTED PACKAGES\nThe latest supported notebook server images in Red Hat OpenShift AI are installed with Python by\ndefault. See the table in \nOptions for notebook server environments\n for a complete list of packages and\nversions included in these images.\nYou can install packages that are compatible with the supported version of Python on any notebook\nserver that has the binaries required by that package. If the required binaries are not included on the\nnotebook server image you want to use, contact Red Hat Support to request that the binary be\nconsidered for inclusion.\nYou can install packages on a temporary basis by using the \npip install\n command. You can also provide a\nlist of packages to the \npip install\n command using a \nrequirements.txt\n file. See \nInstalling Python\npackages on your notebook server\n for more information.\nYou must re-install these packages each time you start your notebook server.\nYou can remove packages by using the \npip uninstall\n command.\nAdditional resources\nInstalling Python packages on your notebook server\nOptions for notebook server environments\nCHAPTER 10. SUPPORT REQUIREMENTS AND LIMITATIONS\n37",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 41
      },
      "page_content": "CHAPTER 11. COMMON QUESTIONS\nIn addition to documentation, Red Hat provides a rich set of learning resources for OpenShift AI and\nsupported applications.\nOn the \nResources\n page of the OpenShift AI dashboard, you can use the category links to filter the\nresources for various stages of your data science workflow. For example, click the \nModel serving\ncategory to display resources that describe various methods of deploying models. Click \nAll items\n to\nshow the resources for all categories.\nFor the selected category, you can apply additional options to filter the available resources. For\nexample, you can filter by type, such as how-to articles, quick starts, tutorials; these resources provide\nthe answers to common questions.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n38",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 42
      },
      "page_content": "CHAPTER 12. TROUBLESHOOTING COMMON PROBLEMS IN\nJUPYTER FOR ADMINISTRATORS\nIf your users are experiencing errors in Red Hat OpenShift AI relating to Jupyter, their notebooks, or\ntheir notebook server, read this section to understand what could be causing the problem, and how to\nresolve the problem.\nIf you cannot see the problem here or in the release notes, contact Red Hat Support.\n12.1. A USER RECEIVES A \n404: PAGE NOT FOUND\n ERROR WHEN\nLOGGING IN TO JUPYTER\nProblem\nIf you have configured specialized user groups for OpenShift AI, the user name might not be added to\nthe default user group for OpenShift AI.\nDiagnosis\nCheck whether the user is part of the default user group.\n1\n. \nFind the names of groups allowed access to Jupyter.\na\n. \nLog in to the OpenShift Container Platform web console.\nb\n. \nClick \nUser Management\n \n→\n \nGroups\n.\nc\n. \nClick the name of your user group, for example, \nrhoai-users\n.\nThe \nGroup details\n page for that group appears.\n2\n. \nClick the \nDetails\n tab for the group and confirm that the \nUsers\n section for the relevant group\ncontains the users who have permission to access Jupyter.\nResolution\nIf the user is not added to any of the groups with permission access to Jupyter, follow \nAdding\nusers\n to add them.\nIf the user is already added to a group with permission to access Jupyter, contact Red Hat\nSupport.\n12.2. A USER’S NOTEBOOK SERVER DOES NOT START\nProblem\nThe OpenShift Container Platform cluster that hosts the user’s notebook server might not have access\nto enough resources, or the Jupyter pod may have failed.\nDiagnosis\n1\n. \nLog in to the OpenShift Container Platform web console.\n2\n. \nDelete and restart the notebook server pod for this user.\na\n. \nClick \nWorkloads\n \n→\n \nPods\n and set the \nProject\n to \nrhods-notebooks\n.\nb\n. \nSearch for the notebook server pod that belongs to this user, for example, \njupyter-nb-\nCHAPTER 12. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR ADMINISTRATORS\n39",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 43
      },
      "page_content": "b\n. \nSearch for the notebook server pod that belongs to this user, for example, \njupyter-nb-\n<username>-*\n.\nIf the notebook server pod exists, an intermittent failure may have occurred in the notebook\nserver pod.\nIf the notebook server pod for the user does not exist, continue with diagnosis.\n3\n. \nCheck the resources currently available in the OpenShift Container Platform cluster against the\nresources required by the selected notebook server image.\nIf worker nodes with sufficient CPU and RAM are available for scheduling in the cluster, continue\nwith diagnosis.\n4\n. \nCheck the state of the Jupyter pod.\nResolution\nIf there was an intermittent failure of the notebook server pod:\na\n. \nDelete the notebook server pod that belongs to the user.\nb\n. \nAsk the user to start their notebook server again.\nIf the notebook server does not have sufficient resources to run the selected notebook server\nimage, either add more resources to the OpenShift Container Platform cluster, or choose a\nsmaller image size.\nIf the Jupyter pod is in a \nFAILED\n state:\na\n. \nRetrieve the logs for the \njupyter-nb-*\n pod and send them to Red Hat Support for further\nevaluation.\nb\n. \nDelete the \njupyter-nb-*\n pod.\nIf none of the previous resolutions apply, contact Red Hat Support.\n12.3. THE USER RECEIVES A \nDATABASE OR DISK IS FULL\n ERROR OR A\nNO SPACE LEFT ON DEVICE\n ERROR WHEN THEY RUN NOTEBOOK\nCELLS\nProblem\nThe user might have run out of storage space on their notebook server.\nDiagnosis\n1\n. \nLog in to Jupyter and start the notebook server that belongs to the user having problems. If the\nnotebook server does not start, follow these steps to check whether the user has run out of\nstorage space:\na\n. \nLog in to the OpenShift Container Platform web console.\nb\n. \nClick \nWorkloads\n \n→\n \nPods\n and set the \nProject\n to \nrhods-notebooks\n.\nc\n. \nClick the notebook server pod that belongs to this user, for example, \njupyter-nb-<idp>-\n<username>-*\n.\nd\n. \nClick \nLogs\n. The user has exceeded their available capacity if you see lines similar to the\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n40",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 44
      },
      "page_content": "d\n. \nClick \nLogs\n. The user has exceeded their available capacity if you see lines similar to the\nfollowing:\nUnexpected error while saving file: XXXX database or disk is full\nResolution\nIncrease the user’s available storage by expanding their persistent volume: \nExpanding persistent\nvolumes\nWork with the user to identify files that can be deleted from the \n/opt/app-root/src\n directory on\ntheir notebook server to free up their existing storage space.\nNOTE\nWhen you delete files using the JupyterLab file explorer, the files move to the hidden \n/opt/app-root/src/.local/share/Trash/files\n folder in the persistent storage for the\nnotebook. To free up storage space for notebooks, you must permanently delete these\nfiles.\nCHAPTER 12. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR ADMINISTRATORS\n41",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 45
      },
      "page_content": "CHAPTER 13. TROUBLESHOOTING COMMON PROBLEMS IN\nJUPYTER FOR USERS\nIf you are seeing errors in Red Hat OpenShift AI related to Jupyter, your notebooks, or your notebook\nserver, read this section to understand what could be causing the problem.\nIf you cannot see your problem here or in the release notes, contact Red Hat Support.\n13.1. I SEE A \n403: FORBIDDEN\n ERROR WHEN I LOG IN TO JUPYTER\nProblem\nIf your administrator has configured specialized user groups for OpenShift AI, your user name might not\nbe added to the default user group or the default administrator group for OpenShift AI.\nResolution\nContact your administrator so that they can add you to the correct group/s.\n13.2. MY NOTEBOOK SERVER DOES NOT START\nProblem\nThe OpenShift Container Platform cluster that hosts your notebook server might not have access to\nenough resources, or the Jupyter pod may have failed.\nResolution\nCheck the logs in the \nEvents\n section in OpenShift for error messages associated with the problem. For\nexample:\nServer requested\n2021-10-28T13:31:29.830991Z [Warning] 0/7 nodes are available: 2 Insufficient memory,\n2 node(s) had taint {node-role.kubernetes.io/infra: }, that the pod didn't tolerate, 3 node(s) had taint\n \n{node-role.kubernetes.io/master: },\nthat the pod didn't tolerate.\nContact your administrator with details of any relevant error messages so that they can perform further\nchecks.\n13.3. I SEE A \nDATABASE OR DISK IS FULL\n ERROR OR A \nNO SPACE\nLEFT ON DEVICE\n ERROR WHEN I RUN MY NOTEBOOK CELLS\nProblem\nYou might have run out of storage space on your notebook server.\nResolution\nContact your administrator so that they can perform further checks.\nRed Hat OpenShift AI Self-Managed 2.8 Getting started with Red Hat OpenShift AI Self-Managed\n42",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-getting_started_with_red_hat_openshift_ai_self-managed-en-us.pdf",
        "page": 46
      },
      "page_content": "CHAPTER 13. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR USERS\n43",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 0
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\nIntroduction to Red Hat OpenShift AI\nOpenShift AI is a platform for data scientists and developers of artificial intelligence\nand machine learning (AI/ML) applications\nLast Updated: 2024-04-05",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 1
      },
      "page_content": "",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 2
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\n \nIntroduction to Red Hat\nOpenShift AI\nOpenShift AI is a platform for data scientists and developers of artificial intelligence and machine\nlearning (AI/ML) applications",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 3
      },
      "page_content": "Legal Notice\nCopyright \n©\n 2024 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nRed Hat OpenShift AI is a platform for data scientists and developers of artificial intelligence and\nmachine learning applications.",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 4
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. OVERVIEW OF OPENSHIFT AI\nCHAPTER 2. PRODUCT FEATURES\n2.1. FEATURES FOR DATA SCIENTISTS\n2.2. FEATURES FOR IT OPERATIONS ADMINISTRATORS\nCHAPTER 3. TRY IT\nCHAPTER 4. GET IT\n3\n4\n4\n5\n6\n7\nTable of Contents\n1",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 5
      },
      "page_content": "Red Hat OpenShift AI Self-Managed 2.8 Introduction to Red Hat OpenShift AI\n2",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 6
      },
      "page_content": "CHAPTER 1. OVERVIEW OF OPENSHIFT AI\nRed Hat OpenShift AI is a platform for data scientists and developers of artificial intelligence and\nmachine learning (AI/ML) applications.\nOpenShift AI provides an environment to develop, train, serve, test, and monitor AI/ML models and\napplications on-premises or in the cloud.\nFor data scientists, OpenShift AI includes Jupyter and a collection of default notebook images\noptimized with the tools and libraries required for model development, and the TensorFlow and PyTorch\nframeworks. Deploy and host your models, integrate models into external applications, and export\nmodels to host them in any hybrid cloud environment. You can also accelerate your data science\nexperiments through the use of graphics processing units (GPUs) and Habana Gaudi devices.\nFor administrators, OpenShift AI enables data science workloads in an existing Red Hat OpenShift or\nROSA environment. Manage users with your existing OpenShift identity provider, and manage the\nresources available to notebook servers to ensure data scientists have what they require to create, train,\nand host models. Use accelerators to reduce costs and allow your data scientists to enhance the\nperformance of their end-to-end data science workflows using graphics processing units (GPUs) and\nHabana Gaudi devices.\nOpenShift AI offers two distributions:\nA \nmanaged cloud service add-on\n for Red Hat OpenShift Dedicated (with a Customer Cloud\nSubscription for AWS or GCP) or for Red Hat OpenShift Service on Amazon Web Services\n(ROSA).\nFor information about OpenShift AI on a Red Hat managed environment, see \nProduct\nDocumentation for Red Hat OpenShift AI\n.\nSelf-managed software\n that you can install on-premise or on the public cloud in a self-\nmanaged environment, such as OpenShift Container Platform.\nFor information about OpenShift AI as self-managed software on your OpenShift cluster in a\nconnected or a disconnected environment, see \nProduct Documentation for Red Hat OpenShift\nAI Self-Managed\n.\nFor information about OpenShift AI supported software platforms, components, and dependencies, see\nSupported configurations\n.\nCHAPTER 1. OVERVIEW OF OPENSHIFT AI\n3",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 7
      },
      "page_content": "CHAPTER 2. PRODUCT FEATURES\nRed Hat OpenShift AI provides several features for data scientists and IT operations administrators.\n2.1. FEATURES FOR DATA SCIENTISTS\nContainers\nWhile tools such as JupyterLab already offer intuitive ways for data scientists to develop models on\ntheir machines, there are always inherent complexities involved with collaboration and sharing work.\nMoreover, using specialized hardware such as powerful GPUs can be very expensive when you have\nto buy and maintain your own. The Jupyter environment that is included with OpenShift AI lets you\ntake your development environment anywhere you need it to be. Because all of the workloads are\nrun as containers, collaboration is as easy as sharing an image with your team members, or even\nsimply adding it to the list of default containers that they can use. As a result, GPUs and large\namounts of memory are significantly more accessible, since you are no longer limited by what your\nlaptop can support.\nIntegration with third-party machine learning tools\nWe have all run into situations where our favorite tools or services do not play well with one another.\nOpenShift AI is designed with flexibility in mind. You can use a wide range of open source and third-\nparty tools with OpenShift AI. These tools support the complete machine learning lifecycle, from\ndata engineering and feature extraction to model deployment and management.\nCollaboration on notebooks with Git\nUse Jupyter’s Git interface to work collaboratively with others, and keep good track of the changes\nto your code.\nSecurely built notebook images\nChoose from a default set of notebook images that are pre-configured with the tools and libraries\nthat you need for model development. Software stacks, especially those involved in machine learning,\ntend to be complex systems. There are many modules and libraries in the Python ecosystem that can\nbe used, so determining which versions of what libraries to use can be very challenging. OpenShift AI\nincludes many packaged notebook images that have been built with insight from data scientists and\nrecommendation engines. You can start new projects quickly on the right foot without worrying\nabout downloading unproven and possibly insecure images from random upstream repositories.\nCustom notebooks\nIn addition to notebook images provided and supported by Red Hat and independent software\nvendors (ISVs), you can configure custom notebook images that cater to your project’s specific\nrequirements.\nData science pipelines\nOpenShift AI supports data science pipelines for a mature and efficient way of running your data\nscience workloads. You can standardize and automate machine learning workflows that enable you to\ndevelop and deploy your data science models.\nModel serving\nAs a data scientist, you can deploy your trained machine-learning models to serve intelligent\napplications in production. Deploying or serving a model makes the model’s functions available as a\nservice endpoint that can be used for testing or integration into applications. You have much control\nover how this serving is performed.\nOptimize your data science models with accelerators\nIf you work with large data sets, you can optimize the performance of your data science models in\nOpenShift AI with NVIDIA graphics processing units (GPUs) or Habana Gaudi devices. Accelerators\nenable you to scale your work, reduce latency, and increase productivity.\nRed Hat OpenShift AI Self-Managed 2.8 Introduction to Red Hat OpenShift AI\n4",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 8
      },
      "page_content": "2.2. FEATURES FOR IT OPERATIONS ADMINISTRATORS\nManage users with an identity provider\nOpenShift AI supports the same authentication systems as your OpenShift cluster. By default,\nOpenShift AI is accessible to all users listed in your identity provider and those users do not need a\nseparate set of credentials to access OpenShift AI. Optionally, you can limit the set of users who\nhave access by creating an OpenShift group that specifies a subset of users. You can also create an\nOpenShift group that identifies the list of users who have administrator access to OpenShift AI.\nManage resources with OpenShift\nUse your existing OpenShift knowledge to configure and manage resources for your OpenShift AI\nusers.\nControl Red Hat usage data collection\nChoose whether to allow Red Hat to collect data about OpenShift AI usage in your cluster. Usage\ndata collection is enabled by default when you install OpenShift AI on your OpenShift cluster.\nApply autoscaling to your cluster to reduce usage costs\nUse the cluster autoscaler to adjust the size of your cluster to meet its current needs and optimize\ncosts.\nManage resource usage by stopping idle notebooks\nReduce resource usage in your OpenShift AI deployment by automatically stopping notebook\nservers that have been idle for a period of time.\nImplement model-serving runtimes\nOpenShift AI provides support for model-serving runtimes. A model-serving runtime provides\nintegration with a specified model server and the model frameworks that it supports. By default,\nOpenShift AI includes the OpenVINO Model Server runtime. However, if this runtime doesn’t meet\nyour needs (for example, if it doesn’t support a particular model framework), you can add your own\ncustom runtimes.\nInstall in a disconnected environment\nOpenShift AI Self-Managed supports installation in a disconnected environment. Disconnected\nclusters are on a restricted network, typically behind a firewall and unable to reach the Internet. In this\ncase, clusters cannot access the remote registries where Red Hat provided OperatorHub sources\nreside. In this case, you deploy the OpenShift AI Operator to a disconnected environment by using a\nprivate registry in which you have mirrored (copied) the relevant images.\nManage accelerators\nEnable NVIDIA graphics processing units (GPUs) or Habana Gaudi devices in OpenShift AI and allow\nyour data scientists to use compute-heavy workloads.\nCHAPTER 2. PRODUCT FEATURES\n5",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 9
      },
      "page_content": "CHAPTER 3. TRY IT\nData scientists and developers can try OpenShift AI and access tutorials and activities in the \nRed Hat\nDeveloper sandbox\n environment.\nIT operations administrators can try OpenShift AI in your own cluster with a \n60-day product trial\n.\nRed Hat OpenShift AI Self-Managed 2.8 Introduction to Red Hat OpenShift AI\n6",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-introduction_to_red_hat_openshift_ai-en-us.pdf",
        "page": 10
      },
      "page_content": "CHAPTER 4. GET IT\nManaged cloud service\nYou have the following options for subscribing to OpenShift AI as a managed service:\nFor OpenShift Dedicated, subscribe through Red Hat.\nFor Red Hat OpenShift Service on Amazon Web Services (ROSA), subscribe through\nRed Hat or subscribe through the AWS Marketplace.\nSelf-managed software\nTo get Red Hat OpenShift AI as self-managed software, sign up for it with your Red Hat account\nteam.\nCHAPTER 4. GET IT\n7",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 0
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\nRelease notes\nFeatures, enhancements, resolved issues, and known issues associated with this\nrelease\nLast Updated: 2024-04-09",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 1
      },
      "page_content": "",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 2
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\n \nRelease notes\nFeatures, enhancements, resolved issues, and known issues associated with this release",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 3
      },
      "page_content": "Legal Notice\nCopyright \n©\n 2024 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nThese release notes provide an overview of new features, enhancements, resolved issues, and\nknown issues in version 2.8.1 of Red Hat OpenShift AI.",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 4
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. OVERVIEW OF OPENSHIFT AI\nCHAPTER 2. NEW FEATURES AND ENHANCEMENTS\n2.1. NEW FEATURES\n2.2. ENHANCEMENTS\nCHAPTER 3. TECHNOLOGY PREVIEW FEATURES\nCHAPTER 4. SUPPORT REMOVALS\n4.1. EMBEDDED SUBSCRIPTION CHANNEL DEPRECATED\n4.2. REMOVAL OF BIAS DETECTION (TRUSTYAI)\n4.3. UPCOMING DEPRECATION OF DATA SCIENCE PIPELINES V1\n4.4. VERSION 1.2 NOTEBOOK CONTAINER IMAGES FOR WORKBENCHES ARE NO LONGER SUPPORTED\n4.5. BETA SUBSCRIPTION CHANNEL DEPRECATED\nCHAPTER 5. RESOLVED ISSUES\n5.1. ISSUES RESOLVED IN RED HAT OPENSHIFT AI 2.8.1\n5.2. ISSUES RESOLVED IN RED HAT OPENSHIFT AI 2.8\nCHAPTER 6. KNOWN ISSUES\n3\n4\n4\n4\n5\n6\n6\n6\n6\n6\n6\n7\n7\n7\n10\nTable of Contents\n1",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 5
      },
      "page_content": "Red Hat OpenShift AI Self-Managed 2.8 Release notes\n2",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 6
      },
      "page_content": "CHAPTER 1. OVERVIEW OF OPENSHIFT AI\nRed Hat OpenShift AI is a platform for data scientists and developers of artificial intelligence and\nmachine learning (AI/ML) applications.\nOpenShift AI provides an environment to develop, train, serve, test, and monitor AI/ML models and\napplications on-premises or in the cloud.\nFor data scientists, OpenShift AI includes Jupyter and a collection of default notebook images\noptimized with the tools and libraries required for model development, and the TensorFlow and PyTorch\nframeworks. Deploy and host your models, integrate models into external applications, and export\nmodels to host them in any hybrid cloud environment. You can also accelerate your data science\nexperiments through the use of graphics processing units (GPUs) and Habana Gaudi devices.\nFor administrators, OpenShift AI enables data science workloads in an existing Red Hat OpenShift or\nROSA environment. Manage users with your existing OpenShift identity provider, and manage the\nresources available to notebook servers to ensure data scientists have what they require to create, train,\nand host models. Use accelerators to reduce costs and allow your data scientists to enhance the\nperformance of their end-to-end data science workflows using graphics processing units (GPUs) and\nHabana Gaudi devices.\nOpenShift AI offers two distributions:\nA \nmanaged cloud service add-on\n for Red Hat OpenShift Dedicated (with a Customer Cloud\nSubscription for AWS or GCP) or for Red Hat OpenShift Service on Amazon Web Services\n(ROSA).\nFor information about OpenShift AI on a Red Hat managed environment, see \nProduct\nDocumentation for Red Hat OpenShift AI\n.\nSelf-managed software\n that you can install on-premise or on the public cloud in a self-\nmanaged environment, such as OpenShift Container Platform.\nFor information about OpenShift AI as self-managed software on your OpenShift cluster in a\nconnected or a disconnected environment, see \nProduct Documentation for Red Hat OpenShift\nAI Self-Managed\n.\nFor information about OpenShift AI supported software platforms, components, and dependencies, see\nSupported configurations\n.\nCHAPTER 1. OVERVIEW OF OPENSHIFT AI\n3",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 7
      },
      "page_content": "CHAPTER 2. NEW FEATURES AND ENHANCEMENTS\nThis section describes new features and enhancements in Red Hat OpenShift AI 2.8.\n2.1. NEW FEATURES\nSupport for self-signed certificates\nYou can now use self-signed certificates in your Red Hat OpenShift AI deployments and Data\nScience Projects in an OpenShift Container Platform cluster.\nSome OpenShift AI components have additional options or required configuration for self-signed\ncertificates, as described in \nWorking with certificates\n (for disconnected environments, see \nWorking\nwith certificates\n).\n2.2. ENHANCEMENTS\nUpgraded OpenVINO Model Server\nThe OpenVINO Model Server has been upgraded to version 2023.3. For information on the changes\nand enhancements, see \nOpenVINO™ Model Server 2023.3\n.\nSupport for gRPC protocol on single-model serving platform\nThe single-model serving platform now supports the gRPC API protocol in addition to REST. This\nsupport means that when you add a custom model serving runtime to the platform, you can specify\nwhich protocol the runtime uses.\nExtended support with new release channels\nStarting with OpenShift AI 2.8, Red Hat provides production updates and support for the Red Hat\nOpenShift AI Operator in two new channels, in addition to the \nfast\n, \nstable\n, and \nalpha\n channels:\nThe \nstable-2.8\n channel allows you to stay on the latest 2.8.x release with full support for\nseven months.\nThe \neus-2.8\n channel allows you to stay on the latest 2.8.x release with full support for seven\nmonths, followed by Extended Update Support for eleven months.\nFor more information about subscription channels, see \nInstalling the Red Hat OpenShift AI Operator\n.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n4",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 8
      },
      "page_content": "CHAPTER 3. TECHNOLOGY PREVIEW FEATURES\nIMPORTANT\nThis section describes Technology Preview features in Red Hat OpenShift AI 2.8.\nTechnology Preview features are not supported with Red Hat production service level\nagreements (SLAs) and might not be functionally complete. Red Hat does not\nrecommend using them in production. These features provide early access to upcoming\nproduct features, enabling customers to test functionality and provide feedback during\nthe development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee \nTechnology Preview Features Support Scope\n.\nDistributed workloads\nDistributed workloads enable data scientists to use multiple cluster nodes in parallel for faster, more\nefficient data processing and model training. The CodeFlare framework simplifies task orchestration\nand monitoring, and offers seamless integration for automated resource scaling and optimal node\nutilization with advanced GPU support.\nDesigned for data scientists, the CodeFlare framework enables direct workload configuration from\nJupyter Notebooks or Python code, ensuring a low barrier of adoption, and streamlined,\nuninterrupted workflows. Distributed workloads significantly reduce task completion time, and\nenable the use of larger datasets and more complex models. The distributed workloads feature is\ncurrently available in Red Hat OpenShift AI 2.8 as a Technology Preview feature. This feature was\nfirst introduced in OpenShift AI 2.4.\ncode-server notebook image\nRed Hat OpenShift AI now includes the code-server notebook image. See \ncode-server in GitHub\n for\nmore information.\nWith the code-server notebook image, you can customize your notebook environment to meet your\nneeds using a variety of extensions to add new languages, themes, debuggers, and connect to\nadditional services. Enhance the efficiency of your data science work with syntax highlighting, auto-\nindentation, and bracket matching.\nNOTE\nElyra-based pipelines are not available with the code-server notebook image.\nThe code-server notebook image is currently available in Red Hat OpenShift AI 2.8 as a Technology\nPreview feature. This feature was first introduced in OpenShift AI 2.6.\nCHAPTER 3. TECHNOLOGY PREVIEW FEATURES\n5",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 9
      },
      "page_content": "CHAPTER 4. SUPPORT REMOVALS\nThis section describes major changes in support for user-facing features in Red Hat OpenShift AI.\n4.1. EMBEDDED SUBSCRIPTION CHANNEL DEPRECATED\nStarting with OpenShift AI 2.8, the \nembedded\n subscription channel has been removed. You can no\nlonger select the \nembedded\n channel for a new installation of the Operator. For more information about\nsubscription channels, see \nInstalling the Red Hat OpenShift AI Operator\n.\n4.2. REMOVAL OF BIAS DETECTION (TRUSTYAI)\nStarting with OpenShift AI 2.7, the bias detection (TrustyAI) functionality has been removed. If you\npreviously had this functionality enabled, upgrading to OpenShift AI 2.7 or later will remove the feature.\nThe default TrustyAI notebook image remains supported.\n4.3. UPCOMING DEPRECATION OF DATA SCIENCE PIPELINES V1\nCurrently, data science pipelines in OpenShift AI are based on Kubeflow Pipelines v1. See \nWorking with\ndata science pipelines\n for more information.\nData science pipelines in upcoming releases will be based on Kubeflow Pipelines v2, using a different\nengine. OpenShift AI 2.8 is a stable release that will be supported for 7 months. We recommend that\ncurrent data science pipeline users stay on OpenShift AI 2.8 until you are ready to migrate to the new\npipelines solution.\nFor a detailed view of the 2.8 release lifecycle, including its full support phase window, see \nRed Hat\nOpenShift AI Self-Managed Life Cycle\n.\n4.4. VERSION 1.2 NOTEBOOK CONTAINER IMAGES FOR\nWORKBENCHES ARE NO LONGER SUPPORTED\nWhen you create a workbench, you specify a notebook container image to use with the workbench.\nStarting with OpenShift AI 2.5, when you create a new workbench, version 1.2 notebook container\nimages are not available to select. Workbenches that are already running with a version 1.2 notebook\nimage continue to work normally. However, Red Hat recommends that you update your workbench to\nuse the latest notebook container image.\n4.5. BETA SUBSCRIPTION CHANNEL DEPRECATED\nStarting with OpenShift AI 2.5, the \nbeta\n subscription channel has been removed. You can no longer\nselect the \nbeta\n channel for a new installation of the Operator. For more information about subscription\nchannels, see \nInstalling the Red Hat OpenShift AI Operator\n.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n6",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 10
      },
      "page_content": "CHAPTER 5. RESOLVED ISSUES\nThe following notable issues are resolved in Red Hat OpenShift AI 2.8.1 and 2.8.\n5.1. ISSUES RESOLVED IN RED HAT OPENSHIFT AI 2.8.1\nRHOAIENG-4937\n (previously documented as \nRHOAIENG-4572\n) - Unable to run data science\npipelines after install and upgrade in certain circumstances\nPreviously, you were unable to run data science pipelines after installing or upgrading OpenShift AI in\nthe following circumstances:\nYou installed OpenShift AI and you had a valid CA certificate. Within the \ndefault-dsci\n object,\nyou changed the \nmanagementState\n field for the \ntrustedCABundle\n field to \nRemoved\n post-\ninstallation.\nYou upgraded OpenShift AI from version 2.6 to version 2.8 and you had a valid CA certificate.\nYou upgraded OpenShift AI from version 2.7 to version 2.8 and you had a valid CA certificate.\nThis issue is now resolved.\nRHOAIENG-4327\n - Workbenches do not use the self-signed certificates from centrally configured\nbundle automatically\nThere are two bundle options to include self-signed certificates in OpenShift AI, \nca-bundle.crt\n and \nodh-\nca-bundle.crt\n. Self-signed certificates should apply to workbenches that you create after configuring\nself-signed certificates centrally. Previously, workbenches did not use the self-signed certificates from\nthe centrally configured bundle automatically and you had to define environment variables that pointed\nto your certificate path. This issue is now resolved.\nRHOAIENG-673\n (previously documented as RHODS-12946) - Cannot install from PyPI mirror in\ndisconnected environment or when using private certificates\nIn disconnected environments, Red Hat OpenShift AI cannot connect to the public-facing PyPI\nrepositories, so you must specify a repository inside your network. Previously, if you were using private\nTLS certificates and a data science pipeline was configured to install Python packages, the pipeline run\nwould fail. This issue is now resolved.\nRHOAIENG-637\n (previously documented as RHODS-12904) - Pipeline submitted from Elyra might\nfail when using private certificate\nIf you use a private TLS certificate and you submit a pipeline from Elyra, previously the pipeline could fail\nwith a \ncertificate verify failed\n error message. This issue is now resolved.\n5.2. ISSUES RESOLVED IN RED HAT OPENSHIFT AI 2.8\nRHOAIENG-3355\n - OVMS on KServe does not use accelerators correctly\nPreviously, when you deployed a model using the single-model serving platform and selected the\nOpenVINO Model Server\n serving runtime, if you requested an accelerator to be attached to your model\nserver, the accelerator hardware was detected but was not used by the model when responding to\nqueries. This issue is now resolved.\nRHOAIENG-2869\n - Cannot edit existing model framework and model path in a multi-model project\nCHAPTER 5. RESOLVED ISSUES\n7",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 11
      },
      "page_content": "Previously, when you tried to edit a model in a multi-model project using the \nDeploy model\n dialog, the\nModel framework\n and \nPath\n values did not update. This issue is now resolved.\nRHOAIENG-2724\n - Model deployment fails because fields automatically reset in dialog\nPreviously, when you deployed a model or edited a deployed model, the \nModel servers\n and \nModel\nframework\n fields in the \"Deploy model\" dialog might have reset to the default state. The \nDeploy\n button\nmight have remained enabled even though these mandatory fields no longer contained valid values. This\nissue is now resolved.\nRHOAIENG-2099\n - Data science pipeline server fails to deploy in fresh cluster\nPreviously, when you created a data science pipeline server on a fresh cluster, the user interface\nremained in a loading state and the pipeline server did not start. This issue is now resolved.\nRHOAIENG-1199\n (previously documented as \nODH-DASHBOARD-1928\n) - Custom serving runtime\ncreation error message is unhelpful\nPreviously, when you tried to create or edit a custom model-serving runtime and an error occurred, the\nerror message did not indicate the cause of the error. The error messages have been improved.\nRHOAIENG-675\n (previously documented as RHODS-12906) - Cannot use ModelMesh with object\nstorage that uses private certificates\nPreviously, when you stored models in an object storage provider that used a private TLS certificate, the\nmodel serving pods failed to pull files from the object storage, and the \nsigned by unknown authority\nerror message was shown. This issue is now resolved.\nRHOAIENG-556\n - ServingRuntime for KServe model is created regardless of error\nPreviously, when you tried to deploy a KServe model and an error occurred, the \nInferenceService\ncustom resource (CR) was still created and the model was shown in the \nData Science Project\n page, but\nthe status would always remain unknown. The KServe deploy process has been updated so that the\nServingRuntime is not created if an error occurs.\nRHOAIENG-548\n (previously documented as \nODH-DASHBOARD-1776\n) - Error messages when\nuser does not have project administrator permission\nPreviously, if you did not have administrator permission for a project, you could not access some\nfeatures, and the error messages did not explain why. For example, when you created a model server in\nan environment where you only had access to a single namespace, an \nError creating model server\nerror message appeared. However, the model server is still successfully created. This issue is now\nresolved.\nRHOAIENG-66\n - Ray dashboard route deployed by CodeFlare SDK exposes self-signed certs\ninstead of cluster cert\nPreviously, when you deployed a Ray cluster by using the CodeFlare SDK with the \nopenshift_oauth=True\n option, the resulting route for the Ray cluster was secured by using the \npassthrough\n method and as a result, the self-signed certificate used by the OAuth proxy was exposed.\nThis issue is now resolved.\nRHOAIENG-12\n - Cannot access Ray dashboard from some browsers\nIn some browsers, users of the distributed workloads feature might not have been able to access the\nRay dashboard because the browser automatically changed the prefix of the dashboard URL from \nhttp\nto \nhttps\n. This issue is now resolved.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n8",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 12
      },
      "page_content": "RHODS-6216\n - The ModelMesh oauth-proxy container is intermittently unstable\nPreviously, ModelMesh pods did not deploy correctly due to a failure of the ModelMesh \noauth-proxy\ncontainer. This issue occurred intermittently and only if authentication was enabled in the ModelMesh\nruntime environment. This issue is now resolved.\nCHAPTER 5. RESOLVED ISSUES\n9",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 13
      },
      "page_content": "CHAPTER 6. KNOWN ISSUES\nThis section describes known issues in Red Hat OpenShift AI 2.8.1 and any known methods of working\naround these issues.\nRHOAIENG-5067\n - Model server metrics page does not load for a model server based on the\nModelMesh component\nData science project names that contain capital letters or spaces can cause issues on the model server\nmetrics page for model servers based on the ModelMesh component. The metrics page might not\nreceive data correctly, resulting in a \n400 Bad Request\n error and preventing the page from loading.\nWorkaround\nIn OpenShift Container Platform, change the display names of your data science projects to meet\nKubernetes resource name standards: use only lowercase alphanumeric characters and hyphens.\nRHOAIENG-5025\n - Self-signed certificates do not apply to the first created workbench\nAfter self-signed certificates are configured centrally, the certificates do not apply to the first\nworkbench created in a data science project.\nWorkaround\nFor each data science project that contains a workbench, delete the first workbench that was\ncreated after configuring self-signed certificates, and then create a new workbench. The self-signed\ncertificates work as expected with the new workbench.\nRHOAIENG-4966\n - Self-signed certificates in a custom CA bundle might be missing from the \nodh-\ntrusted-ca-bundle\n configuration map\nSometimes after self-signed certificates are configured in a custom CA bundle, the custom certificate is\nmissing from the \nodh-trusted-ca-bundle\n ConfigMap, or the non-reserved namespaces do not contain\nthe \nodh-trusted-ca-bundle\n ConfigMap when the ConfigMap is set to \nmanaged\n. These issues rarely\noccur.\nWorkaround\nRestart the Red Hat OpenShift AI Operator pod.\nRHOAIENG-4524\n - BuildConfig definitions for RStudio images contain occurrences of incorrect\nbranch\nThe BuildConfig definitions for the \nRStudio\n and \nCUDA - RStudio\n workbench images point to the wrong\nbranch in OpenShift AI. The BuildConfig definitions incorrectly point to the \nmain\n branch instead of the \nrhoai-2.8\n branch.\nWorkaround\nTo use the \nRStudio\n and \nCUDA - RStudio\n workbench images in OpenShift AI, follow the steps in the\nBranch workaround for RStudio image BuildConfig definition\n knowledgebase article.\nRHOAIENG-4497\n - Models on the multi-model serving platform with self-signed certificates stop\nworking after upgrading to 2.8\nIn previous versions, if you wanted to use a self-signed certificate when serving models on the multi-\nmodel serving platform, you had to manually configure the \nstorage-config\n secret used by your data\nconnection to specify a certificate authority (CA) bundle.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n10",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 14
      },
      "page_content": "If you upgrade a previous version of OpenShift AI that used that workaround to the latest version, the\nmulti-model serving platform can no longer serve models.\nWorkaround\nTo use a self-signed certificate with both the multi- and single-model serving platforms, follow the\nsteps in \nAdding a CA bundle\n.\nRHOAIENG-4430\n - CA Bundle does not work for KServe without a data connection\nIf you have installed a certificate authority (CA) bundle on your OpenShift cluster to use self-signed\ncertificates and then use the OpenShift AI dashboard to create a data connection to serve a model,\nOpenShift AI automatically stores the certificate in a secret called \nstorage-config\n. However, if you\nbypass the OpenShift AI dashboard and configure the underlying \nInferenceService\n resource to specify\na different secret name or a service account, OpenShift AI fails to validate SSL connections to the\nmodel and the model status includes \n[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n \nself signed certificate\n.\nWorkaround\nUse the OpenShift AI dashboard to create the data connection for your model. Do not manually\nmodify the \nInferenceService\n resource to specify a different secret name or a service account.\nRHOAIENG-4252\n - Data science pipeline server deletion process fails to remove \nScheduledWorkFlow\n resource\nThe pipeline server deletion process does not remove the \nScheduledWorkFlow\n resource. As a result,\nnew DataSciencePipelinesApplications (DSPAs) do not recognize the redundant \nScheduledWorkFlow\nresource.\nWorkaround\n1\n. \nDelete the pipeline server. For more information, see \nDeleting a pipeline server\n.\n2\n. \nIn the OpenShift command-line interface (CLI), log in to your cluster as a cluster\nadministrator and perform the following command to delete the redundant \nScheduledWorkFlow\n resource.\n$ oc -n <data science project name> delete scheduledworkflows --all\nRHOAIENG-4240\n - Jobs fail to submit to Ray cluster in unsecured environment\nWhen running distributed data science workloads from notebooks in an unsecured OpenShift cluster, a \nConnectionError: Failed to connect to Ray\n error message might be shown.\nWorkaround\nIn the \nClusterConfiguration\n section of the notebook, set the \nopenshift_oauth\n option to \nTrue\n.\nRHOAIENG-3981\n - In unsecured environment, the functionality to wait for Ray cluster to be ready\ngets stuck\nWhen running distributed data science workloads from notebooks in an unsecured OpenShift cluster,\nthe functionality to wait for the Ray cluster to be ready before proceeding (\ncluster.wait_ready()\n) gets\nstuck even when the Ray cluster is ready.\nWorkaround\nCHAPTER 6. KNOWN ISSUES\n11",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 15
      },
      "page_content": "Perform one of the following actions:\nIn the \nClusterConfiguration\n section of the notebook, set the \nopenshift_oauth\n option to \nTrue\n.\nInstead of using the \ncluster.wait_ready()\n, functionality, you can manually check the Ray\ncluster availability by opening the Ray cluster Route URL. When the Ray dashboard is\navailable on the URL, then the cluster is ready.\nRHOAIENG-3963\n - Unnecessary managed resource warning\nWhen you edit and save the \nOdhDashboardConfig\n custom resource for the \nredhat-ods-applications\nproject, the system incorrectly displays the following \nManaged resource\n warning message.\nThis resource is managed by DSC default-doc and any modifications may be overwritten. Edit the\n \nmanaging resource to preserve changes.\nYou can safely ignore this message.\nWorkaround\nClick \nSave\n to close the warning message and apply your edits.\nRHOAIENG-1825\n - After setting up self-signed certificates, executing pipelines might fail with\nworkbenches that contain Elyra\nAfter configuring self-signed certificates centrally, executing pipelines with workbenches that contain\nElyra might fail.\nWorkaround\nSee the following knowledgebase articles for workaround steps:\nWorkbench workaround for executing a pipeline using Elyra\nWorkbench workaround for an object storage connection with a self-signed certificate\nHow to execute a pipeline from a Jupyter notebook in a disconnected environment\nWhen you deploy a model using the single-model serving platform and select the \nOpenVINO Model\nServer\n serving runtime, if you request an accelerator to be attached to your model server, the\naccelerator hardware is detected but is not used by the model when responding to queries. The queries\nare computed by using the CPUs only.\nWorkaround\nTo configure OVMS to use accelerators in preference to CPUs, update your OVMS runtime template\nto add \n--target_device AUTO\n to the CLI options.\nRHOAIENG-3134\n - OVMS supports different model frameworks in single- and multi-model serving\nplatforms\nWhen you deploy a model using the single-model serving platform and select the \nOpenVINO Model\nServer\n runtime, you see additional frameworks in the \nModel framework (name - version)\n list.\nWorkaround\nNone.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n12",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 16
      },
      "page_content": "When you use the OpenVINO Model Server (OVMS) runtime to deploy a model on the single model\nserving platform (which uses KServe), there is a mismatch between the directory layout expected by\nOVMS and that of the model-pulling logic used by KServe. Specifically, OVMS requires the model files\nto be in the \n/<mnt>/models/1/\n directory, while KServe places them in the \n/<mnt>/models/\n directory.\nWorkaround\nPerform the following actions:\n1\n. \nIn your S3-compatible storage bucket, place your model files in a directory called \n1/\n, for\nexample, \n/<s3_storage_bucket>/models/1/<model_files>\n.\n2\n. \nTo use the OVMS runtime to deploy a model on the single model serving platform, choose\none of the following options to specify the path to your model files:\nIf you are using the OpenShift AI dashboard to deploy your model, in the \nPath\n field for\nyour data connection, use the \n/<s3_storage_bucket>/models/\n format to specify the\npath to your model files. Do not specify the \n1/\n directory as part of the path.\nIf you are creating your own \nInferenceService\n custom resource to deploy your model,\nconfigure the value of the \nstorageURI\n field as \n/<s3_storage_bucket>/models/\n. Do not\nspecify the \n1/\n directory as part of the path.\nKServe pulls model files from the subdirectory in the path that you specified. In this case, KServe\ncorrectly pulls model files from the \n/<s3_storage_bucket>/models/1/\n directory in your S3-compatible\nstorage.\nRHOAIENG-3018\n - OVMS on KServe does not expose the correct endpoint in the dashboard\nWhen you use the OpenVINO Model Server (OVMS) runtime to deploy a model on the single-model\nserving platform, the URL shown in the \nInference endpoint\n field for the deployed model is not\ncomplete. To send queries to the model, you must add the \n/v2/models/_<model-name>_/infer\n string to\nthe end of the URL. Replace \n_<model-name>_\n with the name of your deployed model.\nWorkaround\nNone.\nRHOAIENG-2542\n - Inference service pod does not always get an Istio sidecar\nWhen you deploy a model using the single model serving platform (which uses KServe), the \nistio-proxy\ncontainer might be missing in the resulting pod, even if the inference service has the \nsidecar.istio.io/inject=true\n annotation.\nIn OpenShift AI 2.7, the missing \nistio-proxy\n container might not present a problem. However, if the pod\nexperiences connectivity issues, they might be caused by the missing container.\nWorkaround\nDelete the faulty pod. OpenShift AI automatically creates a new pod, which should have the missing\ncontainer.\nRHOAIENG-3378\n - Internal Image Registry is an undeclared hard dependency for Jupyter\nnotebooks spawn process\nBefore you can start OpenShift AI notebooks and workbenches, you must first enable the internal,\nintegrated container image registry in OpenShift Container Platform. Attempts to start notebooks or\nworkbenches without first enabling the image registry will fail with an \"InvalidImageName\" error.\nCHAPTER 6. KNOWN ISSUES\n13",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 17
      },
      "page_content": "You can confirm whether the image registry is enabled for a cluster by using the following command:\n$ oc get pods -n openshift-image-registry\nWorkaround\nEnable the internal, integrated container image registry in OpenShift Container Platform.\nSee \nImage Registry Operator in OpenShift Container Platform\n for more information about how to set up\nand configure the image registry.\nWhen you try to edit a model in a multi-model project using the \nDeploy model\n dialog, the \nModel\nframework\n and \nPath\n values do not update.\nWorkaround\nNone available.\nWhen you create a second model server in a project where one server is using token authentication, and\nthe other server does not use authentication, the deployment of the second model might fail to start.\nWorkaround\nNone available.\nWhen you deploy a model or edit a deployed model, the \nModel servers\n and \nModel framework\n fields in\nthe \"Deploy model\" dialog might reset to the default state. The \nDeploy\n button might remain enabled\neven though these mandatory fields no longer contain valid values.\nIf you click \nDeploy\n when the \nModel servers\n and \nModel framework\n fields are not set, the model\ndeployment pods are not created.\nWorkaround\nNone available.\nRHOAIENG-2620\n - Unable to create duplicate bias metrics from existing bias metrics\nYou can’t duplicate existing bias metrics.\nWorkaround\n1\n. \nIn the left menu of the OpenShift AI dashboard, click \nModel Serving\n.\n2\n. \nOn the \nDeployed models\n page, click the name of the model with the bias metric that you\nwant to duplicate.\n3\n. \nIn the metrics page for the model, click the \nModel bias\n tab.\n4\n. \nClick the action menu (\n⋮\n) next to the metric that you want to copy and then click \nDuplicate\n.\n5\n. \nThe \nConfigure bias metrics\n dialog will open with prepopulated values for the bias\nconfiguration. For each of the \nPrivileged value\n, \nUnprivileged value\n and \nOutput value\nfields, cut the value and then paste it back in.\nNote: Do not copy and paste these values.\n6\n. \nClick \nConfigure\n.\nThe \nAverage response time\n server metric graph shows multiple lines if the ModelMesh pod is restarted.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n14",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 18
      },
      "page_content": "Workaround\nNone available.\nRHOAIENG-2585\n - UI does not display an error/warning when UWM is not enabled in the cluster\nRed Hat OpenShift AI does not correctly warn users if User Workload Monitoring (UWM) is \ndisabled\n in\nthe cluster. UWM is necessary for the correct functionality of model metrics.\nWorkaround\nManually ensure that UWM is enabled in your cluster, as described in \nEnabling monitoring for user-\ndefined projects\n.\nRHOAIENG-2555\n - Model framework selector does not reset when changing Serving Runtime in\nform\nWhen you use the \nDeploy model\n dialog to deploy a model on the single model serving platform, if you\nselect a runtime and a supported framework, but then switch to a different runtime, the existing\nframework selection is not reset. This means that it is possible to deploy the model with a framework\nthat is not supported for the selected runtime.\nWorkaround\nWhile deploying a model, if you change your selected runtime, click the \nSelect a framework\n list again\nand select a supported framework.\nThe Prometheus target for the TrustyAI controller manager is down due to a mismatch with the\nendpoint’s port. Alerts for TrustyAI will fire if the controller deployment pod is down.\nWorkaround\nNone available.\nIf you upgrade the Red Hat OpenShift AI operator from version 2.4 to 2.5, and then update the operator\nto version 2.6, 2.7, or 2.8, all components related to hardware resource-consuming model monitoring are\nremoved from the cluster. Some residual model-monitoring resources, which do not consume hardware\nresources, will still be present.\nWorkaround\nTo delete these resources, execute the following \noc delete\n commands with cluster-admin privileges:\n$ oc delete service rhods-model-monitoring -n redhat-ods-monitoring\n$ oc delete service prometheus-operated -n redhat-ods-monitoring\n$ oc delete sa prometheus-custom -n redhat-ods-monitoring\n$ oc delete sa rhods-prometheus-operator -n redhat-ods-monitoring\n$ oc delete prometheus rhods-model-monitoring -n redhat-ods-monitoring\n$ oc delete route rhods-model-monitoring -n redhat-ods-monitoring\nRHOAIENG-2468\n - Services in the same project as KServe might become inaccessible in\nOpenShift\nIf you deploy a non-OpenShift AI service in a data science project that contains models deployed on the\nsingle model serving platform (which uses KServe), the accessibility of the service might be affected by\nthe network configuration of your OpenShift cluster. This is particularly likely if you are using the \nOVN-\nKubernetes network plugin\n in combination with host network namespaces.\nWorkaround\nCHAPTER 6. KNOWN ISSUES\n15",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 19
      },
      "page_content": "Perform one of the following actions:\nDeploy the service in another data science project that does not contain models deployed on\nthe single model serving platform. Or, deploy the service in another OpenShift project.\nIn the data science project where the service is, add a \nnetwork policy\n to accept ingress traffic\nto your application pods, as shown in the following example:\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-ingress-to-myapp\nspec:\n  podSelector:\n    matchLabels:\n      app: myapp\n  ingress:\n     - {}\nRHOAIENG-2312\n - Importing numpy fails in code-server workbench\nImporting \nnumpy\n in your code-server workbench fails.\nWorkaround\n1\n. \nIn your code-server workbench, from the \nActivity bar\n, select the menu icon( \n \n) >\nView\n > \nCommand Palette\n to open the Command Palette.\nIn Firefox, you can use the F1 keyboard shortcut to open the command palette.\n2\n. \nEnter \npython: s\n.\n3\n. \nFrom the drop-down list, select the \nPython: Select interpreter\n action.\n4\n. \nIn the \nSelect Interpreter\n dialog, select \nEnter interpreter path…\n.\n5\n. \nEnter \n/opt/app-root/bin/python3\n as the interpreter path and press \nEnter\n.\n6\n. \nFrom the drop-down list, select the new Python interpreter.\n7\n. \nConfirm that the new interpreter (\napp-root\n) appears on the \nStatus bar\n. The selected\ninterpreter persists if the workbench is stopped and started again, so the workaround should\nneed to be performed only once for each workbench.\nYou can’t edit the deployment settings (for example, the number of replicas) of a model you deployed\nwith a single-model platform.\nWorkaround\nNone available.\nRHOAIENG-2269\n - (Single-model) Dashboard fails to display the correct number of model replicas\nOn a single-model platform, the \nModels and model servers\n section of a data science project does not\nshow the correct number of model replicas.\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n16",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 20
      },
      "page_content": "Workaround\nCheck the number of replicas using the following CLI command:\n$ oc -n \n<project_resource_name>\n get pods --selector\n \nserving.kserve.io/inferenceservice=\n<model_resource_name>\nYou can find your \n<project_resource_name>\n and \n<model_resource_name>\n values in the OpenShift\nAI dashboard.\nYou can also check the number of model replicas from the OpenShift Container Platform web console,\nunder \nWorkloads\n > \nPods\n.\nOn the \nEndpoint performance\n tab of the model metrics screen, if you set the \nRefresh interval\n to 15\nseconds and the \nTime range\n to 1 hour, the graph results change continuously.\nWorkaround\nNone available.\nRHOAIENG-2183\n - Endpoint performance graphs might show incorrect labels\nIn the \nEndpoint performance\n tab of the model metrics screen, the graph tooltip might show incorrect\nlabels.\nWorkaround\nNone available.\nRHOAIENG-1919\n - Model Serving page fails to fetch or report the model route URL soon after its\ndeployment\nWhen deploying a model from the OpenShift AI dashboard, the system displays the following warning\nmessage while the \nStatus\n column of your model indicates success with an \nOK\n/green checkmark.\nFailed to get endpoint for this deployed model. routes.rout.openshift.io\"<model_name>\" not found\nWorkaround\nRefresh your browser page.\nThe Knative \nnet-istio-controller\n pod (which is a dependency for KServe) might continuously crash due\nto an out-of-memory (OOM) error.\nWorkaround\nIn the custom resource (CR) for your KnativeServing instance, add an \nENABLE_SECRET_INFORMER_FILTERING_BY_CERT_UID=true\n annotation to inject an\nenvironment variable to the \nnet-istio-controller\n pod. Injecting this environment variable reduces the\nnumber of secrets that the \nnet-istio-controller\n watches and loads into memory.\nFor more information about this configuration, see \nCreating a Knative Serving instance\n.\nThe Red Hat OpenShift AI Add-on uninstall does not delete OpenShift AI components after being\ntriggered via OCM APIs.\nWorkaround\nManually delete the remaining OpenShift AI resources as follows:\n1\n. \nDelete the \nDataScienceCluster\n CR.\nCHAPTER 6. KNOWN ISSUES\n17",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 21
      },
      "page_content": "1\n. \nDelete the \nDataScienceCluster\n CR.\n2\n. \nWait until all pods are deleted from the \nredhat-ods-applications\n namespace.\n3\n. \nIf Serverless was set to \nManaged\n in the \nDataScienceCluster\n CR, wait until all pods are\ndeleted from the \nknative-serving\n namespace.\n4\n. \nDelete the \nDSCInitialization\n CR.\n5\n. \nIf Service Mesh was set to \nManaged\n in the \nDSCInitialization\n CR, wait until all pods are\ndeleted from the \nistio-system\n namespace.\n6\n. \nUninstall the Red Hat OpenShift AI Operator.\n7\n. \nWait until all pods are deleted from the \nredhat-ods-operator\n namespace and the \nredhat-\nods-monitoring\n namespace.\nRHOAIENG-880\n - Default pipelines service account is unable to create Ray clusters\nYou cannot create Ray clusters using the default pipelines Service Account.\nWorkaround\nAuthenticate using the CodeFlare SDK, by adding the following lines to the pipeline code:\nfrom codeflare_sdk.cluster.auth import TokenAuthentication\n   auth = TokenAuthentication(\n       token=openshift_token, server=openshift_server, skip_tls=True\n   )\n   auth_return = auth.login()\nIf a deployed model does not receive at least one HTTP request for each of the two data types (success\nand failed), the graphs that show HTTP request performance metrics (for all models on the model server\nor for the specific model) render incorrectly, with a straight line that indicates a steadily increasing\nnumber of failed requests.\nWorkaround\nAfter the deployed data model receives at least one HTTP request that is successful and one that is\nfailed, the graphs show the HTTP request performance metrics correctly. The graphs work correctly\nas long as one HTTP request of each data type (success and failed) occur at any point in the history\nof the deployed model, regardless of the time range that you specify for the graphs.\nA No Components Found page might appear when you access the Red Hat OpenShift AI dashboard.\nWorkaround\nRefresh the browser page.\nRHOAIENG-234\n - Unable to view .ipynb files in VSCode in Insecured cluster\nWhen you use the code-server notebook image on Google Chrome in an insecure cluster, you cannot\nview .ipynb files.\nWorkaround\nUse a different browser.\nWhen you set a number of model server replicas different from the default (1), the model (server) is still\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n18",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 22
      },
      "page_content": "When you set a number of model server replicas different from the default (1), the model (server) is still\ndeployed with 1 replica. —-\nRHOAIENG-2184\n - Cannot create Ray clusters or distributed workloads\nUsers cannot create Ray clusters or distributed workloads in namespaces where they have \nadmin\n or \nedit\n permissions.\nWorkaround\nTo grant the appropriate permissions, create a ClusterRole for the resources created by the KubeRay\nOperator and CodeFlare Operator, and specify the \nadmin\n and \nedit\n aggregation labels, as described\nin the Red Hat Knowledgebase solution \nHow to grant permission to create Ray clusters and\ndistributed workloads in RHOAI\n.\nRHOAIENG-2099\n - Data science pipeline server fails to deploy in fresh cluster\nWhen you create a data science pipeline server on a fresh cluster, the user interface remains in a loading\nstate and the pipeline server does not start. A “Pipeline server failed” error message might be displayed.\nWorkaround\nDelete the pipeline server and create a new one.\nIf the problem persists, disable the database health check in the DSPA custom resource:\n1\n. \nUse the following command to edit the custom resource:\n$ oc edit dspa pipelines-definition -n my-project\n2\n. \nSet the \nspec.database.disableHealthCheck\n value to \ntrue\n.\n3\n. \nSave the change.\nRHOAIENG-908\n - Cannot use ModelMesh if KServe was previously enabled and then removed\nWhen both ModelMesh and KServe are enabled in the \nDataScienceCluster\n object, and you\nsubsequently remove KServe, you can no longer deploy new models with ModelMesh. You can continue\nto use models that were previously deployed with ModelMesh.\nExample error message:\nError creating model serverInternal error occurred: failed calling webhook \"inferenceservice.kserve-\nwebhook-server.defaulter\": failed to call webhook: Post \"https://kserve-webhook-server-\nservice.redhat-ods-applications.svc:443/mutate-serving-kserve-io-v1beta1-inferenceservice?\ntimeout=10s\": service \"kserve-webhook-server-service\" not found\nWorkaround\nYou can resolve this issue in either of the following ways:\nRe-enable KServe.\nDelete the KServe MutatingWebHook configuration by completing the following steps as a\nuser with \ncluster-admin\n permissions:\n1\n. \nLog in to your cluster by using the \noc\n client.\n2\n. \nEnter the following command:\nCHAPTER 6. KNOWN ISSUES\n19",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 23
      },
      "page_content": "oc delete mutatingwebhookconfigurations inferenceservice.serving.kserve.io\nRHOAIENG-807\n - Accelerator profile toleration removed when restarting a workbench\nIf you create a workbench that uses an accelerator profile that in turn includes a toleration, restarting the\nworkbench removes the toleration information, which means that the restart cannot complete. A freshly\ncreated GPU-enabled workbench might start the first time, but never successfully restarts afterwards\nbecause the generated pod remains forever pending.\nRHOAIENG-804\n - Cannot deploy Large Language Models with KServe on FIPS-enabled clusters\nRed Hat OpenShift AI is not yet fully designed for FIPS. You cannot deploy Large Language Models\n(LLMs) with KServe on FIPS-enabled clusters.\nRHOAIENG-517\n - User with edit permissions cannot see created models\nA user with edit permissions cannot see any created models, unless they are the project owner or have\nadmin permissions for the project.\nWorkaround\nIf the project owner or a user with admin permissions subsequently creates a model, the user with\nedit permissions can then see all models.\nRHOAIENG-499\n - Uninstalling Red Hat OpenShift AI Self Managed by using the CLI does not\nuninstall\nIf you uninstall Red Hat OpenShift AI by using the command-line interface, then the \nDataScienceCluster\n CR, the \nDSCInitialization\n CR, and the Red Hat OpenShift AI Operator are not\nremoved.\nWorkaround\nManually delete the remaining OpenShift AI resources as follows:\n1\n. \nDelete the \nDataScienceCluster\n CR.\n2\n. \nWait until all pods are deleted from the \nredhat-ods-applications\n namespace.\n3\n. \nIf Serverless was set to \nManaged\n in the \nDataScienceCluster\n CR, wait until all pods are\ndeleted from the \nknative-serving\n namespace.\n4\n. \nDelete the \nDSCInitialization\n CR.\n5\n. \nIf Service Mesh was set to \nManaged\n in the \nDSCInitialization\n CR, wait until all pods are\ndeleted from the \nistio-system\n namespace.\n6\n. \nUninstall the Red Hat OpenShift AI Operator.\n7\n. \nWait until all pods are deleted from the \nredhat-ods-operator\n namespace and the \nredhat-\nods-monitoring\n namespace.\nRHOAIENG-343\n - Manual configuration of OpenShift Service Mesh and OpenShift Serverless does\nnot work for KServe\nIf you install OpenShift Serverless and OpenShift Service Mesh and then install Red Hat OpenShift AI\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n20",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 24
      },
      "page_content": "If you install OpenShift Serverless and OpenShift Service Mesh and then install Red Hat OpenShift AI\nwith KServe enabled, KServe is not deployed.\nWorkaround\n1\n. \nEdit the \nDSCInitialization\n resource: Set the \nmanagementState\n field of the \nserviceMesh\ncomponent to \nUnmanaged\n.\n2\n. \nEdit the \nDataScienceCluster\n resource: Within the \nkserve\n component, set the \nmanagementState\n field of the \nserving\n component to \nUnmanaged\n. For more information,\nsee \nInstalling KServe\n.\nRHOAIENG-339\n - KServe component images are not updated after upgrade to 2.5\nPreviously, the KServe component was a Limited Availability feature. If you enabled the \nkserve\ncomponent and created models in an earlier version, then after you upgrade to Red Hat OpenShift AI\n2.5, you must update some OpenShift AI resources as follows:\n1\n. \nLog in as an admin user to the OpenShift Container Platform cluster where OpenShift AI 2.5 is\ninstalled:\n$ oc login\n2\n. \nUpdate the \nDSCInitialization\n resource as follows:\n$ oc patch $(oc get dsci -A -oname) --type='json' -p='[{\"op\": \"replace\", \"path\":\n \n\"/spec/serviceMesh/managementState\", \"value\":\"Unmanaged\"}]'\n3\n. \nUpdate the \nDataScienceCluster\n resource as follows:\n$ oc patch $(oc get dsc -A -oname) --type='json' -p='[{\"op\": \"replace\", \"path\":\n \n\"/spec/components/kserve/serving/managementState\", \"value\":\"Unmanaged\"}]'\n4\n. \nUpdate the \nInferenceServices\n CRD as follows:\n$ oc patch crd inferenceservices.serving.kserve.io --type=json -p='[{\"op\": \"remove\", \"path\":\n \n\"/spec/conversion\"}]'\n5\n. \nOptionally, restart the Operator pod.\nRHOAIENG-293\n - Deprecated ModelMesh monitoring stack not deleted after upgrading from 2.4\nto 2.5\nIn Red Hat OpenShift AI 2.5, the former ModelMesh monitoring stack is no longer deployed because it is\nreplaced by user workload monitoring. However, the former monitoring stack is not deleted during an\nupgrade to OpenShift AI 2.5. Some components remain and use cluster resources.\nRHOAIENG-288\n - Recommended image version label for workbench is shown for two versions\nMost of the workbench images that are available in OpenShift AI are provided in multiple versions. The\nonly recommended version is the latest version. In the current release, the \nRecommended\n tag is\nerroneously shown for multiple versions of an image.\nRHOAIENG-162\n - Project remains selected after navigating to another page\nCHAPTER 6. KNOWN ISSUES\n21",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 25
      },
      "page_content": "When you select a project on the \nData Science Projects\n page, the project remains selected, even after\nyou navigate to another page. For example, if you subsequently open the \nModel Serving\n page, the\npage lists only the models for the previously selected project, instead of the models for all projects.\nWorkaround\nFrom the \nProject\n list, select \nAll projects\n.\nRHOAIENG-84\n - Cannot use self-signed certificates with KServe\nThe single model serving platform does not support self-signed certificates.\nWorkaround\nTo deploy a model from S3 storage, disable SSL authentication as described in the Red Hat\nKnowledgebase solution \nHow to skip the validation of SSL for KServe\n.\nRHOAIENG-66\n - Ray dashboard route deployed by CodeFlare SDK exposes self-signed certs\ninstead of cluster cert\nWhen you deploy a Ray cluster by using the CodeFlare SDK with the \nopenshift_oauth=True\n option, the\nresulting route for the Ray cluster is secured by using the \npassthrough\n method. As a result, the self-\nsigned certificate used by the OAuth proxy is exposed.\nWorkaround\nUse one of the following workarounds:\nSet the \nopenshift_oauth\n option to \nFalse\n.\nAdd the self-signed certificate used by the OAuth proxy to the client’s truststore.\nCreate a route manually, using a route configuration and certificate that is based on the\nneeds of the client.\nRHOAIENG-1199\n (previously documented as \nODH-DASHBOARD-1928\n - Custom serving runtime\ncreation error message is unhelpful\nWhen you try to create or edit a custom model-serving runtime and an error occurs, the error message\ndoes not indicate the cause of the error.\nExample error message: \nRequest failed with status code 422\nWorkaround\nCheck the YAML code for the serving runtime to identify the reason for the error.\nODH-DASHBOARD-1991\n - ovms-gpu-ootb is missing recommended accelerator annotation\nWhen you add a model server to your project, the \nServing runtime\n list does not show the\nRecommended serving runtime\n label for the NVIDIA GPU.\nWorkaround\nMake a copy of the model-server template and manually add the label.\nRHODS-12717\n - Pipeline server creation might fail on OpenShift Container Platform with Open\nVirtual Network on OpenStack\nWhen you try to create a pipeline server on OpenShift Container Platform with Open Virtual Network on\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n22",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 26
      },
      "page_content": "When you try to create a pipeline server on OpenShift Container Platform with Open Virtual Network on\nOpenStack, the creation might fail with a \nPipeline server failed\n error. See \nOCPBUGS-22251\n.\nRHODS-12899\n - OpenVINO runtime missing annotation for NVIDIA GPUs\nRed Hat OpenShift AI currently includes an out-of-the-box serving runtime that supports NVIDIA GPUs:\nOpenVINO model server (support GPUs)\n. You can use the accelerator profile feature introduced in\nOpenShift AI 2.4 to select a specific accelerator in model serving, based on configured accelerator\nprofiles. If the cluster had NVIDIA GPUs enabled in an earlier OpenShift AI release, the system\nautomatically creates a default NVIDIA accelerator profile during upgrade to OpenShift AI 2.4. However,\nthe \nOpenVINO model server (supports GPUs)\n runtime has not been annotated to indicate that it\nsupports NVIDIA GPUs. Therefore, if a user selects the \nOpenVINO model server (supports GPUs)\nruntime and selects an NVIDIA GPU accelerator in the model server user interface, the system displays a\nwarning that the selected accelerator is not compatible with the selected runtime. In this situation, you\ncan ignore the warning.\nRHOAIENG-637\n (previously documented as RHODS-12904) - Pipeline submitted from Elyra might\nfail when using private certificate\nIf you use a private TLS certificate, and you submit a pipeline from Elyra, the pipeline might fail with a \ncertificate verify failed\n error message. This issue might be caused by either or both of the following\nsituations:\nThe object storage used for the pipeline server is using private TLS certificates.\nThe data science pipeline server API endpoint is using private TLS certificates.\nWorkaround\nProvide the workbench with the correct Certificate Authority (CA) bundle, and set various\nenvironment variables so that the correct CA bundle is recognized. Contact Red Hat Support for\ndetailed steps to resolve this issue.\nRHODS-12906\n - Cannot use ModelMesh with object storage that uses private certificates\nSometimes, when you store models in an object storage provider that uses a private TLS certificate, the\nmodel serving pods fail to pull files from the object storage, and the \nsigned by unknown authority\nerror message is shown.\nWorkaround\nManually update the secret created by the data connection so that the secret includes the correct\nCA bundle. Contact Red Hat Support for detailed steps to resolve this issue.\nRHODS-12937\n - Previously deployed model server might no longer work after upgrade in\ndisconnected environment\nIn disconnected environments, after upgrade to Red Hat OpenShift AI 2.8, previously deployed model\nservers might no longer work. The model status might be incorrectly reported as \nOK\n on the dashboard.\nWorkaround\nUpdate the \ninferenceservices\n resource to replace the \nstorage\n section with the \nstorageUri\n section.\nIn the following instructions, replace \n<placeholders>\n with the values for your environment.\n1\n. \nRemove the \nstorage\n parameter section from the existing \ninferenceservices\n resource:\nCHAPTER 6. KNOWN ISSUES\n23",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 27
      },
      "page_content": "Example:\n\"storage\":\n      \"key\": \"aws-connection-minio-connection\",\n      \"path\": \"mnist-8.onnx\"\n2\n. \nAdd the \nstorageUri\n section to the \ninferenceservices\n resource, with the specified format \ns3://bucket-name/path/to/object\n, as shown in the following example:\nExample:\nstorageUri: 's3://bucket/mnist-8.onnx'\n3\n. \nCapture the secret key name as follows:\n4\n. \nUpdate the annotation as follows:\nRHOAIENG-12\n - Cannot access Ray dashboard from some browsers\nIn some browsers, users of the distributed workloads feature might not be able to access the Ray\ndashboard, because the browser automatically changes the prefix of the dashboard URL from \nhttp\n to \nhttps\n. The distributed workloads feature is currently available in Red Hat OpenShift AI as a Technology\nPreview feature. See \nTechnology Preview features\n.\nWorkaround\nChange the URL prefix from \nhttps\n to \nhttp\n.\nDATA-SCIENCE-PIPELINES-OPERATOR-362\n - Pipeline server fails that uses object storage\nsigned by an unknown authority\nData science pipeline servers fail if you use object storage signed by an unknown authority. As a result,\nyou cannot currently use object storage with a self-signed certificate. This issue has been observed in a\ndisconnected environment.\nWorkaround\nConfigure your system to use object storage with a self-signed certificate, as described in the\nRed Hat Knowledgebase solution \nData Science Pipelines workaround for an object storage\nconnection with a self-signed certificate\n.\nRHOAIENG-548\n (previously documented as \nODH-DASHBOARD-1776\n) - Error messages when\nuser does not have project administrator permission\nIf you do not have administrator permission for a project, you cannot access some features, and the\nerror messages do not explain why. For example, when you create a model server in an environment\n\"storage\"\n:\n      \n\"key\"\n: \n\"\n<your_key>\n\"\n,\n      \n\"path\"\n: \n\"\n<your_path>\n\"\nsecret_key=$(oc get secret -n \n<project_name>\n | grep -i aws-connection | awk \n'{print $1}'\n)\noc annotate $(oc get inferenceservices -n \n<project_name>\n -o name) -n \n<project_name>\n \nserving.kserve.io/secretKey=\n\"\n$secret_key\n\"\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n24",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 28
      },
      "page_content": "where you only have access to a single namespace, an \nError creating model server\n error message\nappears. However, the model server is still successfully created.\nDATA-SCIENCE-PIPELINES-OPERATOR-294\n - Scheduled pipeline run that uses data-passing\nmight fail to pass data between steps, or fail the step entirely\nA scheduled pipeline run that uses an S3 object store to store the pipeline artifacts might fail with an\nerror such as the following:\nBad value for --endpoint-url \"cp\": scheme is missing. Must be of the form http://<hostname>/ or\n \nhttps://<hostname>/\nThis issue occurs because the S3 object store endpoint is not successfully passed to the pods for the\nscheduled pipeline run.\nWorkaround\nDepending on the size of the pipeline artifacts being passed, you can either partially or completely\nwork around this issue by applying a custom artifact-passing script and then restarting the pipeline\nserver. Specifically, this workaround results in the following behavior:\nFor pipeline artifacts smaller than 3 kilobytes, the pipeline run now successfully passes the\nartifacts into your S3 object store.\nFor pipeline artifacts larger than 3 kilobytes, the pipeline run still \ndoes not\n pass the artifacts\ninto your S3 object store. However, the workaround ensures that the run continues to\ncompletion. Any smaller artifacts in the remainder of the pipeline run are successfully stored.\nTo apply this workaround, perform the following actions:\n1\n. \nIn a text editor, paste the following YAML-based artifact-passing script. The script defines a \nConfigMap\n object.\napiVersion:\n v1\ndata:\n  artifact_script:\n |-\n  \n#!/usr/bin/env sh\n    push_artifact() {\n       workspace_dir=$(echo $(context.taskRun.name) | sed -e \n\"s/$(context.pipeline.name)-\n//g\"\n)\n        \nworkspace_dest=/workspace/${workspace_dir}/artifacts/$(context.pipelineRun.name)/$(context.\ntaskRun.name)\n        artifact_name=$(basename $\n2\n)\n        if [ -f \n\"$workspace_dest/$artifact_name\"\n ]; then\n            echo sending to: ${workspace_dest}/${artifact_name}\n            tar -cvzf $\n1.\ntgz -C ${workspace_dest} ${artifact_name}\n            aws s3 --endpoint <Endpoint> cp $\n1.\ntgz\n \ns3://<Bucket>/artifacts/$PIPELINERUN/$PIPELINETASK/$\n1.\ntgz\n        elif [ -f \n\"$2\"\n ]; then\n            tar -cvzf $\n1.\ntgz -C $(dirname $\n2\n) ${artifact_name}\n            aws s3 --endpoint <Endpoint> cp $\n1.\ntgz\n \ns3://<Bucket>/artifacts/$PIPELINERUN/$PIPELINETASK/$\n1.\ntgz\n        else\n            echo \n\"$2 file does not exist. Skip artifact tracking for $1\"\n        fi\nCHAPTER 6. KNOWN ISSUES\n25",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 29
      },
      "page_content": "$ oc apply -f \n<configmap_file_name>\n.yaml\n. Restart the pipeline server.\n+\n[source,subs=\"+quotes\"]\n$ oc project \n<data_science_project_name>\n $ oc delete pod $(oc get pods -l app=ds-pipeline-pipelines-\ndefinition --no-headers | awk \n{print $1}\n)\n   \nRHODS-9764\n - Data connection details get reset when editing a workbench\n  \n   When you edit a workbench that has an existing data connection and then select the \nCreate new\n \ndata connection\n option, the edit page might revert to the \nUse existing data connection\n option before\n \nyou have finished specifying the new connection details.\n  \nWorkaround\n      To work around this issue, perform the following actions:\n     \n1\n. \n        Select the \nCreate new data connection\n option again.\n       \n2\n. \n        Specify the new connection details and click \nUpdate workbench\n before the page\n \nreverts to the \nUse existing data connection\n option.\n       \n    }\n    push_log() {\n        cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log\n        push_artifact main-log step-main.log\n    }\n    strip_eof() {\n        if [ -f \n\"$2\"\n ]; then\n            awk \n'NF'\n $\n2\n | head -c \n-1\n > $\n1\n_temp_save && cp $\n1\n_temp_save $\n2\n        fi\n    }\nkind:\n ConfigMap\nmetadata:\n  name:\n custom-script\n -\n---\n. In the script, replace any occurrences  of _<Endpoint>_ with your S3 endpoint (for example,\n \nhttps://s3.amazonaws.com), and occurrences of _<Bucket>_ with your S3 bucket name.\n. Save the YAML file for the `ConfigMap` object.\n. Apply the YAML file.\n+\n[source,subs=\n\"+quotes\"\n]\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n26",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 30
      },
      "page_content": " \n   \nRHODS-9030\n - Uninstall process for OpenShift AI might become stuck when removing \nkfdefs\n \nresources\n  \n   The steps for uninstalling OpenShift AI self-managed are described in \nUninstalling OpenShift AI\n \nself-managed\n.\n  \n   However, even when you follow this guide, you might see that the uninstall process does not finish\n \nsuccessfully. Instead, the process stays on the step of deleting \nkfdefs\n resources that are used by the\n \nKubeflow Operator. As shown in the following example, \nkfdefs\n resources might exist in the \nredhat-\nods-applications\n, \nredhat-ods-monitoring\n, and \nrhods-notebooks\n namespaces:\n  \n$ oc get kfdefs.kfdef.apps.kubeflow.org -A\nNAMESPACE                  NAME                                   AGE\nredhat-ods-applications    rhods-anaconda                         3h6m\nredhat-ods-applications    rhods-dashboard                        3h6m\nredhat-ods-applications    rhods-data-science-pipelines-operator  3h6m\nredhat-ods-applications    rhods-model-mesh                       3h6m\nredhat-ods-applications    rhods-nbc                              3h6m\nredhat-ods-applications    rhods-osd-config                       3h6m\nredhat-ods-monitoring      modelmesh-monitoring                   3h6m\nredhat-ods-monitoring      monitoring                             3h6m\nrhods-notebooks            rhods-notebooks                        3h6m\nrhods-notebooks            rhods-osd-config                       3h5m\n   Failed removal of the \nkfdefs\n resources might also prevent later installation of a newer version of\n \nOpenShift AI.\n  \nWorkaround\n      To manually delete the \nkfdefs\n resources so that you can complete the uninstall process, see\n \nthe \n\"Force individual object removal when it has finalizers\"\n section of the Red Hat Knowledgebase\n \nsolution \nUnable to Delete a Project or Namespace in OCP\n.\n     \n \n   \nRHODS-8939\n - For a Jupyter notebook created in a previous release, default shared memory might\n \ncause a runtime error\n  \n   For a Jupyter notebook created in a release earlier than 1.31, the default shared memory for a\n \nJupyter notebook is set to 64 MB and you cannot change this default value in the notebook\n \nconfiguration.\n  \nCHAPTER 6. KNOWN ISSUES\n27",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 31
      },
      "page_content": "   For example, PyTorch relies on shared memory and the default size of 64 MB is not enough for\n \nlarge use cases, such as training a model or performing heavy data manipulations. Jupyter reports a\n \n\"no space left on device\" message and \n/dev/smh\n is full.\n  \n   Starting with release 1.31, this issue is fixed and any new notebook’s shared memory is set to the\n \nsize of the node.\n  \nWorkaround\n      For a Jupyter notebook created in a release earlier than 1.31, either recreate the Jupyter\n \nnotebook or follow these steps:\n     \n1\n. \n        In your data science project, create a workbench as described in \nCreating a project\n \nworkbench\n.\n       \n2\n. \n        In the data science project page, in the \nWorkbenches\n section, click the \nStatus\n toggle\n \nfor the workbench to change it from \nRunning\n to \nStopped\n.\n       \n3\n. \n        Open your OpenShift Console and then select \nAdministrator\n.\n       \n4\n. \n        Select \nHome\n \n→\n \nAPI Explorer\n.\n       \n5\n. \n        In the \nFilter by kind\n field, type \nnotebook\n.\n       \n6\n. \n        Select the \nkubeflow v1\n notebook.\n       \n7\n. \n        Select the \nInstances\n tab and then select the instance for the workbench that you\n \ncreated in Step 1.\n       \n8\n. \n        Click the \nYAML\n tab and then select \nActions\n \n→\n \nEdit Notebook\n.\n       \n9\n. \n        Edit the YAML file to add the following information to the configuration:\n       \nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n28",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 32
      },
      "page_content": "          For the container that has the name of your Workbench notebook, add the\n \nfollowing lines to the \nvolumeMounts\n section:\n         \n- mountPath: /dev/shm\n  name: shm\n          For example, if your workbench name is \nmyworkbench\n, update the YAML file\n \nas follows:\n         \nspec:\n    containers:\n      - env\n        ...\n        name: myworkbench\n        ...\n         volumeMounts:\n         - mountPath: /dev/shm\n           name: shm\n          In the volumes section, add the lines shown in the following example:\n         \n     volumes:\n       name: shm\n       emptyDir:\n         medium: Memory\n          \nNote:\n Optionally, you can specify a limit to the amount of memory to use for the\n \nemptyDir\n.\n         \n10\n. \n        Click \nSave\n.\n       \n11\n. \n        In the data science dashboard, in the \nWorkbenches\n section of the data science\n \nproject, click the \nStatus\n toggle for the workbench. The status changes from \nStopped\n to\n \nStarting\n and then \nRunning\n.\n       \n12\n. \n        Restart the notebook.\n       \n \nCHAPTER 6. KNOWN ISSUES\n29",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 33
      },
      "page_content": "WARNING\n    If you later edit the notebook’s configuration through the Data Science\n \ndashboard UI, your workaround edit to the notebook configuration will be erased.\n   \n   \nRHODS-8865\n - A pipeline server fails to start unless you specify an Amazon Web Services (AWS)\n \nSimple Storage Service (S3) bucket resource\n  \n   When you create a data connection for a data science project, the \nAWS_S3_BUCKET\n field is not\n \ndesignated as a mandatory field. However, if you do not specify a value for this field, and you attempt\n \nto configure a pipeline server, the pipeline server fails to start successfully.\n  \n   \nRHODS-6907\n - Attempting to increase the size of a Persistent Volume (PV) fails when it is not\n \nconnected to a workbench\n  \n   Attempting to increase the size of a Persistent Volume (PV) that is not connected to a workbench\n \nfails. When changing a data science project’s storage, users can still edit the size of the PV in the\n \nuser interface, but this action does not have any effect.\n  \n   \nRHODS-6950\n - Unable to scale down a workbench’s GPUs when all GPUs in the cluster are being\n \nused\n  \n   It is not possible to scale down a workbench’s GPUs if all GPUs in the cluster are being used. This\n \nissue applies to GPUs being used by one workbench, and GPUs being used by multiple\n \nworkbenches.\n  \nWorkaround\n      To workaround around this issue, perform the following steps:\n     \n1\n. \n        Stop all active workbenches that are using GPUs.\n       \n2\n. \n        Wait until the relevant GPUs are available again.\n       \n\nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n30",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 34
      },
      "page_content": "3\n. \n        Edit the workbench and scale down the GPU instances.\n       \n \n   \nRHODS-6539\n - Anaconda Professional Edition cannot be validated and enabled in OpenShift AI\n  \n   Anaconda Professional Edition cannot be enabled as the dashboard’s key validation for Anaconda\n \nProfessional Edition is inoperable.\n  \n   \nRHODS-6346\n - Unclear error message displays when using invalid characters to create a data\n \nscience project\n  \n   When creating a data science project’s data connection, workbench, or storage connection using\n \ninvalid special characters, the following error message is displayed:\n  \nthe object provided is unrecognized (must be of type Secret): couldn't get version/kind; json parse\n \nerror: unexpected end of JSON input ({\"apiVersion\":\"v1\",\"kind\":\"Sec ...)\n   The error message fails to clearly indicate the problem.\n  \n   \nRHODS-6913\n - When editing the configuration settings of a workbench, a misleading error\n \nmessage appears\n  \n   When you edit the configuration settings of a workbench, a warning message appears stating the\n \nworkbench will restart if you make any changes to its configuration settings. This warning is\n \nmisleading, as if you change the values of its environment variables, the workbench does not\n \nautomatically restart.\n  \n   \nRHODS-6373\n - Workbenches fail to start when cumulative character limit is exceeded\n  \n   When the cumulative character limit of a data science project’s title and workbench title exceeds 62\n \ncharacters, workbenches fail to start.\n  \n   \nRHODS-6216\n - The ModelMesh oauth-proxy container is intermittently unstable\n  \nCHAPTER 6. KNOWN ISSUES\n31",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-release_notes-en-us.pdf",
        "page": 35
      },
      "page_content": "   ModelMesh pods do not deploy correctly due to a failure of the ModelMesh \noauth-proxy\n container.\n \nThis issue occurs intermittently and only if authentication is enabled in the ModelMesh runtime\n \nenvironment. It is more likely to occur when additional ModelMesh instances are deployed in different\n \nnamespaces.\n  \n   \nRHODS-4769\n - GPUs on nodes with unsupported taints cannot be allocated to notebook servers\n  \n   GPUs on nodes marked with any taint other than the supported \nnvidia.com/gpu\n taint cannot be\n \nselected when creating a notebook server. To avoid this issue, use only the \nnvidia.com/gpu\n taint on\n \nGPU nodes used with OpenShift AI.\n  \nRed Hat OpenShift AI Self-Managed 2.8 Release notes\n32",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 0
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\nServing models\nServe models in Red Hat OpenShift AI Self-Managed\nLast Updated: 2024-04-26",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 1
      },
      "page_content": "",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 2
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\n \nServing models\nServe models in Red Hat OpenShift AI Self-Managed",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 3
      },
      "page_content": "Legal Notice\nCopyright \n©\n 2024 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nServe models in Red Hat OpenShift AI Self-Managed. Serving trained models enables you to test\nand implement them into intelligent applications.",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 4
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. ABOUT MODEL SERVING\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n2.1. CONFIGURING MODEL SERVERS\n2.1.1. Enabling the multi-model serving platform\n2.1.2. Adding a custom model-serving runtime for the multi-model serving platform\n2.1.3. Adding a model server for the multi-model serving platform\n2.1.4. Deleting a model server\n2.2. WORKING WITH DEPLOYED MODELS\n2.2.1. Deploying a model by using the multi-model serving platform\n2.2.2. Viewing a deployed model\n2.2.3. Updating the deployment properties of a deployed model\n2.2.4. Deleting a deployed model\n2.3. CONFIGURING MONITORING FOR THE MULTI-MODEL SERVING PLATFORM\n2.4. VIEWING METRICS FOR THE MULTI-MODEL SERVING PLATFORM\nCHAPTER 3. SERVING LARGE MODELS\n3.1. ABOUT THE SINGLE MODEL SERVING PLATFORM\n3.2. CONFIGURING AUTOMATED INSTALLATION OF KSERVE\n3.3. MANUALLY INSTALLING KSERVE\n3.3.1. Installing KServe dependencies\n3.3.1.1. Creating an OpenShift Service Mesh instance\n3.3.1.2. Creating a Knative Serving instance\n3.3.1.3. Creating secure gateways for Knative Serving\n3.3.2. Installing KServe\n3.4. DEPLOYING MODELS BY USING THE SINGLE-MODEL SERVING PLATFORM\n3.4.1. Enabling the single model serving platform\n3.4.2. Adding a custom model-serving runtime for the single-model serving platform\n3.4.3. Deploying models on the single model serving platform\n3.4.4. Accessing the inference endpoints for models deployed on the single model serving platform\n3.5. CONFIGURING MONITORING FOR THE SINGLE-MODEL SERVING PLATFORM\n3.6. VIEWING METRICS FOR THE SINGLE MODEL SERVING PLATFORM\nCHAPTER 4. MONITORING MODEL PERFORMANCE\n4.1. VIEWING PERFORMANCE METRICS FOR ALL MODELS ON A MODEL SERVER\n4.2. VIEWING HTTP REQUEST METRICS FOR A DEPLOYED MODEL\n3\n4\n4\n4\n4\n7\n8\n9\n9\n11\n11\n12\n13\n14\n16\n16\n17\n20\n21\n21\n23\n26\n30\n31\n31\n32\n33\n36\n37\n39\n41\n41\n42\nTable of Contents\n1",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 5
      },
      "page_content": "Red Hat OpenShift AI Self-Managed 2.8 Serving models\n2",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 6
      },
      "page_content": "CHAPTER 1. ABOUT MODEL SERVING\nServing trained models on Red Hat OpenShift AI means deploying the models on your OpenShift cluster\nto test and then integrate them into intelligent applications. Deploying a model makes it available as a\nservice that you can access by using an API. This enables you to return predictions based on data inputs\nthat you provide through API calls. This process is known as model \ninferencing\n. When you serve a model\non OpenShift AI, the inference endpoints that you can access for the deployed model are shown in the\ndashboard.\nOpenShift AI provides the following model serving platforms:\nSingle model serving platform\nFor deploying large models such as large language models (LLMs), OpenShift AI includes a \nsingle\nmodel serving platform\n that is based on the \nKServe\n component. Because each model is deployed\nfrom its own model server, the single model serving platform helps you to deploy, monitor, scale, and\nmaintain large models that require increased resources.\nMulti-model serving platform\nFor deploying small and medium-sized models, OpenShift AI includes a \nmulti-model serving platform\nthat is based on the \nModelMesh\n component. On the multi-model serving platform, you can deploy\nmultiple models on the same model server. Each of the deployed models shares the server resources.\nThis approach can be advantageous on OpenShift clusters that have finite compute resources or\npods.\nCHAPTER 1. ABOUT MODEL SERVING\n3",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 7
      },
      "page_content": "CHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\nFor deploying small and medium-sized models, OpenShift AI includes a \nmulti-model serving platform\nthat is based on the ModelMesh component. On the multi-model serving platform, multiple models can\nbe deployed from the same model server and share the server resources.\n2.1. CONFIGURING MODEL SERVERS\n2.1.1. Enabling the multi-model serving platform\nTo use the multi-model serving platform, you must first enable the platform.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the admin group (for example, \nrhoai-admins\n) in OpenShift.\nProcedure\n1\n. \nIn the left menu of the OpenShift AI dashboard, click \nSettings\n \n→\n \nCluster settings\n.\n2\n. \nLocate the \nModel serving platforms\n section.\n3\n. \nSelect the \nMulti-model serving platform\n checkbox.\n4\n. \nClick \nSave changes\n.\n2.1.2. Adding a custom model-serving runtime for the multi-model serving platform\nA model-serving runtime adds support for a specified set of model frameworks (that is, formats). By\ndefault, the multi-model serving platform includes the OpenVINO Model Server runtime. However, if\nthis runtime doesn’t meet your needs (it doesn’t support a particular model format, for example), you\ncan add your own, custom runtime.\nAs an administrator, you can use the Red Hat OpenShift AI dashboard to add and enable a custom\nmodel-serving runtime. You can then choose the custom runtime when you create a new model server\nfor the multi-model serving platform.\nNOTE\nOpenShift AI enables you to add your own custom runtimes, but does not support the\nruntimes themselves. You are responsible for correctly configuring and maintaining\ncustom runtimes. You are also responsible for ensuring that you are licensed to use any\ncustom runtimes that you add.\nPrerequisites\nYou have logged in to OpenShift AI as an administrator.\nYou are familiar with how to \nadd a model server to your project\n. When you have added a custom\nmodel-serving runtime, you must configure a new model server to use the runtime.\nYou have reviewed the example runtimes in the \nkserve/modelmesh-serving\n repository. You can\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n4",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 8
      },
      "page_content": "You have reviewed the example runtimes in the \nkserve/modelmesh-serving\n repository. You can\nuse these examples as starting points. However, each runtime requires some further\nmodification before you can deploy it in OpenShift AI. The required modifications are described\nin the following procedure.\nNOTE\nOpenShift AI includes the OpenVINO Model Server runtime by default. You do\nnot need to add this runtime to OpenShift AI.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n > \nServing runtimes\n.\nThe \nServing runtimes\n page opens and shows the model-serving runtimes that are already\ninstalled and enabled.\n2\n. \nTo add a custom runtime, choose one of the following options:\nTo start with an existing runtime (for example the OpenVINO Model Server runtime), click\nthe action menu (\n⋮\n) next to the existing runtime and then click \nDuplicate\n.\nTo add a new custom runtime, click \nAdd serving runtime\n.\n3\n. \nIn the \nSelect the model serving platforms this runtime supports\n list, select \nMulti-model\nserving platform\n.\nNOTE\nThe multi-model serving platform supports only the REST protocol. Therefore,\nyou cannot change the default value in the \nSelect the API protocol this runtime\nsupports\n list.\n4\n. \nOptional: If you started a new runtime (rather than duplicating an existing one), add your code\nby choosing one of the following options:\nUpload a YAML file\na\n. \nClick \nUpload files\n.\nb\n. \nIn the file browser, select a YAML file on your computer. This file might be the one of\nthe example runtimes that you downloaded from the \nkserve/modelmesh-serving\nrepository.\nThe embedded YAML editor opens and shows the contents of the file that you\nuploaded.\nEnter YAML code directly in the editor\na\n. \nClick \nStart from scratch\n.\nb\n. \nEnter or paste YAML code directly in the embedded editor. The YAML that you paste\nmight be copied from one of the example runtimes in the \nkserve/modelmesh-serving\nrepository.\n5\n. \nOptional: If you are adding one of the example runtimes in the \nkserve/modelmesh-serving\nrepository, perform the following modifications:\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n5",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 9
      },
      "page_content": "a\n. \nIn the YAML editor, locate the \nkind\n field for your runtime. Update the value of this field to \nServingRuntime\n.\nb\n. \nIn the \nkustomization.yaml\n file in the \nkserve/modelmesh-serving\n repository, take note of the \nnewName\n and \nnewTag\n values for the runtime that you want to add. You will specify these\nvalues in a later step.\nc\n. \nIn the YAML editor for your custom runtime, locate the \ncontainers.image\n field.\nd\n. \nUpdate the value of the \ncontainers.image\n field in the format \nnewName:newTag\n, based on\nthe values that you previously noted in the \nkustomization.yaml\n file. Some examples are\nshown.\nNvidia Triton Inference Server\nimage: nvcr.io/nvidia/tritonserver:23.04-py3\nSeldon Python MLServer\nimage: seldonio/mlserver:1.3.2\nTorchServe\nimage: pytorch/torchserve:0.7.1-cpu\n6\n. \nIn the \nmetadata.name\n field, ensure that the value of the runtime you are adding is unique (that\nis, the value doesn’t match a runtime that you have already added).\n7\n. \nOptional: To configure a custom display name for the runtime that you are adding, add a \nmetadata.annotations.openshift.io/display-name\n field and specify a value, as shown in the\nfollowing example:\napiVersion: serving.kserve.io/v1alpha1\nkind: ServingRuntime\nmetadata:\n  name: mlserver-0.x\n  annotations:\n    openshift.io/display-name: MLServer\nNOTE\nIf you do not configure a custom display name for your runtime, OpenShift AI\nshows the value of the \nmetadata.name\n field.\n8\n. \nClick \nAdd\n.\nThe \nServing runtimes\n page opens and shows the updated list of runtimes that are installed.\nObserve that the runtime you added is automatically enabled.\n9\n. \nOptional: To edit your custom runtime, click the action menu (\n⋮\n) and select \nEdit\n.\nVerification\nThe custom model-serving runtime that you added is shown in an enabled state on the \nServing\nruntimes\n page.\nAdditional resources\nTo learn how to configure a model server that uses a custom model-serving runtime that you\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n6",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 10
      },
      "page_content": "To learn how to configure a model server that uses a custom model-serving runtime that you\nhave added, see \nAdding a model server to your data science project\n.\n2.1.3. Adding a model server for the multi-model serving platform\nWhen you have enabled the multi-model serving platform, you must configure a model server to deploy\nmodels. If you require extra computing power for use with large datasets, you can assign accelerators to\nyour model server.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use specialized OpenShift AI groups, you are part of the user group or admin group (for\nexample, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that you can add a model server to.\nYou have enabled the multi-model serving platform.\nIf you want to use a custom model-serving runtime for your model server, you have added and\nenabled the runtime. See \nAdding a custom model-serving runtime\n.\nIf you want to use graphics processing units (GPUs) with your model server, you have enabled\nGPU support in OpenShift AI. See \nEnabling GPU support in OpenShift AI\n.\nProcedure\n1\n. \nIn the left menu of the OpenShift AI dashboard, click \nData Science Projects\n.\n2\n. \nClick the name of the project that you want to configure a model server for.\nA project details page opens.\n3\n. \nIn the \nModels and model servers\n section, perform one of the following actions:\nIf you see a \n​Multi-model serving platform\n tile, click \nAdd model server\n on the tile.\nIf you do not see any tiles, click the \nAdd model server\n button.\nThe \nAdd model server\n dialog opens.\n4\n. \nIn the \nModel server name\n field, enter a unique name for the model server.\n5\n. \nFrom the \nServing runtime\n list, select a model-serving runtime that is installed and enabled in\nyour OpenShift AI deployment.\nNOTE\nIf you are using a \ncustom\n model-serving runtime with your model server and want\nto use GPUs, you must ensure that your custom runtime supports GPUs and is\nappropriately configured to use them.\n6\n. \nIn the \nNumber of model replicas to deploy\n field, specify a value.\n7\n. \nFrom the \nModel server size\n list, select a value.\n8\n. \nOptional: If you selected \nCustom\n in the preceding step, configure the following settings in the\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n7",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 11
      },
      "page_content": "8\n. \nOptional: If you selected \nCustom\n in the preceding step, configure the following settings in the\nModel server size\n section to customize your model server:\na\n. \nIn the \nCPUs requested\n field, specify the number of CPUs to use with your model server.\nUse the list beside this field to specify the value in cores or millicores.\nb\n. \nIn the \nCPU limit\n field, specify the maximum number of CPUs to use with your model server.\nUse the list beside this field to specify the value in cores or millicores.\nc\n. \nIn the \nMemory requested\n field, specify the requested memory for the model server in\ngibibytes (Gi).\nd\n. \nIn the \nMemory limit\n field, specify the maximum memory limit for the model server in\ngibibytes (Gi).\n9\n. \nOptional: From the \nAccelerator\n list, select an accelerator.\na\n. \nIf you selected an accelerator in the preceding step, specify the number of accelerators to\nuse.\n10\n. \nOptional: In the \nModel route\n section, select the \nMake deployed models available through an\nexternal route\n checkbox to make your deployed models available to external clients.\n11\n. \nOptional: In the \nToken authorization\n section, select the \nRequire token authentication\ncheckbox to require token authentication for your model server. To finish configuring token\nauthentication, perform the following actions:\na\n. \nIn the \nService account name\n field, enter a service account name for which the token will be\ngenerated. The generated token is created and displayed in the \nToken secret\n field when\nthe model server is configured.\nb\n. \nTo add an additional service account, click \nAdd a service account\n and enter another service\naccount name.\n12\n. \nClick \nAdd\n.\nThe model server that you configured appears in the \nModels and model servers\n section of\nthe project details page.\n13\n. \nOptional: To update the model server, click the action menu (\n⋮\n) beside the model server and\nselect \nEdit model server\n.\n2.1.4. Deleting a model server\nWhen you no longer need a model server to host models, you can remove it from your data science\nproject.\nNOTE\nWhen you remove a model server, you also remove the models that are hosted on that\nmodel server. As a result, the models are no longer available to applications.\nPrerequisites\nYou have created a data science project and an associated model server.\nYou have notified the users of the applications that access the models that the models will no\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n8",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 12
      },
      "page_content": "You have notified the users of the applications that access the models that the models will no\nlonger be available.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project from which you want to delete the model server.\nA project details page opens.\n3\n. \nClick the action menu (\n⋮\n) beside the project whose model server you want to delete in the\nModels and model servers\n section and then click \nDelete model server\n.\nThe \nDelete model server\n dialog opens.\n4\n. \nEnter the name of the model server in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete model server\n.\nVerification\nThe model server that you deleted is no longer displayed in the \nModels and model servers\nsection on the project details page.\n2.2. WORKING WITH DEPLOYED MODELS\n2.2.1. Deploying a model by using the multi-model serving platform\nYou can deploy trained models on OpenShift AI to enable you to test and implement them into\nintelligent applications. Deploying a model makes it available as a service that you can access by using an\nAPI. This enables you to return predictions based on data inputs.\nWhen you have enabled the multi-model serving platform, you can deploy models on the platform.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n) in OpenShift.\nYou have enabled the multi-model serving platform.\nYou have created a data science project and added a model server.\nYou have access to S3-compatible object storage.\nFor the model that you want to deploy, you know the associated folder path in your S3-\ncompatible object storage bucket.\nProcedure\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n9",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 13
      },
      "page_content": "1\n. \nIn the left menu of the OpenShift AI dashboard, click \nData Science Projects\n.\n2\n. \nClick the name of the project that you want to deploy a model in.\nA project details page opens.\n3\n. \nIn the \nModels and model servers\n section, click \nDeploy model\n.\n4\n. \nConfigure properties for deploying your model as follows:\na\n. \nIn the \nModel name\n field, enter a unique name for the model that you are deploying.\nb\n. \nFrom the \nModel framework\n list, select a framework for your model.\nNOTE\nThe \nModel framework\n list shows only the frameworks that are supported by\nthe model-serving runtime that you specified when you configured your\nmodel server.\nc\n. \nTo specify the location of the model you want to deploy from S3-compatible object\nstorage, perform one of the following sets of actions:\nTo use an existing data connection\ni\n. \nSelect \nExisting data connection\n.\nii\n. \nFrom the \nName\n list, select a data connection that you previously defined.\niii\n. \nIn the \nPath\n field, enter the folder path that contains the model in your specified\ndata source.\nTo use a new data connection\ni\n. \nTo define a new data connection that your model can access, select \nNew data\nconnection\n.\nii\n. \nIn the \nName\n field, enter a unique name for the data connection.\niii\n. \nIn the \nAccess key\n field, enter the access key ID for the S3-compatible object\nstorage provider.\niv\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object\nstorage account that you specified.\nv\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage\nbucket.\nvi\n. \nIn the \nRegion\n field, enter the default region of your S3-compatible object storage\naccount.\nvii\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nviii\n. \nIn the \nPath\n field, enter the folder path in your S3-compatible object storage that\ncontains your data file.\nd\n. \nClick \nDeploy\n.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n10",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 14
      },
      "page_content": "Verification\nConfirm that the deployed model is shown in the \nModels and model servers\n section of your\nproject, and on the \nModel Serving\n page of the dashboard with a checkmark in the \nStatus\ncolumn.\n2.2.2. Viewing a deployed model\nTo analyze the results of your work, you can view a list of deployed models on Red Hat OpenShift AI. You\ncan also view the current statuses of deployed models and their endpoints.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nModel Serving\n.\nThe \nDeployed models\n page opens.\nFor each model, the page shows details such as the model name, the project in which the model\nis deployed, the serving runtime that the model uses, and the deployment status.\n2\n. \nOptional: For a given model, click the link in the \nInference endpoint\n column to see the\ninference endpoints for the deployed model.\nVerification\nA list of previously deployed data science models is displayed on the \nDeployed models\n page.\n2.2.3. Updating the deployment properties of a deployed model\nYou can update the deployment properties of a model that has been deployed previously. This allows\nyou to change the model’s data connection and name.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have deployed a model on OpenShift AI.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nModel serving\n.\nThe \nDeployed models\n page opens.\n2\n. \nClick the action menu (\n⋮\n) beside the model whose deployment properties you want to update\nand click \nEdit\n.\nThe \nDeploy model\n dialog opens.\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n11",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 15
      },
      "page_content": "3\n. \nUpdate the deployment properties of the model as follows:\na\n. \nIn the \nModel Name\n field, enter a new, unique name for the model.\nb\n. \nFrom the \nModel framework\n list, select a framework for your model.\nNOTE\nThe \nModel framework\n list shows only the frameworks that are supported by\nthe model-serving runtime that you specified when you configured your\nmodel server.\nc\n. \nTo update how you have specified the location of your model, perform one of the following\nsets of actions:\nIf you previously specified an existing data connection\ni\n. \nIn the \nPath\n field, update the folder path that contains the model in your specified\ndata source.\nIf you previously specified a new data connection\ni\n. \nIn the \nName\n field, update a unique name for the data connection.\nii\n. \nIn the \nAccess key\n field, update the access key ID for the S3-compatible object\nstorage provider.\niii\n. \nIn the \nSecret key\n field, update the secret access key for the S3-compatible object\nstorage account that you specified.\niv\n. \nIn the \nEndpoint\n field, update the endpoint of your S3-compatible object storage\nbucket.\nv\n. \nIn the \nRegion\n field, update the default region of your S3-compatible object storage\naccount.\nvi\n. \nIn the \nBucket\n field, update the name of your S3-compatible object storage bucket.\nvii\n. \nIn the \nPath\n field, update the folder path in your S3-compatible object storage that\ncontains your data file.\nd\n. \nClick \nDeploy\n.\nVerification\nThe model whose deployment properties you updated is displayed on the \nModel Serving\n page\nof the dashboard.\n2.2.4. Deleting a deployed model\nYou can delete models you have previously deployed. This enables you to remove deployed models that\nare no longer required.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n12",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 16
      },
      "page_content": "If you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have deployed a model.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nModel serving\n.\nThe \nDeployed models\n page opens.\n2\n. \nClick the action menu (\n⋮\n) beside the deployed model that you want to delete and click \nDelete\n.\nThe \nDelete deployed model\n dialog opens.\n3\n. \nEnter the name of the deployed model in the text field to confirm that you intend to delete it.\n4\n. \nClick \nDelete deployed model\n.\nVerification\nThe model that you deleted is no longer displayed on the \nDeployed models\n page.\n2.3. CONFIGURING MONITORING FOR THE MULTI-MODEL SERVING\nPLATFORM\nThe multi-model serving platform includes metrics for the ModelMesh component. When you have\nconfigured monitoring, you can grant Prometheus access to scrape the available metrics.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYou have downloaded and installed the OpenShift command-line interface (CLI). See \nInstalling\nthe OpenShift CLI\n.\nYou are familiar with \ncreating a config map\n for monitoring a user-defined workflow. You will\nperform similar steps in this procedure.\nYou are familiar with \nenabling monitoring\n for user-defined projects in OpenShift. You will\nperform similar steps in this procedure.\nYou have \nassigned\n the \nmonitoring-rules-view\n role to users that will monitor metrics.\nProcedure\n1\n. \nIn a terminal window, if you are not already logged in to your OpenShift cluster as a cluster\nadministrator, log in to the OpenShift CLI as shown in the following example:\n$ oc login \n<openshift_cluster_url>\n -u \n<admin_username>\n -p \n<password>\n2\n. \nDefine a \nConfigMap\n object in a YAML file called \nuwm-cm-conf.yaml\n with the following\ncontents:\napiVersion: v1\nkind: ConfigMap\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n13",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 17
      },
      "page_content": "metadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d\nThe \nuser-workload-monitoring-config\n object configures the components that monitor user-\ndefined projects. Observe that the retention time is set to the recommended value of 15 days.\n3\n. \nApply the configuration to create the \nuser-workload-monitoring-config\n object.\n$ oc apply -f uwm-cm-conf.yaml\n4\n. \nDefine another \nConfigMap\n object in a YAML file called \nuwm-cm-enable.yaml\n with the\nfollowing contents:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true\nThe \ncluster-monitoring-config\n object enables monitoring for user-defined projects.\n5\n. \nApply the configuration to create the \ncluster-monitoring-config\n object.\n$ oc apply -f uwm-cm-enable.yaml\n2.4. VIEWING METRICS FOR THE MULTI-MODEL SERVING PLATFORM\nWhen a cluster administrator has configured monitoring for the multi-model serving platform, non-\nadmin users can use the OpenShift web console to view metrics.\nPrerequisites\nA cluster administrator has configured monitoring for the multi-model serving platform.\nYou have been \nassigned\n the \nmonitoring-rules-view\n role.\nYou are familiar with how to \nmonitor project metrics\n in the OpenShift Container Platform web\nconsole.\nProcedure\n1\n. \nLog in to the OpenShift Container Platform web console.\n2\n. \nSwitch to the \nDeveloper\n perspective.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n14",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 18
      },
      "page_content": "3\n. \nIn the left menu, click \nObserve\n.\n4\n. \nAs described in \nmonitoring project metrics\n, use the web console to run queries for \nmodelmesh_*\n metrics.\nCHAPTER 2. SERVING SMALL AND MEDIUM-SIZED MODELS\n15",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 19
      },
      "page_content": "CHAPTER 3. SERVING LARGE MODELS\nFor deploying large models such as large language models (LLMs), Red Hat OpenShift AI includes a\nsingle model serving platform\n that is based on the KServe component. Because each model is deployed\nfrom its own model server, the single model serving platform helps you to deploy, monitor, scale, and\nmaintain large models that require increased resources.\n3.1. ABOUT THE SINGLE MODEL SERVING PLATFORM\nThe single model serving platform consists of the following components:\nKServe\n: A Kubernetes custom resource definition (CRD) that orchestrates model serving for all\ntypes of models. It includes model-serving runtimes that implement the loading of given types\nof model servers. KServe handles the lifecycle of the deployment object, storage access, and\nnetworking setup.\nRed Hat OpenShift Serverless\n: A cloud-native development model that allows for serverless\ndeployments of models. OpenShift Serverless is based on the open source \nKnative\n project.\nRed Hat OpenShift Service Mesh\n: A service mesh networking layer that manages traffic flows\nand enforces access policies. OpenShift Service Mesh is based on the open source \nIstio\n project.\nTo install the single model serving platform, you have the following options:\nAutomated installation\nIf you have not already created a \nServiceMeshControlPlane\n or \nKNativeServing\n resource on your\nOpenShift cluster, you can configure the Red Hat OpenShift AI Operator to install KServe and its\ndependencies.\nManual installation\nIf you have already created a \nServiceMeshControlPlane\n or \nKNativeServing\n resource on your\nOpenShift cluster, you \ncannot\n configure the Red Hat OpenShift AI Operator to install KServe and its\ndependencies. In this situation, you must install KServe manually.\nWhen you have installed KServe, you can use the OpenShift AI dashboard to deploy models using pre-\ninstalled or custom model-serving runtimes.\nOpenShift AI includes the following pre-installed runtimes for KServe:\nA standalone TGIS runtime\nA composite Caikit-TGIS runtime\nOpenVINO Model Server\nNOTE\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n16",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 20
      },
      "page_content": "NOTE\nText Generation Inference Server (TGIS)\n is based on an early fork of \nHugging\nFace TGI\n. Red Hat will continue to develop the standalone TGIS runtime to\nsupport TGI models. If a model does not work in the current version of OpenShift\nAI, support might be added in a future version. In the meantime, you can also add\nyour own, custom runtime to support a TGI model. For more information, see\nAdding a custom model-serving runtime for the single model serving platform\n.\nThe composite Caikit-TGIS runtime is based on \nCaikit\n and \nText Generation\nInference Server (TGIS)\n. To use this runtime, you must convert your models to\nCaikit format. For an example, see \nConverting Hugging Face Hub models to\nCaikit format\n in the \ncaikit-tgis-serving\n repository.\nYou can also configure monitoring for the single model serving platform and use Prometheus to scrape\nthe available metrics.\n3.2. CONFIGURING AUTOMATED INSTALLATION OF KSERVE\nIf you have not already created a \nServiceMeshControlPlane\n or \nKNativeServing\n resource on your\nOpenShift cluster, you can configure the Red Hat OpenShift AI Operator to install KServe and its\ndependencies.\nIMPORTANT\nIf you have created a \nServiceMeshControlPlane\n or \nKNativeServing\n resource on your\ncluster, the Red Hat OpenShift AI Operator cannot install KServe and its dependencies\nand the installation does not proceed. In this situation, you must follow the manual\ninstallation instructions to install KServe.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYour cluster has a node with 4 CPUs and 16 GB memory.\nYou have downloaded and installed the OpenShift command-line interface (CLI). For more\ninformation, see \nInstalling the OpenShift CLI\n.\nYou have \ninstalled\n the Red Hat OpenShift Service Mesh Operator and dependent Operators.\nNOTE\nTo enable automated installation of KServe, install \nonly\n the required Operators\nfor Red Hat OpenShift Service Mesh. Do not perform any additional\nconfiguration or create a \nServiceMeshControlPlane\n resource.\nYou have \ninstalled\n the Red Hat OpenShift Serverless Operator.\nNOTE\nTo enable automated installation of KServe, install \nonly\n the Red Hat OpenShift\nServerless Operator. Do not perform any additional configuration or create a \nKNativeServing\n resource.\nCHAPTER 3. SERVING LARGE MODELS\n17",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 21
      },
      "page_content": "You have \ninstalled\n the Red Hat OpenShift AI Operator and \ncreated\n a \nDataScienceCluster\nobject.\nProcedure\n1\n. \nLog in to the OpenShift web console as a cluster administrator.\n2\n. \nIn the web console, click \nOperators\n \n→\n \nInstalled Operators\n and then click the Red Hat\nOpenShift AI Operator.\n3\n. \nInstall OpenShift Service Mesh as follows:\na\n. \nClick the \nDSC Initialization\n tab.\nb\n. \nClick the \ndefault-dsci\n object.\nc\n. \nClick the \nYAML\n tab.\nd\n. \nIn the \nspec\n section, validate that the value of the \nmanagementState\n field for the \nserviceMesh\n component is set to \nManaged\n, as shown:\nspec:\n applicationsNamespace: redhat-ods-applications\n monitoring:\n   managementState: Managed\n   namespace: redhat-ods-monitoring\n serviceMesh:\n   controlPlane:\n     metricsCollection: Istio\n     name: data-science-smcp\n     namespace: istio-system\n   managementState: Managed\nNOTE\nDo not change the \nistio-system\n namespace that is specified for the \nserviceMesh\n component by default. Other namespace values are not\nsupported.\ne\n. \nClick \nSave\n.\nBased on the configuration you added to the \nDSCInitialization\n object, the Red Hat\nOpenShift AI Operator installs OpenShift Service Mesh.\n4\n. \nInstall both KServe and OpenShift Serverless as follows:\na\n. \nIn the web console, click \nOperators\n \n→\n \nInstalled Operators\n and then click the Red Hat\nOpenShift AI Operator.\nb\n. \nClick the \nData Science Cluster\n tab.\nc\n. \nClick the \ndefault-dsc\n DSC object.\nd\n. \nClick the \nYAML\n tab.\ne\n. \nIn the \nspec.components\n section, configure the \nkserve\n component as shown.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n18",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 22
      },
      "page_content": "spec:\n components:\n   kserve:\n     managementState: Managed\n     serving:\n       ingressGateway:\n         certificate:\n           secretName: knative-serving-cert\n           type: SelfSigned\n       managementState: Managed\n       name: knative-serving\nf\n. \nClick \nSave\n.\nThe preceding configuration creates an ingress gateway for OpenShift Serverless to receive\ntraffic from OpenShift Service Mesh. In this configuration, observe the following details:\nThe configuration shown generates a self-signed certificate to secure incoming traffic\nto your OpenShift cluster and stores the certificate in the \nknative-serving-cert\n secret\nthat is specified in the \nsecretName\n field. To provide \nyour own\n certificate, update the\nvalue of the \nsecretName\n field to specify your secret name and change the value of the\ntype field to \nProvided\n.\nNOTE\nIf you provide your own certificate, the certificate must specify the\ndomain name used by the ingress controller of your OpenShift cluster.\nYou can check this value by running the following command:\n$ oc get ingresses.config.openshift.io cluster -o\n \njsonpath='{.spec.domain}'\nYou must set the value of the \nmanagementState\n field to \nManaged\n for both the \nkserve\n and \nserving\n components. Setting \nkserve.managementState\n to \nManaged\ntriggers automated installation of KServe. Setting \nserving.managementState\n to \nManaged\n triggers automated installation of OpenShift Serverless. However,\ninstallation of OpenShift Serverless will \nnot\n be triggered if \nkserve.managementState\n is\nnot also set to \nManaged\n.\nVerification\nVerify installation of OpenShift Service Mesh as follows:\nIn the web console, click \nWorkloads\n \n→\n \nPods\n.\nFrom the project list, select \nistio-system\n. This is the project in which OpenShift Service\nMesh is installed.\nConfirm that there are running pods for the service mesh control plane, ingress gateway,\nand egress gateway. These pods have the naming patterns shown in the following example:\nNAME                                          READY     STATUS    RESTARTS   AGE\nistio-egressgateway-7c46668687-fzsqj           1/1       Running   0          22h\nistio-ingressgateway-77f94d8f85-fhsp9          1/1       Running   0          22h\nistiod-data-science-smcp-cc8cfd9b8-2rkg4      1/1       Running   0          22h\nCHAPTER 3. SERVING LARGE MODELS\n19",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 23
      },
      "page_content": "Verify installation of OpenShift Serverless as follows:\nIn the web console, click \nWorkloads\n \n→\n \nPods\n.\nFrom the project list, select \nknative-serving\n. This is the project in which OpenShift\nServerless is installed.\nConfirm that there are numerous running pods in the \nknative-serving\n project, including\nactivator, autoscaler, controller, and domain mapping pods, as well as pods for the Knative\nIstio controller (which controls the integration of OpenShift Serverless and OpenShift\nService Mesh). An example is shown.\nNAME                                      READY     STATUS    RESTARTS  AGE\nactivator-7586f6f744-nvdlb                2/2       Running   0         22h\nactivator-7586f6f744-sd77w                2/2       Running   0         22h\nautoscaler-764fdf5d45-p2v98              2/2       Running   0         22h\nautoscaler-764fdf5d45-x7dc6               2/2       Running   0         22h\nautoscaler-hpa-7c7c4cd96d-2lkzg           1/1       Running   0         22h\nautoscaler-hpa-7c7c4cd96d-gks9j          1/1       Running   0         22h\ncontroller-5fdfc9567c-6cj9d               1/1       Running   0         22h\ncontroller-5fdfc9567c-bf5x7               1/1       Running   0         22h\ndomain-mapping-56ccd85968-2hjvp           1/1       Running   0         22h\ndomain-mapping-56ccd85968-lg6mw           1/1       Running   0         22h\ndomainmapping-webhook-769b88695c-gp2hk    1/1       Running   0         22h\ndomainmapping-webhook-769b88695c-npn8g    1/1       Running   0         22h\nnet-istio-controller-7dfc6f668c-jb4xk     1/1       Running   0         22h\nnet-istio-controller-7dfc6f668c-jxs5p     1/1       Running   0         22h\nnet-istio-webhook-66d8f75d6f-bgd5r        1/1       Running   0         22h\nnet-istio-webhook-66d8f75d6f-hld75       1/1       Running   0         22h\nwebhook-7d49878bc4-8xjbr                  1/1       Running   0         22h\nwebhook-7d49878bc4-s4xx4                  1/1       Running   0         22h\nVerify installation of KServe as follows:\nIn the web console, click \nWorkloads\n \n→\n \nPods\n.\nFrom the project list, select \nredhat-ods-applications\n.This is the project in which OpenShift\nAI components are installed, including KServe.\nConfirm that the project includes a running pod for the KServe controller manager, similar\nto the following example:\nNAME                                          READY   STATUS    RESTARTS   AGE\nkserve-controller-manager-7fbb7bccd4-t4c5g    1/1     Running   0          22h\nodh-model-controller-6c4759cc9b-cftmk         1/1     Running   0          129m\nodh-model-controller-6c4759cc9b-ngj8b         1/1     Running   0          129m\nodh-model-controller-6c4759cc9b-vnhq5         1/1     Running   0          129m\n3.3. MANUALLY INSTALLING KSERVE\nIf you have already installed the Red Hat OpenShift Service Mesh Operator and created a \nServiceMeshControlPlane\n resource \nor\n if you have installed the Red Hat OpenShift Serverless\nOperator and created a \nKNativeServing\n resource, the Red Hat OpenShift AI Operator cannot install\nKServe and its dependencies. In this situation, you must install KServe manually.\nIMPORTANT\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n20",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 24
      },
      "page_content": "IMPORTANT\nThe procedures in this section show how to perform a \nnew\n installation of KServe and its\ndependencies and are intended as a complete installation and configuration reference. If\nyou have already installed and configured OpenShift Service Mesh or OpenShift\nServerless, you might not need to follow all steps. If you are unsure about what updates to\napply to your existing configuration to use KServe, contact Red Hat Support.\n3.3.1. Installing KServe dependencies\nBefore you install KServe, you must install and configure some dependencies. Specifically, you must\ncreate Red Hat OpenShift Service Mesh and Knative Serving instances and then configure secure\ngateways for Knative Serving.\n3.3.1.1. Creating an OpenShift Service Mesh instance\nThe following procedure shows how to create a Red Hat OpenShift Service Mesh instance.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYour cluster has a node with 4 CPUs and 16 GB memory.\nYou have downloaded and installed the OpenShift command-line interface (CLI). See \nInstalling\nthe OpenShift CLI\n.\nYou have \ninstalled\n the Red Hat OpenShift Service Mesh Operator and dependent Operators.\nProcedure\n1\n. \nIn a terminal window, if you are not already logged in to your OpenShift cluster as a cluster\nadministrator, log in to the OpenShift CLI as shown in the following example:\n$ oc login \n<openshift_cluster_url>\n -u \n<admin_username>\n -p \n<password>\n2\n. \nCreate the required namespace for Red Hat OpenShift Service Mesh.\n$ oc create ns istio-system\nYou see the following output:\nnamespace/istio-system created\n3\n. \nDefine a \nServiceMeshControlPlane\n object in a YAML file named \nsmcp.yaml\n with the following\ncontents:\napiVersion: maistra.io/v2\nkind: ServiceMeshControlPlane\nmetadata:\n  name: minimal\n  namespace: istio-system\nspec:\n  tracing:\nCHAPTER 3. SERVING LARGE MODELS\n21",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 25
      },
      "page_content": "    type: None\n  addons:\n    grafana:\n      enabled: false\n    kiali:\n      name: kiali\n      enabled: false\n    prometheus:\n      enabled: false\n    jaeger:\n      name: jaeger\n  security:\n    dataPlane:\n      mtls: true\n    identity:\n      type: ThirdParty\n  techPreview:\n    meshConfig:\n      defaultConfig:\n        terminationDrainDuration: 35s\n  gateways:\n    ingress:\n      service:\n        metadata:\n          labels:\n            knative: ingressgateway\n  proxy:\n    networking:\n      trafficControl:\n        inbound:\n          excludedPorts:\n            - 8444\n            - 8022\nFor more information about the values in the YAML file, see the \nService Mesh control plane\nconfiguration reference\n.\n4\n. \nCreate the service mesh control plane.\n$ oc apply -f smcp.yaml\nVerification\nVerify creation of the service mesh instance as follows:\nIn the OpenShift CLI, enter the following command:\n$ oc get pods -n istio-system\nThe preceding command lists all running pods in the \nistio-system\n project. This is the\nproject in which OpenShift Service Mesh is installed.\nConfirm that there are running pods for the service mesh control plane, ingress gateway,\nand egress gateway. These pods have the following naming patterns:\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n22",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 26
      },
      "page_content": "NAME                                          READY   STATUS      RESTARTS    AGE\nistio-egressgateway-7c46668687-fzsqj          1/1     Running     0           22h\nistio-ingressgateway-77f94d8f85-fhsp9         1/1     Running     0           22h\nistiod-data-science-smcp-cc8cfd9b8-2rkg4      1/1     Running     0           22h\n3.3.1.2. Creating a Knative Serving instance\nThe following procedure shows how to install Knative Serving and then create an instance.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYour cluster has a node with 4 CPUs and 16 GB memory.\nYou have downloaded and installed the OpenShift command-line interface (CLI). See \nInstalling\nthe OpenShift CLI\n.\nYou have \ncreated\n a Red Hat OpenShift Service Mesh instance.\nYou have \ninstalled\n the Red Hat OpenShift Serverless Operator.\nProcedure\n1\n. \nIn a terminal window, if you are not already logged in to your OpenShift cluster as a cluster\nadministrator, log in to the OpenShift CLI as shown in the following example:\n$ oc login \n<openshift_cluster_url>\n -u \n<admin_username>\n -p \n<password>\n2\n. \nCheck whether the required project (that is, \nnamespace\n) for Knative Serving already exists.\n$ oc get ns knative-serving\nIf the project exists, you see output similar to the following example:\nNAME              STATUS   AGE\nknative-serving   Active   4d20h\n3\n. \nIf the \nknative-serving\n project \ndoesn’t\n already exist, create it.\n$ oc create ns knative-serving\nYou see the following output:\nnamespace/knative-serving created\n4\n. \nDefine a \nServiceMeshMember\n object in a YAML file called \ndefault-smm.yaml\n with the\nfollowing contents:\napiVersion: maistra.io/v1\nkind: ServiceMeshMember\nmetadata:\n  name: default\nCHAPTER 3. SERVING LARGE MODELS\n23",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 27
      },
      "page_content": "1\n  namespace: knative-serving\nspec:\n  controlPlaneRef:\n    namespace: istio-system\n    name: minimal\n5\n. \nCreate the \nServiceMeshMember\n object in the \nistio-system\n namespace.\n$ oc apply -f default-smm.yaml\nYou see the following output:\nservicemeshmember.maistra.io/default created\n6\n. \nDefine a \nKnativeServing\n object in a YAML file called \nknativeserving-istio.yaml\n with the\nfollowing contents:\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\n  annotations:\n    serverless.openshift.io/default-enable-http2: \"true\"\nspec:\n  workloads:\n    - name: net-istio-controller\n      env:\n        - container: controller\n          envVars:\n            - name: ENABLE_SECRET_INFORMER_FILTERING_BY_CERT_UID\n              value: 'true'\n    - annotations:\n        sidecar.istio.io/inject: \"true\" \n1\n        sidecar.istio.io/rewriteAppHTTPProbers: \"true\" \n2\n      name: activator\n    - annotations:\n        sidecar.istio.io/inject: \"true\"\n        sidecar.istio.io/rewriteAppHTTPProbers: \"true\"\n      name: autoscaler\n  ingress:\n    istio:\n      enabled: true\n  config:\n    features:\n      kubernetes.podspec-affinity: enabled\n      kubernetes.podspec-nodeselector: enabled\n      kubernetes.podspec-tolerations: enabled\nThe preceding file defines a custom resource (CR) for a \nKnativeServing\n object. The CR also\nadds the following actions to each of the activator and autoscaler pods:\nInjects an Istio sidecar to the pod. This makes the pod part of the service mesh.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n24",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 28
      },
      "page_content": "2\nEnables the Istio sidecar to rewrite the HTTP liveness and readiness probes for the pod.\nNOTE\nIf you configure a custom domain for a Knative service, you can use a TLS\ncertificate to secure the mapped service. To do this, you must create a TLS\nsecret, and then update the \nDomainMapping\n CR to use the TLS secret that you\nhave created. For more information, see \nSecuring a mapped service using a TLS\ncertificate\n in the Red Hat OpenShift Serverless documentation.\n7\n. \nCreate the \nKnativeServing\n object in the specified \nknative-serving\n namespace.\n$ oc apply -f knativeserving-istio.yaml\nYou see the following output:\nknativeserving.operator.knative.dev/knative-serving created\nVerification\nReview the default \nServiceMeshMemberRoll\n object in the \nistio-system\n namespace.\n$ oc describe smmr default -n istio-system\nIn the description of the \nServiceMeshMemberRoll\n object, locate the \nStatus.Members\n field\nand confirm that it includes the \nknative-serving\n namespace.\nVerify creation of the Knative Serving instance as follows:\nIn the OpenShift CLI, enter the following command:\n$ oc get pods -n knative-serving\nThe preceding command lists all running pods in the \nknative-serving\n project. This is the\nproject in which you created the Knative Serving instance.\nConfirm that there are numerous running pods in the \nknative-serving\n project, including\nactivator, autoscaler, controller, and domain mapping pods, as well as pods for the Knative\nIstio controller, which controls the integration of OpenShift Serverless and OpenShift\nService Mesh. An example is shown.\nNAME                                      READY       STATUS     RESTARTS    AGE\nactivator-7586f6f744-nvdlb                2/2         Running    0           22h\nactivator-7586f6f744-sd77w                2/2         Running    0           22h\nautoscaler-764fdf5d45-p2v98              2/2         Running    0           22h\nautoscaler-764fdf5d45-x7dc6               2/2         Running    0           22h\nautoscaler-hpa-7c7c4cd96d-2lkzg           1/1         Running    0           22h\nautoscaler-hpa-7c7c4cd96d-gks9j          1/1         Running    0           22h\ncontroller-5fdfc9567c-6cj9d               1/1         Running    0           22h\ncontroller-5fdfc9567c-bf5x7               1/1         Running    0           22h\ndomain-mapping-56ccd85968-2hjvp           1/1         Running    0           22h\ndomain-mapping-56ccd85968-lg6mw           1/1         Running    0           22h\ndomainmapping-webhook-769b88695c-gp2hk    1/1         Running     0           22h\nCHAPTER 3. SERVING LARGE MODELS\n25",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 29
      },
      "page_content": "domainmapping-webhook-769b88695c-npn8g    1/1         Running    0           22h\nnet-istio-controller-7dfc6f668c-jb4xk     1/1         Running    0           22h\nnet-istio-controller-7dfc6f668c-jxs5p     1/1         Running    0           22h\nnet-istio-webhook-66d8f75d6f-bgd5r        1/1         Running    0           22h\nnet-istio-webhook-66d8f75d6f-hld75       1/1         Running    0           22h\nwebhook-7d49878bc4-8xjbr                  1/1         Running    0           22h\nwebhook-7d49878bc4-s4xx4                  1/1         Running    0           22h\n3.3.1.3. Creating secure gateways for Knative Serving\nTo secure traffic between your Knative Serving instance and the service mesh, you must create secure\ngateways for your Knative Serving instance.\nThe following procedure shows how to use OpenSSL to generate a wildcard certificate and key and then\nuse them to create local and ingress gateways for Knative Serving.\nIMPORTANT\nIf you have your own wildcard certificate and key to specify when configuring the\ngateways, you can skip to step 11 of this procedure.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYou have downloaded and installed the OpenShift command-line interface (CLI). See \nInstalling\nthe OpenShift CLI\n.\nYou have \ncreated\n a Red Hat OpenShift Service Mesh instance.\nYou have \ncreated\n a Knative Serving instance.\nIf you intend to generate a wildcard certificate and key, you have \ndownloaded and installed\nOpenSSL.\nProcedure\n1\n. \nIn a terminal window, if you are not already logged in to your OpenShift cluster as a cluster\nadministrator, log in to the OpenShift CLI as shown in the following example:\n$ oc login \n<openshift_cluster_url>\n -u \n<admin_username>\n -p \n<password>\nIMPORTANT\nIf you have your own wildcard certificate and key to specify when configuring the\ngateways, skip to step 11 of this procedure.\n2\n. \nSet environment variables to define base directories for generation of a wildcard certificate and\nkey for the gateways.\n$ export BASE_DIR=/tmp/kserve\n$ export BASE_CERT_DIR=${BASE_DIR}/certs\n3\n. \nSet an environment variable to define the common name used by the ingress controller of your\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n26",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 30
      },
      "page_content": "3\n. \nSet an environment variable to define the common name used by the ingress controller of your\nOpenShift cluster.\n$ export COMMON_NAME=$(oc get ingresses.config.openshift.io cluster -o\n \njsonpath='{.spec.domain}' | awk -F'.' '{print $(NF-1)\".\"$NF}')\n4\n. \nSet an environment variable to define the domain name used by the ingress controller of your\nOpenShift cluster.\n$ export DOMAIN_NAME=$(oc get ingresses.config.openshift.io cluster -o\n \njsonpath='{.spec.domain}')\n5\n. \nCreate the required base directories for the certificate generation, based on the environment\nvariables that you previously set.\n$ mkdir ${BASE_DIR}\n$ mkdir ${BASE_CERT_DIR}\n6\n. \nCreate the OpenSSL configuration for generation of a wildcard certificate.\n$ cat <<EOF> ${BASE_DIR}/openssl-san.config\n[ req ]\ndistinguished_name = req\n[ san ]\nsubjectAltName = DNS:*.${DOMAIN_NAME}\nEOF\n7\n. \nGenerate a root certificate.\n$ openssl req -x509 -sha256 -nodes -days 3650 -newkey rsa:2048 \\\n-subj \"/O=Example Inc./CN=${COMMON_NAME}\" \\\n-keyout $BASE_DIR/root.key \\\n-out $BASE_DIR/root.crt\n8\n. \nGenerate a wildcard certificate signed by the root certificate.\n$ openssl req -x509 -newkey rsa:2048 \\\n-sha256 -days 3560 -nodes \\\n-subj \"/CN=${COMMON_NAME}/O=Example Inc.\" \\\n-extensions san -config ${BASE_DIR}/openssl-san.config \\\n-CA $BASE_DIR/root.crt \\\n-CAkey $BASE_DIR/root.key \\\n-keyout $BASE_DIR/wildcard.key  \\\n-out $BASE_DIR/wildcard.crt\n$ openssl x509 -in ${BASE_DIR}/wildcard.crt -text\n9\n. \nVerify the wildcard certificate.\n$ openssl verify -CAfile ${BASE_DIR}/root.crt ${BASE_DIR}/wildcard.crt\n10\n. \nExport the wildcard key and certificate that were created by the script to new environment\nvariables.\nCHAPTER 3. SERVING LARGE MODELS\n27",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 31
      },
      "page_content": "$ export TARGET_CUSTOM_CERT=${BASE_CERT_DIR}/wildcard.crt\n$ export TARGET_CUSTOM_KEY=${BASE_CERT_DIR}/wildcard.key\n11\n. \nOptional: To export \nyour own\n wildcard key and certificate to new environment variables, enter\nthe following commands:\n$ export TARGET_CUSTOM_CERT=\n<path_to_certificate>\n$ export TARGET_CUSTOM_KEY=\n<path_to_key>\nNOTE\nIn the certificate that you provide, you must specify the domain name used by\nthe ingress controller of your OpenShift cluster. You can check this value by\nrunning the following command:\n$ oc get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}'\n12\n. \nCreate a TLS secret in the \nistio-system\n namespace using the environment variables that you\nset for the wildcard certificate and key.\n$ oc create secret tls wildcard-certs --cert=${TARGET_CUSTOM_CERT} --\nkey=${TARGET_CUSTOM_KEY} -n istio-system\n13\n. \nCreate a \ngateways.yaml\n YAML file with the following contents:\napiVersion: v1\nkind: Service \n1\nmetadata:\n  labels:\n    experimental.istio.io/disable-gateway-port-translation: \"true\"\n  name: knative-local-gateway\n  namespace: istio-system\nspec:\n  ports:\n    - name: http2\n      port: 80\n      protocol: TCP\n      targetPort: 8081\n  selector:\n    knative: ingressgateway\n  type: ClusterIP\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: knative-ingress-gateway \n2\n  namespace: knative-serving\nspec:\n  selector:\n    knative: ingressgateway\n  servers:\n    - hosts:\n        - '*'\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n28",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 32
      },
      "page_content": "1\n2\n3\n      port:\n        name: https\n        number: 443\n        protocol: HTTPS\n      tls:\n        credentialName: wildcard-certs\n        mode: SIMPLE\n---\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n name: knative-local-gateway \n3\n namespace: knative-serving\nspec:\n selector:\n   knative: ingressgateway\n servers:\n   - port:\n       number: 8081\n       name: https\n       protocol: HTTPS\n     tls:\n       mode: ISTIO_MUTUAL\n     hosts:\n       - \"*\"\nDefines a service in the \nistio-system\n namespace for the Knative local gateway.\nDefines an ingress gateway in the \nknative-serving namespace\n. The gateway uses the\nTLS secret you created earlier in this procedure. The ingress gateway handles external\ntraffic to Knative.\nDefines a local gateway for Knative in the \nknative-serving\n namespace.\n14\n. \nApply the \ngateways.yaml\n file to create the defined resources.\n$ oc apply -f gateways.yaml\nYou see the following output:\nservice/knative-local-gateway created\ngateway.networking.istio.io/knative-ingress-gateway created\ngateway.networking.istio.io/knative-local-gateway created\nVerification\nReview the gateways that you created.\n$ oc get gateway --all-namespaces\nConfirm that you see the local and ingress gateways that you created in the \nknative-serving\nnamespace, as shown in the following example:\nCHAPTER 3. SERVING LARGE MODELS\n29",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 33
      },
      "page_content": "NAMESPACE          NAME                       AGE\nknative-serving    knative-ingress-gateway    69s\nknative-serving     knative-local-gateway      2m\n3.3.2. Installing KServe\nTo complete manual installation of KServe, you must install the Red Hat OpenShift AI Operator. Then,\nyou can configure the Operator to install KServe.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYour cluster has a node with 4 CPUs and 16 GB memory.\nYou have downloaded and installed the OpenShift command-line interface (CLI). See \nInstalling\nthe OpenShift CLI\n.\nYou have \ncreated\n a Red Hat OpenShift Service Mesh instance.\nYou have \ncreated\n a Knative Serving instance.\nYou have \ncreated secure gateways\n for Knative Serving.\nYou have \ninstalled\n the Red Hat OpenShift AI Operator and \ncreated\n a \nDataScienceCluster\nobject.\nProcedure\n1\n. \nLog in to the OpenShift web console as a cluster administrator.\n2\n. \nIn the web console, click \nOperators\n \n→\n \nInstalled Operators\n and then click the Red Hat\nOpenShift AI Operator.\n3\n. \nFor installation of KServe, configure the OpenShift Service Mesh component as follows:\na\n. \nClick the \nDSC Initialization\n tab.\nb\n. \nClick the \ndefault-dsci\n object.\nc\n. \nClick the \nYAML\n tab.\nd\n. \nIn the \nspec\n section, add and configure the \nserviceMesh\n component as shown:\nspec:\n serviceMesh:\n   \nmanagementState: Unmanaged\ne\n. \nClick \nSave\n.\n4\n. \nFor installation of KServe, configure the KServe and OpenShift Serverless components as\nfollows:\na\n. \nIn the web console, click \nOperators\n \n→\n \nInstalled Operators\n and then click the Red Hat\nOpenShift AI Operator.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n30",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 34
      },
      "page_content": "b\n. \nClick the \nData Science Cluster\n tab.\nc\n. \nClick the \ndefault-dsc\n DSC object.\nd\n. \nClick the \nYAML\n tab.\ne\n. \nIn the \nspec.components\n section, configure the \nkserve\n component as shown:\nspec:\n components:\n   \nkserve:\n     \nmanagementState: Managed\nf\n. \nWithin the \nkserve\n component, add the \nserving\n component, and configure it as shown:\nspec:\n components:\n   kserve:\n     managementState: Managed\n     \nserving:\n       \nmanagementState: Unmanaged\ng\n. \nClick \nSave\n.\n3.4. DEPLOYING MODELS BY USING THE SINGLE-MODEL SERVING\nPLATFORM\nOn the single-model serving platform, each model is deployed on its own model server. This helps you to\ndeploy, monitor, scale, and maintain large models that require increased resources.\nIMPORTANT\nIf you want to use the single-model serving platform to deploy a model from S3-\ncompatible storage that uses a self-signed SSL certificate, you must install a certificate\nauthority (CA) bundle on your OpenShift Container Platform cluster. For more\ninformation, see \nWorking with certificates\n (OpenShift AI Self-Managed) or \nWorking with\ncertificates\n (OpenShift AI Self-Managed in a disconnected environment).\n3.4.1. Enabling the single model serving platform\nWhen you have installed KServe, you can use the Red Hat OpenShift AI dashboard to enable the single\nmodel serving platform. You can also use the dashboard to enable model-serving runtimes for the\nplatform.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the admin group (for example, \nrhoai-admins\n) in OpenShift.\nYou have installed KServe.\nCHAPTER 3. SERVING LARGE MODELS\n31",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 35
      },
      "page_content": "Procedure\n1\n. \nEnable the single model serving platform as follows:\na\n. \nIn the left menu, click \nSettings\n \n→\n \nCluster settings\n.\nb\n. \nLocate the \nModel serving platforms\n section.\nc\n. \nTo enable the single model serving platform for projects, select the \nSingle model serving\nplatform\n checkbox.\nd\n. \nClick \nSave changes\n.\n2\n. \nEnable pre-installed runtimes for the single-model serving platform as follows:\na\n. \nIn the left menu of the OpenShift AI dashboard, click \nSettings\n \n→\n \nServing runtimes\n.\nThe \nServing runtimes\n page shows any custom runtimes that you have added, as well as the\nfollowing pre-installed runtimes:\nCaikit TGIS ServingRuntime for KServe\nOpenVINO Model Server\nTGIS Standalone ServingRuntime for KServe\nb\n. \nSet the runtime that you want to use to \nEnabled\n.\nThe single model serving platform is now available for model deployments.\n3.4.2. Adding a custom model-serving runtime for the single-model serving\nplatform\nA model-serving runtime adds support for a specified set of model frameworks (that is, formats). You\nhave the option of using the \npre-installed runtimes\n included with OpenShift AI or adding your own,\ncustom runtimes. This is useful in instances where the pre-installed runtimes don’t meet your needs. For\nexample, you might find that the TGIS runtime does not support a particular model format that is\nsupported by \nHugging Face Text Generation Inference (TGI)\n. In this case, you can create a custom\nruntime to add support for the model.\nAs an administrator, you can use the OpenShift AI interface to add and enable a custom model-serving\nruntime. You can then choose the custom runtime when you deploy a model on the single-model serving\nplatform.\nNOTE\nOpenShift AI enables you to add your own custom runtimes, but does not support the\nruntimes themselves. You are responsible for correctly configuring and maintaining\ncustom runtimes. You are also responsible for ensuring that you are licensed to use any\ncustom runtimes that you add.\nPrerequisites\nYou have logged in to OpenShift AI as an administrator.\nYou have built your custom runtime and added the image to a container image repository such\nas \nQuay\n.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n32",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 36
      },
      "page_content": "Procedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n > \nServing runtimes\n.\nThe \nServing runtimes\n page opens and shows the model-serving runtimes that are already\ninstalled and enabled.\n2\n. \nTo add a custom runtime, choose one of the following options:\nTo start with an existing runtime (for example, \nTGIS Standalone ServingRuntime for\nKServe\n), click the action menu (\n⋮\n) next to the existing runtime and then click \nDuplicate\n.\nTo add a new custom runtime, click \nAdd serving runtime\n.\n3\n. \nIn the \nSelect the model serving platforms this runtime supports\n list, select \nSingle-model\nserving platform\n.\n4\n. \nIn the \nSelect the API protocol this runtime supports\n list, select \nREST\n or \ngRPC\n.\n5\n. \nOptional: If you started a new runtime (rather than duplicating an existing one), add your code\nby choosing one of the following options:\nUpload a YAML file\na\n. \nClick \nUpload files\n.\nb\n. \nIn the file browser, select a YAML file on your computer.\nThe embedded YAML editor opens and shows the contents of the file that you\nuploaded.\nEnter YAML code directly in the editor\na\n. \nClick \nStart from scratch\n.\nb\n. \nEnter or paste YAML code directly in the embedded editor.\nNOTE\nIn many cases, creating a custom runtime will require adding new or custom\nparameters to the \nenv\n section of the \nServingRuntime\n specification.\n6\n. \nClick \nAdd\n.\nThe \nServing runtimes\n page opens and shows the updated list of runtimes that are installed.\nObserve that the custom runtime that you added is automatically enabled. The API protocol\nthat you specified when creating the runtime is shown.\n7\n. \nOptional: To edit your custom runtime, click the action menu (\n⋮\n) and select \nEdit\n.\nVerification\nThe custom model-serving runtime that you added is shown in an enabled state on the \nServing\nruntimes\n page.\n3.4.3. Deploying models on the single model serving platform\nWhen you have enabled the single model serving platform, you can enable a pre-installed or custom\nmodel-serving runtime and start to deploy models on the platform.\nNOTE\nCHAPTER 3. SERVING LARGE MODELS\n33",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 37
      },
      "page_content": "NOTE\nText Generation Inference Server (TGIS)\n is based on an early fork of \nHugging Face TGI\n.\nRed Hat will continue to develop the standalone TGIS runtime to support TGI models. If a\nmodel does not work in the current version of OpenShift AI, support might be added in a\nfuture version. In the meantime, you can also add your own, custom runtime to support a\nTGI model. For more information, see \nAdding a custom model-serving runtime for the\nsingle model serving platform\n.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have installed KServe.\nYou have enabled the single model serving platform.\nYou have created a data science project.\nTo use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an\nexample, see \nConverting Hugging Face Hub models to Caikit format\n in the \ncaikit-tgis-serving\nrepository.\nYou know the folder path for the data connection that you want the model to access.\nIf you want to use graphics processing units (GPUs) with your model server, you have enabled\nGPU support in OpenShift AI. See \nEnabling GPU support in OpenShift AI\n.\nProcedure\n1\n. \nIn the left menu, click \nData Science Projects\n.\n2\n. \nClick the name of the project that you want to deploy a model in.\n3\n. \nIn the \nModels and model servers\n section, perform one of the following actions:\nIf you see a \n​​Single model serving platform\n tile, click \nDeploy model\n on the tile.\nIf you do not see any tiles, click the \nDeploy model\n button.\nThe \nDeploy model\n dialog opens.\n4\n. \nConfigure properties for deploying your model as follows:\na\n. \nIn the \nModel name\n field, enter a unique name for the model that you are deploying.\nb\n. \nIn the \nServing runtime\n field, select an enabled runtime.\nc\n. \nFrom the \nModel framework\n list, select a value.\nd\n. \nIn the \nNumber of model replicas to deploy\n field, specify a value.\ne\n. \nFrom the \nModel server size\n list, select a value.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n34",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 38
      },
      "page_content": "f\n. \nTo specify the location of your model, perform one of the following sets of actions:\nTo use an existing data connection\ni\n. \nSelect \nExisting data connection\n.\nii\n. \nFrom the \nName\n list, select a data connection that you previously defined.\niii\n. \nIn the \nPath\n field, enter the folder path that contains the model in your specified\ndata source.\nIMPORTANT\nThe OpenVINO Model Server runtime has specific requirements for\nhow you specify the model path. For more information, see known\nissue \nRHOAIENG-3025\n in the OpenShift AI release notes.\nTo use a new data connection\ni\n. \nTo define a new data connection that your model can access, select \nNew data\nconnection\n.\nii\n. \nIn the \nName\n field, enter a unique name for the data connection.\niii\n. \nIn the \nAccess key\n field, enter the access key ID for your S3-compatible object\nstorage provider.\niv\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object\nstorage account that you specified.\nv\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage\nbucket.\nvi\n. \nIn the \nRegion\n field, enter the default region of your S3-compatible object storage\naccount.\nvii\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nviii\n. \nIn the \nPath\n field, enter the folder path in your S3-compatible object storage that\ncontains your data file.\nIMPORTANT\nThe OpenVINO Model Server runtime has specific requirements for\nhow you specify the model path. For more information, see known\nissue \nRHOAIENG-3025\n in the OpenShift AI release notes.\ng\n. \nClick \nDeploy\n.\nVerification\nConfirm that the deployed model is shown in the \nModels and model servers\n section of your\nproject, and on the \nModel Serving\n page of the dashboard with a check mark in the \nStatus\ncolumn.\n3.4.4. Accessing the inference endpoints for models deployed on the single model\nCHAPTER 3. SERVING LARGE MODELS\n35",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 39
      },
      "page_content": "3.4.4. Accessing the inference endpoints for models deployed on the single model\nserving platform\nWhen you deploy a model by using the single model serving platform, the model is available as a service\nthat you can access using API requests. This enables you to return predictions based on data inputs. To\nuse API requests to interact with your deployed model, you must know how to access the inference\nendpoints that are available.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have deployed a model by using the single model serving platform.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nModel Serving\n.\n2\n. \nFrom the \nProject\n list, select the project that you deployed a model in.\n3\n. \nIn the \nDeployed models\n table, for the model that you want to access, copy the URL shown in\nthe \nInference endpoint\n column.\n4\n. \nDepending on what action you want to perform with the model (and if the model supports that\naction), add one of the following paths to the end of the inference endpoint URL:\nCaikit TGIS ServingRuntime for KServe\n:443/api/v1/task/text-generation\n:443/api/v1/task/server-streaming-text-generation\nTGIS Standalone ServingRuntime for KServe\n:443 fmaas.GenerationService/Generate\n:443 fmaas.GenerationService/GenerateStream\nNOTE\nTo query the endpoints for the TGIS standalone runtime, you must also\ndownload the files in the \nproto\n directory of the IBM \ntext-generation-\ninference\n repository.\nOpenVINO Model Server\n/v2/models/<model-name>/infer\nAs indicated by the paths shown, the single model serving platform uses the HTTPS port of your\nOpenShift router (usually port 443) to serve external API requests.\n5\n. \nUse the endpoints to make API requests to your deployed model, as shown in the following\nexample commands:\nCaikit TGIS ServingRuntime for KServe\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n36",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 40
      },
      "page_content": "curl --json '{\"model_id\": \"<model_name>\", \"inputs\": \"<text>\"}'\n \nhttps://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation\nTGIS Standalone ServingRuntime for KServe\ngrpcurl -proto text-generation-inference/proto/generation.proto -d '{\"requests\": [{\"text\":\"\n<text>\"}]}' -H 'mm-model-id: <model_name>' -insecure <inference_endpoint_url>:443\n \nfmaas.GenerationService/Generate\nOpenVINO Model Server\ncurl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ \"model_name\": \"\n<model_name>\", \"inputs\": [{ \"name\": \"<name_of_model_input>\", \"shape\": [<shape>],\n \n\"datatype\": \"<data_type>\", \"data\": [<data>] }]}'\nAdditional resources\nText Generation Inference Server (TGIS)\nCaikit API documentation\nOpenVINO KServe-compatible REST API documentation\n3.5. CONFIGURING MONITORING FOR THE SINGLE-MODEL SERVING\nPLATFORM\nThe single-model serving platform includes metrics for \nsupported runtimes\n. You can also configure\nmonitoring for OpenShift Service Mesh. The service mesh metrics helps you to understand\ndependencies and traffic flow between components in the mesh. When you have configured monitoring,\nyou can grant Prometheus access to scrape the available metrics.\nPrerequisites\nYou have cluster administrator privileges for your OpenShift Container Platform cluster.\nYou have created OpenShift Service Mesh and Knative Serving instances and installed KServe.\nYou have downloaded and installed the OpenShift command-line interface (CLI). See \nInstalling\nthe OpenShift CLI\n.\nYou are familiar with \ncreating a config map\n for monitoring a user-defined workflow. You will\nperform similar steps in this procedure.\nYou are familiar with \nenabling monitoring\n for user-defined projects in OpenShift. You will\nperform similar steps in this procedure.\nYou have \nassigned\n the \nmonitoring-rules-view\n role to users that will monitor metrics.\nProcedure\n1\n. \nIn a terminal window, if you are not already logged in to your OpenShift cluster as a cluster\nadministrator, log in to the OpenShift CLI as shown in the following example:\nCHAPTER 3. SERVING LARGE MODELS\n37",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 41
      },
      "page_content": "$ oc login \n<openshift_cluster_url>\n -u \n<admin_username>\n -p \n<password>\n2\n. \nDefine a \nConfigMap\n object in a YAML file called \nuwm-cm-conf.yaml\n with the following\ncontents:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\n    prometheus:\n      logLevel: debug\n      retention: 15d\nThe \nuser-workload-monitoring-config\n object configures the components that monitor user-\ndefined projects. Observe that the retention time is set to the recommended value of 15 days.\n3\n. \nApply the configuration to create the \nuser-workload-monitoring-config\n object.\n$ oc apply -f uwm-cm-conf.yaml\n4\n. \nDefine another \nConfigMap\n object in a YAML file called \nuwm-cm-enable.yaml\n with the\nfollowing contents:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true\nThe \ncluster-monitoring-config\n object enables monitoring for user-defined projects.\n5\n. \nApply the configuration to create the \ncluster-monitoring-config\n object.\n$ oc apply -f uwm-cm-enable.yaml\n6\n. \nCreate \nServiceMonitor\n and \nPodMonitor\n objects to monitor metrics in the service mesh control\nplane as follows:\na\n. \nCreate an \nistiod-monitor.yaml\n YAML file with the following contents:\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: istiod-monitor\n  namespace: istio-system\nspec:\n  targetLabels:\n  - app\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n38",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 42
      },
      "page_content": "  selector:\n    matchLabels:\n      istio: pilot\n  endpoints:\n  - port: http-monitoring\n    interval: 30s\nb\n. \nDeploy the \nServiceMonitor\n CR in the specified \nistio-system\n namespace.\n$ oc apply -f istiod-monitor.yaml\nYou see the following output:\nservicemonitor.monitoring.coreos.com/istiod-monitor created\nc\n. \nCreate an \nistio-proxies-monitor.yaml\n YAML file with the following contents:\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: istio-proxies-monitor\n  namespace: istio-system\nspec:\n  selector:\n    matchExpressions:\n    - key: istio-prometheus-ignore\n      operator: DoesNotExist\n  podMetricsEndpoints:\n  - path: /stats/prometheus\n    interval: 30s\nd\n. \nDeploy the \nPodMonitor\n CR in the specified \nistio-system\n namespace.\n$ oc apply -f istio-proxies-monitor.yaml\nYou see the following output:\npodmonitor.monitoring.coreos.com/istio-proxies-monitor created\n3.6. VIEWING METRICS FOR THE SINGLE MODEL SERVING\nPLATFORM\nWhen a cluster administrator has configured monitoring for the single model serving platform, non-\nadmin users can use the OpenShift web console to view metrics.\nPrerequisites\nA cluster administrator has configured monitoring for the single model serving platform.\nYou have been \nassigned\n the \nmonitoring-rules-view\n role.\nYou are familiar with how to \nmonitor project metrics\n in the OpenShift Container Platform web\nconsole.\nCHAPTER 3. SERVING LARGE MODELS\n39",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 43
      },
      "page_content": "Procedure\n1\n. \nLog in to the OpenShift Container Platform web console.\n2\n. \nSwitch to the \nDeveloper\n perspective.\n3\n. \nIn the left menu, click \nObserve\n.\n4\n. \nAs described in \nmonitoring project metrics\n, use the web console to run queries for \ncaikit_*\n, \ntgi_*\n, \novms_*\n or \nistio_*\n metrics.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n40",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 44
      },
      "page_content": "CHAPTER 4. MONITORING MODEL PERFORMANCE\n4.1. VIEWING PERFORMANCE METRICS FOR ALL MODELS ON A\nMODEL SERVER\nIn OpenShift AI, you can monitor the following metrics for all the models that are deployed on a model\nserver:\nHTTP requests\n - The number of HTTP requests that have failed or succeeded for all models on\nthe server.\nNote: You can also view the number of HTTP requests that have failed or succeeded for a\nspecific model, as described in \nViewing HTTP request metrics for a deployed model\n.\nAverage response time (ms)\n - For all models on the server, the average time it takes the model\nserver to respond to requests.\nCPU utilization (%)\n - The percentage of the CPU’s capacity that is currently being used by all\nmodels on the server.\nMemory utilization (%)\n - The percentage of the system’s memory that is currently being used\nby all models on the server.\nYou can specify a time range and a refresh interval for these metrics to help you determine, for\nexample, when the peak usage hours are and how the models are performing at a specified time.\nPrerequisites\nYou have installed Red Hat OpenShift AI.\nOn the OpenShift cluster where OpenShift AI is installed, user workload monitoring is enabled.\nYou have logged in to OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nThere are deployed data science models in your data science project.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard navigation menu, click \nData Science Projects\n and then\nselect the project that contains the data science models that you want to monitor.\n2\n. \nOn the \nComponents\n page, scroll down to the \nModels and model servers\n section.\n3\n. \nIn the row for the model server that you are interested in, click the action menu (\n⋮\n) and then\nselect \nView model server metrics\n.\n4\n. \nOptional: On the metrics page for the model server, set the following options:\nTime range\n - Specifies how long to track the metrics. You can select one of these values: 1\nhour, 24 hours, 7 days, and 30 days.\nRefresh interval\n - Specifies how frequently the graphs on the metrics page are refreshed\nCHAPTER 4. MONITORING MODEL PERFORMANCE\n41",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-serving_models-en-us.pdf",
        "page": 45
      },
      "page_content": "Refresh interval\n - Specifies how frequently the graphs on the metrics page are refreshed\n(to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1\nminute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.\n5\n. \nScroll down to view data graphs for HTTP requests, average response time, CPU utilization, and\nmemory utilization.\nVerification\nOn the metrics page for the model server, the graphs provide performance metric data.\n4.2. VIEWING HTTP REQUEST METRICS FOR A DEPLOYED MODEL\nYou can view a graph that illustrates the HTTP requests that have failed or succeeded for a specific\nmodel.\nPrerequisites\nYou have installed Red Hat OpenShift AI.\nOn the OpenShift cluster where OpenShift AI is installed, user workload monitoring is enabled.\nYou have logged in to OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have deployed a model in a data science project.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard navigation menu, select \nModel Serving\n.\n2\n. \nOn the \nDeployed models\n page, select the model that you are interested in.\n3\n. \nOptional: On the \nEndpoint performance\n tab, set the following options:\nTime range\n - Specifies how long to track the metrics. You can select one of these values: 1\nhour, 24 hours, 7 days, and 30 days.\nRefresh interval\n - Specifies how frequently the graphs on the metrics page are refreshed\n(to show the latest data). You can select one of these values: 15 seconds, 30 seconds, 1\nminute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.\nVerification\nThe \nEndpoint performance\n tab shows a graph of the HTTP metrics for the model.\nRed Hat OpenShift AI Self-Managed 2.8 Serving models\n42",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 0
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\nWorking on data science projects\nOrganize your work in projects and workbenches, create and collaborate on\nnotebooks, train and deploy models, configure model servers, and implement\npipelines\nLast Updated: 2024-04-05",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 1
      },
      "page_content": "",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 2
      },
      "page_content": "Red Hat OpenShift AI Self-Managed\n \n2.8\n \nWorking on data science projects\nOrganize your work in projects and workbenches, create and collaborate on notebooks, train and\ndeploy models, configure model servers, and implement pipelines",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 3
      },
      "page_content": "Legal Notice\nCopyright \n©\n 2024 Red Hat, Inc.\nThe text of and illustrations in this document are licensed by Red Hat under a Creative Commons\nAttribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\navailable at\nhttp://creativecommons.org/licenses/by-sa/3.0/\n. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\nprovide the URL for the original version.\nRed Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\nSection 4d of CC-BY-SA to the fullest extent permitted by applicable law.\nRed Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\nFedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\nand other countries.\nLinux ®\n is the registered trademark of Linus Torvalds in the United States and other countries.\nJava ®\n is a registered trademark of Oracle and/or its affiliates.\nXFS ®\n is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\nand/or other countries.\nMySQL ®\n is a registered trademark of MySQL AB in the United States, the European Union and\nother countries.\nNode.js ®\n is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\nofficial Joyent Node.js open source or commercial project.\nThe \nOpenStack ®\n Word Mark and OpenStack logo are either registered trademarks/service marks\nor trademarks/service marks of the OpenStack Foundation, in the United States and other\ncountries and are used with the OpenStack Foundation's permission. We are not affiliated with,\nendorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\nAll other trademarks are the property of their respective owners.\nAbstract\nOrganize your work in projects and workbenches, create and collaborate on notebooks, train and\ndeploy models, configure model servers, and implement pipelines.",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 4
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nTable of Contents\nCHAPTER 1. CREATING AND IMPORTING NOTEBOOKS\n1.1. CREATING A NEW NOTEBOOK\n1.1.1. Notebook images for data scientists\n1.2. UPLOADING AN EXISTING NOTEBOOK FILE FROM LOCAL STORAGE\n1.3. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT REPOSITORY USING JUPYTERLAB\n1.4. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT REPOSITORY USING THE COMMAND LINE\nINTERFACE\n1.5. ADDITIONAL RESOURCES\nCHAPTER 2. COLLABORATING ON NOTEBOOKS USING GIT\n2.1. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT REPOSITORY USING JUPYTERLAB\n2.2. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT REPOSITORY USING THE COMMAND LINE\nINTERFACE\n2.3. UPDATING YOUR PROJECT WITH CHANGES FROM A REMOTE GIT REPOSITORY\n2.4. PUSHING PROJECT CHANGES TO A GIT REPOSITORY\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n3.1. USING DATA SCIENCE PROJECTS\n3.1.1. Creating a data science project\n3.1.2. Updating a data science project\n3.1.3. Deleting a data science project\n3.2. USING PROJECT WORKBENCHES\n3.2.1. Creating a project workbench\n3.2.2. Starting a workbench\n3.2.3. Updating a project workbench\n3.2.4. Deleting a workbench from a data science project\n3.3. USING DATA CONNECTIONS\n3.3.1. Adding a data connection to your data science project\n3.3.2. Deleting a data connection\n3.3.3. Updating a connected data source\n3.4. CONFIGURING CLUSTER STORAGE\n3.4.1. Adding cluster storage to your data science project\n3.4.2. Updating cluster storage\n3.4.3. Deleting cluster storage from a data science project\n3.5. CONFIGURING DATA SCIENCE PIPELINES\n3.5.1. Configuring a pipeline server\n3.5.2. Defining a pipeline\n3.5.3. Importing a data science pipeline\n3.6. CONFIGURING ACCESS TO DATA SCIENCE PROJECTS\n3.6.1. Configuring access to data science projects\n3.6.2. Sharing access to a data science project\n3.6.3. Updating access to a data science project\n3.6.4. Removing access to a data science project\n3.7. VIEWING PYTHON PACKAGES INSTALLED ON YOUR NOTEBOOK SERVER\n3.8. INSTALLING PYTHON PACKAGES ON YOUR NOTEBOOK SERVER\n3.9. UPDATING NOTEBOOK SERVER SETTINGS BY RESTARTING YOUR SERVER\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n4.1. MANAGING DATA SCIENCE PIPELINES\n4.1.1. Configuring a pipeline server\n4.1.2. Defining a pipeline\n4.1.3. Importing a data science pipeline\n5\n5\n5\n7\n7\n8\n9\n10\n10\n10\n11\n12\n13\n13\n13\n14\n15\n15\n15\n17\n17\n18\n19\n19\n20\n20\n21\n21\n22\n23\n24\n24\n25\n26\n26\n26\n27\n28\n29\n30\n31\n32\n34\n34\n34\n36\n36\nTable of Contents\n1",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 5
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1.4. Downloading a data science pipeline\n4.1.5. Deleting a data science pipeline\n4.1.6. Deleting a pipeline server\n4.1.7. Viewing the details of a pipeline server\n4.1.8. Viewing existing pipelines\n4.1.9. Overview of pipeline versions\n4.1.10. Uploading a pipeline version\n4.1.11. Deleting a pipeline version\n4.1.12. Viewing pipeline versions\n4.1.13. Viewing the details of a pipeline version\n4.2. MANAGING PIPELINE RUNS\n4.2.1. Overview of pipeline runs\n4.2.2. Scheduling a pipeline run using a cron job\n4.2.3. Scheduling a pipeline run\n4.2.4. Cloning a scheduled pipeline run\n4.2.5. Stopping a triggered pipeline run\n4.2.6. Deleting a scheduled pipeline run\n4.2.7. Deleting a triggered pipeline run\n4.2.8. Viewing scheduled pipeline runs\n4.2.9. Viewing triggered pipeline runs\n4.2.10. Viewing the details of a pipeline run\n4.2.11. About pipeline logs\n4.2.12. Viewing pipeline step logs\n4.2.13. Downloading pipeline step logs\n4.3. WORKING WITH PIPELINES IN JUPYTERLAB\n4.3.1. Overview of pipelines in JupyterLab\n4.3.2. Accessing the pipeline editor\n4.3.3. Creating a runtime configuration\n4.3.4. Updating a runtime configuration\n4.3.5. Deleting a runtime configuration\n4.3.6. Duplicating a runtime configuration\n4.3.7. Running a pipeline in JupyterLab\n4.3.8. Exporting a pipeline in JupyterLab\n4.4. ADDITIONAL RESOURCES\nCHAPTER 5. WORKING WITH ACCELERATORS\n5.1. OVERVIEW OF ACCELERATORS\n5.2. WORKING WITH ACCELERATOR PROFILES\n5.2.1. Viewing accelerator profiles\n5.2.2. Creating an accelerator profile\n5.2.3. Updating an accelerator profile\n5.2.4. Deleting an accelerator profile\n5.2.5. Configuring a recommended accelerator for notebook images\n5.2.6. Configuring a recommended accelerator for serving runtimes\n5.3. HABANA GAUDI INTEGRATION\n5.3.1. Enabling Habana Gaudi devices\nCHAPTER 6. WORKING WITH DISTRIBUTED WORKLOADS\n6.1. OVERVIEW OF DISTRIBUTED WORKLOADS\n6.2. CONFIGURING DISTRIBUTED WORKLOADS\n6.3. RUNNING DISTRIBUTED DATA SCIENCE WORKLOADS FROM NOTEBOOKS\n6.4. RUNNING DISTRIBUTED DATA SCIENCE WORKLOADS FROM DATA SCIENCE PIPELINES\n6.5. RUNNING DISTRIBUTED DATA SCIENCE WORKLOADS IN A DISCONNECTED ENVIRONMENT\n37\n38\n38\n39\n40\n40\n41\n42\n42\n43\n44\n44\n44\n45\n46\n47\n48\n49\n49\n50\n51\n52\n52\n53\n54\n54\n55\n56\n58\n60\n61\n62\n63\n64\n65\n65\n66\n67\n67\n69\n71\n71\n72\n73\n74\n77\n77\n77\n79\n80\n83\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n2",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 6
      },
      "page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nCHAPTER 7. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR ADMINISTRATORS\n7.1. A USER RECEIVES A 404: PAGE NOT FOUND ERROR WHEN LOGGING IN TO JUPYTER\n7.2. A USER’S NOTEBOOK SERVER DOES NOT START\n7.3. THE USER RECEIVES A DATABASE OR DISK IS FULL ERROR OR A NO SPACE LEFT ON DEVICE ERROR\nWHEN THEY RUN NOTEBOOK CELLS\nCHAPTER 8. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR USERS\n8.1. I SEE A 403: FORBIDDEN ERROR WHEN I LOG IN TO JUPYTER\n8.2. MY NOTEBOOK SERVER DOES NOT START\n8.3. I SEE A DATABASE OR DISK IS FULL ERROR OR A NO SPACE LEFT ON DEVICE ERROR WHEN I RUN MY\nNOTEBOOK CELLS\n85\n85\n85\n86\n88\n88\n88\n88\nTable of Contents\n3",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 7
      },
      "page_content": "Red Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n4",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 8
      },
      "page_content": "CHAPTER 1. CREATING AND IMPORTING NOTEBOOKS\nYou can create a blank notebook or import a notebook from a number of different sources.\n1.1. CREATING A NEW NOTEBOOK\nYou can create a new Jupyter notebook from an existing notebook container image to access its\nresources and properties. The \nNotebook server control panel\n contains a list of available container\nimages that you can run as a single-user notebook server.\nPrerequisites\nEnsure that you have logged in to Red Hat OpenShift AI.\nEnsure that you have launched your notebook server and logged in to Jupyter.\nThe notebook image exists in a registry, image stream, and is accessible.\nProcedure\n1\n. \nClick \nFile\n \n→\n \nNew\n \n→\n \nNotebook\n.\n2\n. \nIf prompted, select a kernel for your notebook from the list.\nIf you want to use a kernel, click \nSelect\n. If you do not want to use a kernel, click \nNo Kernel\n.\nVerification\nCheck that the notebook file is visible in the JupyterLab interface.\n1.1.1. Notebook images for data scientists\nRed Hat OpenShift AI contains Jupyter notebook images optimized with industry-leading tools and\nlibraries required for your data science work. To provide a consistent, stable platform for your model\ndevelopment, all notebook images contain the same version of Python. Notebook images available on\nRed Hat OpenShift AI are pre-built and ready for you to use immediately after OpenShift AI is installed\nor upgraded.\nNotebook images are supported for a minimum of one year. Major updates to pre-configured notebook\nimages occur about every six months. Therefore, two supported notebook image versions are typically\navailable at any given time. You can use this support period to update your code to use components\nfrom the latest available notebook image. Legacy notebook image versions, that is, not the two most\nrecent versions, might still be available for selection. Legacy image versions include a label that\nindicates the image is out-of-date. To use the latest package versions, Red Hat recommends that you\nuse the most recently added notebook image. If necessary, you can still access older notebook images\nfrom the registry, even if they are no longer supported. You can then add the older notebook images as\ncustom notebook images to cater for your project’s specific requirements.\nSee the table in \nOptions for notebook server environments\n for a complete list of packages and versions\nincluded in these images.\nRed Hat OpenShift AI contains the following notebook images that are available by default.\nIMPORTANT\nCHAPTER 1. CREATING AND IMPORTING NOTEBOOKS\n5",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 9
      },
      "page_content": "IMPORTANT\nNotebook images denoted with \n(Technology Preview)\n in this table are not supported\nwith Red Hat production service level agreements (SLAs) and might not be functionally\ncomplete. Red Hat does not recommend using Technology Preview features in\nproduction. These features provide early access to upcoming product features, enabling\ncustomers to test functionality and provide feedback during the development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee Technology Preview Features Support Scope.\nTable 1.1. Default notebook images\nImage name\nDescription\nCUDA\nIf you are working with compute-intensive data science models that require GPU support,\nuse the Compute Unified Device Architecture (CUDA) notebook image to gain access to\nthe NVIDIA CUDA Toolkit. Using this toolkit, you can optimize your work using GPU-\naccelerated libraries and optimization tools.\nStandard Data\nScience\nUse the Standard Data Science notebook image for models that do not require TensorFlow\nor PyTorch. This image contains commonly used libraries to assist you in developing your\nmachine learning models.\nTensorFlow\nTensorFlow is an open source platform for machine learning. With TensorFlow, you can\nbuild, train and deploy your machine learning models. TensorFlow contains advanced data\nvisualization features, such as computational graph visualizations. It also allows you to\neasily monitor and track the progress of your models.\nPyTorch\nPyTorch is an open source machine learning library optimized for deep learning. If you are\nworking with computer vision or natural language processing models, use the Pytorch\nnotebook image.\nMinimal Python\nIf you do not require advanced machine learning features, or additional resources for\ncompute-intensive data science work, you can use the Minimal Python image to develop\nyour models.\nTrustyAI\nUse the TrustyAI notebook image to leverage your data science work with model\nexplainability, tracing, and accountability, and runtime monitoring.\nHabanaAI\nThe HabanaAI notebook image optimizes high-performance deep learning (DL) with\nHabana Gaudi devices. Habana Gaudi devices accelerate DL training workloads and\nmaximize training throughput and efficiency.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n6",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 10
      },
      "page_content": "code-server\n(Technology\nPreview)\nWith the code-server notebook image, you can customize your notebook environment to\nmeet your needs using a variety of extensions to add new languages, themes, debuggers,\nand connect to additional services. Enhance the efficiency of your data science work with\nsyntax highlighting, auto-indentation, and bracket matching, as well as an automatic task\nrunner for seamless automation. See \ncode-server in GitHub\n for more information.\nNOTE\nElyra-based pipelines are not available with the code-server notebook\nimage.\nImage name\nDescription\nAdditional resources\nInstalling Python packages on your notebook server\nOptions for notebook server environments\n1.2. UPLOADING AN EXISTING NOTEBOOK FILE FROM LOCAL\nSTORAGE\nYou can load an existing notebook from local storage into JupyterLab to continue work, or adapt a\nproject for a new use case.\nPrerequisites\nCredentials for logging in to Jupyter.\nA launched and running notebook server.\nA notebook file exists in your local storage.\nProcedure\n1\n. \nIn the \nFile Browser\n in the left sidebar of the JupyterLab interface, click \nUpload Files\n ( \n \n).\n2\n. \nLocate and select the notebook file and click \nOpen\n.\nThe file is displayed in the \nFile Browser\n.\nVerification\nThe notebook file displays in the \nFile Browser\n in the left sidebar of the JupyterLab interface.\nYou can open the notebook file in JupyterLab.\n1.3. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT\nREPOSITORY USING JUPYTERLAB\nYou can use the JupyterLab user interface to clone a Git repository into your workspace to continue\nCHAPTER 1. CREATING AND IMPORTING NOTEBOOKS\n7",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 11
      },
      "page_content": "You can use the JupyterLab user interface to clone a Git repository into your workspace to continue\nyour work or integrate files from an external project.\nPrerequisites\nA launched and running Jupyter server.\nRead access for the Git repository you want to clone.\nProcedure\n1\n. \nCopy the HTTPS URL for the Git repository.\nOn GitHub, click \n\u0000 Code\n \n→\n \nHTTPS\n and click the Clipboard button.\nOn GitLab, click \nClone\n and click the Clipboard button under \nClone with HTTPS\n.\n2\n. \nIn the JupyterLab interface, click the \nGit Clone\n button ( \n \n).\nYou can also click \nGit\n \n→\n \nClone a repository\n in the menu, or click the Git icon ( \n \n) and click\nthe \nClone a repository\n button.\nThe \nClone a repo\n dialog appears.\n3\n. \nEnter the HTTPS URL of the repository that contains your notebook.\n4\n. \nClick \nCLONE\n.\n5\n. \nIf prompted, enter your username and password for the Git repository.\nVerification\nCheck that the contents of the repository are visible in the file browser in JupyterLab, or run\nthe \nls\n command in the terminal to verify that the repository is shown as a directory.\n1.4. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT\nREPOSITORY USING THE COMMAND LINE INTERFACE\nYou can use the command line interface to clone a Git repository into your workspace to continue your\nwork or integrate files from an external project.\nPrerequisites\nA launched and running Jupyter server.\nProcedure\n1\n. \nCopy the HTTPS URL for the Git repository.\nOn GitHub, click \n\u0000 Code\n \n→\n \nHTTPS\n and click the Clipboard button.\nOn GitLab, click \nClone\n and click the Clipboard button under \nClone with HTTPS\n.\n2\n. \nIn JupyterLab, click \nFile\n \n→\n \nNew\n \n→\n \nTerminal\n to open a terminal window.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n8",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 12
      },
      "page_content": "3\n. \nEnter the \ngit clone\n command.\ngit clone \n<git-clone-URL>\nReplace \n`<git-clone-URL>`\n with the HTTPS URL, for example:\n[1234567890@jupyter-nb-jdoe ~]$ \ngit clone https://github.com/example/myrepo.git\nCloning into \nmyrepo\n...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 2821 (delta 1), reused 5 (delta 1), pack-reused 2810\nReceiving objects: 100% (2821/2821), 39.17 MiB | 23.89 MiB/s, done.\nResolving deltas: 100% (1416/1416), done.\nVerification\nCheck that the contents of the repository are visible in the file browser in JupyterLab, or run\nthe \nls\n command in the terminal to verify that the repository is shown as a directory.\n1.5. ADDITIONAL RESOURCES\nCollaborating on notebooks using Git\nCHAPTER 1. CREATING AND IMPORTING NOTEBOOKS\n9",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 13
      },
      "page_content": "CHAPTER 2. COLLABORATING ON NOTEBOOKS USING GIT\nIf your notebooks or other files are stored in Git version control, you can import them from a Git\nrepository onto your notebook server to work with them in JupyterLab. When you are ready, you can\npush your changes back to the Git repository so that others can review or use your models.\n2.1. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT\nREPOSITORY USING JUPYTERLAB\nYou can use the JupyterLab user interface to clone a Git repository into your workspace to continue\nyour work or integrate files from an external project.\nPrerequisites\nA launched and running Jupyter server.\nRead access for the Git repository you want to clone.\nProcedure\n1\n. \nCopy the HTTPS URL for the Git repository.\nOn GitHub, click \n\u0000 Code\n \n→\n \nHTTPS\n and click the Clipboard button.\nOn GitLab, click \nClone\n and click the Clipboard button under \nClone with HTTPS\n.\n2\n. \nIn the JupyterLab interface, click the \nGit Clone\n button ( \n \n).\nYou can also click \nGit\n \n→\n \nClone a repository\n in the menu, or click the Git icon ( \n \n) and click\nthe \nClone a repository\n button.\nThe \nClone a repo\n dialog appears.\n3\n. \nEnter the HTTPS URL of the repository that contains your notebook.\n4\n. \nClick \nCLONE\n.\n5\n. \nIf prompted, enter your username and password for the Git repository.\nVerification\nCheck that the contents of the repository are visible in the file browser in JupyterLab, or run\nthe \nls\n command in the terminal to verify that the repository is shown as a directory.\n2.2. UPLOADING AN EXISTING NOTEBOOK FILE FROM A GIT\nREPOSITORY USING THE COMMAND LINE INTERFACE\nYou can use the command line interface to clone a Git repository into your workspace to continue your\nwork or integrate files from an external project.\nPrerequisites\nA launched and running Jupyter server.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n10",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 14
      },
      "page_content": "Procedure\n1\n. \nCopy the HTTPS URL for the Git repository.\nOn GitHub, click \n\u0000 Code\n \n→\n \nHTTPS\n and click the Clipboard button.\nOn GitLab, click \nClone\n and click the Clipboard button under \nClone with HTTPS\n.\n2\n. \nIn JupyterLab, click \nFile\n \n→\n \nNew\n \n→\n \nTerminal\n to open a terminal window.\n3\n. \nEnter the \ngit clone\n command.\ngit clone \n<git-clone-URL>\nReplace \n`<git-clone-URL>`\n with the HTTPS URL, for example:\n[1234567890@jupyter-nb-jdoe ~]$ \ngit clone https://github.com/example/myrepo.git\nCloning into \nmyrepo\n...\nremote: Enumerating objects: 11, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (10/10), done.\nremote: Total 2821 (delta 1), reused 5 (delta 1), pack-reused 2810\nReceiving objects: 100% (2821/2821), 39.17 MiB | 23.89 MiB/s, done.\nResolving deltas: 100% (1416/1416), done.\nVerification\nCheck that the contents of the repository are visible in the file browser in JupyterLab, or run\nthe \nls\n command in the terminal to verify that the repository is shown as a directory.\n2.3. UPDATING YOUR PROJECT WITH CHANGES FROM A REMOTE GIT\nREPOSITORY\nYou can pull changes made by other users into your data science project from a remote Git repository.\nPrerequisites\nYou have configured the remote Git repository.\nYou have already imported the Git repository into JupyterLab, and the contents of the\nrepository are visible in the file browser in JupyterLab.\nYou have permissions to pull files from the remote Git repository to your local repository.\nYou have credentials for logging in to Jupyter.\nYou have a launched and running Jupyter server.\nProcedure\n1\n. \nIn the JupyterLab interface, click the \nGit\n button ( \n \n).\n2\n. \nClick the \nPull latest changes\n button ( \n \n).\nCHAPTER 2. COLLABORATING ON NOTEBOOKS USING GIT\n11",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 15
      },
      "page_content": "Verification\nYou can view the changes pulled from the remote repository in the \nHistory\n tab of the Git pane.\n2.4. PUSHING PROJECT CHANGES TO A GIT REPOSITORY\nTo build and deploy your application in a production environment, upload your work to a remote Git\nrepository.\nPrerequisites\nYou have opened a notebook in the JupyterLab interface.\nYou have already added the relevant Git repository to your notebook server.\nYou have permission to push changes to the relevant Git repository.\nYou have installed the Git version control extension.\nProcedure\n1\n. \nClick \nFile\n \n→\n \nSave All\n to save any unsaved changes.\n2\n. \nClick the Git icon ( \n \n) to open the Git pane in the JupyterLab interface.\n3\n. \nConfirm that your changed files appear under \nChanged\n.\nIf your changed files appear under \nUntracked\n, click \nGit\n \n→\n \nSimple Staging\n to enable a simplified\nGit process.\n4\n. \nCommit your changes.\na\n. \nEnsure that all files under \nChanged\n have a blue checkmark beside them.\nb\n. \nIn the \nSummary\n field, enter a brief description of the changes you made.\nc\n. \nClick \nCommit\n.\n5\n. \nClick \nGit\n \n→\n \nPush to Remote\n to push your changes to the remote repository.\n6\n. \nWhen prompted, enter your Git credentials and click \nOK\n.\nVerification\nYour most recently pushed changes are visible in the remote Git repository.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n12",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 16
      },
      "page_content": "CHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\nAs a data scientist, you can organize your data science work into a single project. A data science project\nin OpenShift AI can consist of the following components:\nWorkbenches\nCreating a workbench allows you to add a Jupyter notebook to your project.\nCluster storage\nFor data science projects that require data to be retained, you can add cluster storage to the project.\nData connections\nAdding a data connection to your project allows you to connect data inputs to your workbenches.\nPipelines\nStandardize and automate machine learning workflows to enable you to further enhance and deploy\nyour data science models.\nModels and model servers\nDeploy a trained data science model to serve intelligent applications. Your model is deployed with an\nendpoint that allows applications to send requests to the model.\nIMPORTANT\nIf you create an OpenShift project outside of the OpenShift AI user interface, the project\nis not shown on the \nData science projects\n page. In addition, you cannot use features\nexclusive to OpenShift AI, such as workbenches and model serving, with a standard\nOpenShift project.\nTo classify your OpenShift project as a data science project, and to make available\nfeatures exclusive to OpenShift AI, you must add the label \nopendatahub.io/dashboard:\n \n'true'\n to the project namespace. After you add this label, your project is subsequently\nshown on the \nData science projects\n page.\n3.1. USING DATA SCIENCE PROJECTS\n3.1.1. Creating a data science project\nTo start your data science work, create a data science project. Creating a project helps you organize your\nwork in one place. You can also enhance your data science project by adding the following functionality:\nWorkbenches\nStorage for your project’s cluster\nData connections\nModel servers\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n13",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 17
      },
      "page_content": "Procedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick \nCreate data science project\n.\nThe \nCreate a data science project\n dialog opens.\n3\n. \nEnter a \nname\n for your data science project.\n4\n. \nOptional: Edit the \nresource name\n for your data science project. The resource name must\nconsist of lowercase alphanumeric characters, \n-\n, and must start and end with an alphanumeric\ncharacter.\n5\n. \nEnter a \ndescription\n for your data science project.\n6\n. \nClick \nCreate\n.\nA project details page opens. From this page, you can create workbenches, add cluster storage\nand data connections, import pipelines, and deploy models.\nVerification\nThe project that you created is displayed on the \nData science projects\n page.\n3.1.2. Updating a data science project\nYou can update your data science project’s details by changing your project’s name and description text.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the action menu (\n⋮\n) beside the project whose details you want to update and click \nEdit\nproject\n.\nThe \nEdit data science project\n dialog opens.\n3\n. \nOptional: Update the \nname\n for your data science project.\n4\n. \nOptional: Update the \ndescription\n for your data science project.\n5\n. \nClick \nUpdate\n.\nVerification\nThe data science project that you updated is displayed on the \nData science projects\n page.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n14",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 18
      },
      "page_content": "3.1.3. Deleting a data science project\nYou can delete data science projects so that they do not appear on the OpenShift AI \nData science\nprojects\n page when you no longer want to use them.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \n{oai-user-group}\n) in OpenShift.\nYou have created a data science project.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the action menu (\n⋮\n) beside the project that you want to delete and click \nDelete project\n.\nThe \nDelete project\n dialog opens.\n3\n. \nEnter the project name in the text field to confirm that you intend to delete it.\n4\n. \nClick \nDelete project\n.\nVerification\nThe data science project that you deleted is no longer displayed on the \nData science projects\npage.\nDeleting a data science project deletes any associated workbenches, cluster storage, and data\nconnections. This data is permanently deleted and is not recoverable.\n3.2. USING PROJECT WORKBENCHES\n3.2.1. Creating a project workbench\nTo examine and work with models in an isolated area, you can create a workbench. You can use this\nworkbench to create a Jupyter notebook from an existing notebook container image to access its\nresources and properties. For data science projects that require data retention, you can add container\nstorage to the workbench you are creating. If you require extra power for use with large datasets, you\ncan assign accelerators to your workbench to optimize performance.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use specialized OpenShift AI groups, you are part of the user group or admin group (for\nexample, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that you can add a workbench to.\nProcedure\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n15",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 19
      },
      "page_content": "1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to add the workbench to.\nThe \nDetails\n page for the project opens.\n3\n. \nIn the \nWorkbenches\n section, click \nCreate workbench\n.\nThe \nCreate workbench\n page opens.\n4\n. \nConfigure the properties of the workbench you are creating.\na\n. \nIn the \nName\n field, enter a name for your workbench.\nb\n. \nOptional: In the \nDescription\n field, enter a description to define your workbench.\nc\n. \nIn the \nNotebook image\n section, complete the fields to specify the notebook image to use\nwith your workbench.\ni\n. \nFrom the \nImage selection\n list, select a notebook image.\nd\n. \nIn the \nDeployment size\n section, specify the size of your deployment instance.\ni\n. \nFrom the \nContainer size\n list, select a container size for your server.\nii\n. \nOptional: From the \nAccelerator\n list, select an accelerator.\niii\n. \nIf you selected an accelerator in the preceding step, specify the number of accelerators\nto use.\ne\n. \nOptional: Select and specify values for any new \nenvironment variables\n.\na\n. \nConfigure the storage for your OpenShift AI cluster.\ni\n. \nSelect \nCreate new persistent storage\n to create storage that is retained after you log out\nof OpenShift AI. Complete the relevant fields to define the storage.\nii\n. \nSelect \nUse existing persistent storage\n to reuse existing storage and select the storage\nfrom the \nPersistent storage\n list.\nb\n. \nTo use a data connection, in the \nData connections\n section, select the \nUse a data connection\ncheckbox.\nCreate a new data connection as follows:\ni\n. \nSelect \nCreate new data connection\n.\nii\n. \nIn the \nName\n field, enter a unique name for the data connection.\niii\n. \nIn the \nAccess key\n field, enter the access key ID for the S3-compatible object storage\nprovider.\niv\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object\nstorage account that you specified.\nv\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage bucket.\nvi\n. \nIn the \nRegion\n field, enter the default region of your S3-compatible object storage\naccount.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n16",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 20
      },
      "page_content": "vii\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nUse an existing data connection as follows:\ni\n. \nSelect \nUse existing data connection\n.\nii\n. \nFrom the \nData connection\n list, select a data connection that you previously defined.\n1\n. \nClick \nCreate workbench\n.\nVerification\nThe workbench that you created appears on the \nDetails\n page for the project.\nAny cluster storage that you associated with the workbench during the creation process\nappears on the \nDetails\n page for the project.\nThe \nStatus\n column, located in the \nWorkbenches\n section of the \nDetails\n page, displays a status\nof \nStarting\n when the workbench server is starting, and \nRunning\n when the workbench has\nsuccessfully started.\n3.2.2. Starting a workbench\nYou can manually start a data science project’s workbench from the \nDetails\n page for the project. By\ndefault, workbenches start immediately after you create them.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project that contains a workbench.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project whose workbench you want to start.\nThe \nDetails\n page for the project opens.\n3\n. \nClick the toggle in the \nStatus\n column for the relevant workbench to start a workbench that is\nnot running.\nThe status of the workbench that you started changes from \nStopped\n to \nRunning\n. After the\nworkbench has started, click \nOpen\n to open the workbench’s notebook.\nVerification\nThe workbench that you started appears on the \nDetails\n page for the project with the status of\nRunning\n.\n3.2.3. Updating a project workbench\nIf your data science work requires you to change your workbench’s notebook image, container size, or\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n17",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 21
      },
      "page_content": "If your data science work requires you to change your workbench’s notebook image, container size, or\nidentifying information, you can update the properties of your project’s workbench. If you require extra\npower for use with large datasets, you can assign accelerators to your workbench to optimize\nperformance.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use specialized OpenShift AI groups, you are part of the user group or admin group (for\nexample, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project that has a workbench.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project whose workbench you want to update.\nThe \nDetails\n page for the project opens.\n3\n. \nClick the action menu (\n⋮\n) beside the workbench that you want to update in the \nWorkbenches\nsection and click \nEdit workbench\n.\nThe \nEdit workbench\n page opens.\n4\n. \nUpdate any of the workbench properties and then click \nUpdate workbench\n.\nVerification\nThe workbench that you updated appears on the \nDetails\n page for the project.\n3.2.4. Deleting a workbench from a data science project\nYou can delete workbenches from your data science projects to help you remove Jupyter notebooks\nthat are no longer relevant to your work.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project with a workbench.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to delete the workbench from.\nThe \nDetails\n page for the project opens.\n3\n. \nClick the action menu (\n⋮\n) beside the workbench that you want to delete in the \nWorkbenches\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n18",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 22
      },
      "page_content": "3\n. \nClick the action menu (\n⋮\n) beside the workbench that you want to delete in the \nWorkbenches\nsection and click \nDelete workbench\n.\nThe \nDelete workbench\n dialog opens.\n4\n. \nEnter the name of the workbench in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete workbench\n.\nVerification\nThe workbench that you deleted is no longer displayed in the \nWorkbenches\n section on the\nproject \nDetails\n page.\nThe custom resource (CR) associated with the workbench’s Jupyter notebook is deleted.\n3.3. USING DATA CONNECTIONS\n3.3.1. Adding a data connection to your data science project\nYou can enhance your data science project by adding a connection to a data source. When you want to\nwork with a very large data sets, you can store your data in an S3-compatible object storage bucket, so\nthat you do not fill up your local storage. You also have the option of associating the data connection\nwith an existing workbench that does not already have a connection.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that you can add a data connection to.\nYou have access to S3-compatible object storage.\nIf you intend to add the data connection to an existing workbench, you have saved any data in\nthe workbench to avoid losing work.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to add a data connection to.\nA project details page opens.\n3\n. \nIn the \nData connections\n section of the page, click \nAdd data connection\n.\nThe \nAdd data connection\n dialog opens.\n4\n. \nEnter a \nname\n for the data connection.\n5\n. \nIn the \nAccess key\n field, enter the access key ID for your S3-compatible object storage provider.\n6\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object storage\naccount you specified.\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n19",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 23
      },
      "page_content": "7\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage bucket.\n8\n. \nIn the \nRegion\n field, enter the default region of your S3-compatible object storage account.\n9\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\n10\n. \nClick \nAdd data connection\n.\nVerification\nThe data connection that you added appears in the \nData connections\n section on the \nDetails\npage for the project.\nIf you selected a workbench, the data connection is visible in the \nWorkbenches\n section on your\ndata science project page.\n3.3.2. Deleting a data connection\nYou can delete data connections from your data science projects to help you remove connections that\nare no longer relevant to your work.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project with a data connection.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to delete the data connection from.\nThe \nDetails\n page for the project opens.\n3\n. \nClick the action menu (\n⋮\n) beside the data connection that you want to delete in the \nData\nconnections\n section and click \nDelete data connection\n.\nThe \nDelete data connection\n dialog opens.\n4\n. \nEnter the name of the data connection in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete data connection\n.\nVerification\nThe data connection that you deleted is no longer displayed in the \nData connections\n section on\nthe project \nDetails\n page.\n3.3.3. Updating a connected data source\nTo use an existing data source with a different workbench, you can change the data source that is\nconnected to your project’s workbench.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n20",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 24
      },
      "page_content": "Prerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project, created a workbench, and you have defined a data\nconnection.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project whose data source you want to change.\nA project details page opens.\n3\n. \nClick the action menu (\n⋮\n) beside the data source that you want to change in the \nData\nconnections\n section and click \nChange connected workbenches\n.\nThe \nUpdate connected workbenches\n dialog opens.\n4\n. \nSelect an existing \nworkbench\n to connect the data source to from the list.\n5\n. \nClick \nUpdate connected workbenches\n.\nVerification\nThe data connection that you changed is displayed in the \nData connections\n section on the\nproject \nDetails\n page.\nYou can access your S3 data source using environment variables in the connected workbench.\n3.4. CONFIGURING CLUSTER STORAGE\n3.4.1. Adding cluster storage to your data science project\nFor data science projects that require data to be retained, you can add cluster storage to the project.\nAdditionally, you can also connect cluster storage to a specific project’s workbench.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project that you can add cluster storage to.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to add the cluster storage to.\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n21",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 25
      },
      "page_content": "A project details page opens.\n3\n. \nIn the \nCluster storage\n section of the page, click \nAdd cluster storage\n.\nThe \nAdd storage\n dialog opens.\n4\n. \nEnter a \nname\n for the cluster storage.\n5\n. \nEnter a \ndescription\n for the cluster storage.\n6\n. \nUnder \nPersistent storage size\n, enter a new size in gibibytes. The minimum size is 1 GiB, and the\nmaximum size is 16384 GiB.\n7\n. \nOptional: Select a \nworkbench\n from the list to connect the cluster storage to an existing\nworkbench.\n8\n. \nIf you selected a workbench to connect the storage to, enter the storage directory in the \nMount\nfolder\n field.\n9\n. \nClick \nAdd storage\n.\nVerification\nThe cluster storage that you added appears in the \nCluster storage\n section on the \nDetails\n page\nfor the project.\nA new persistent volume claim (PVC) is created with the storage size that you defined.\nThe persistent volume claim (PVC) is visible as an attached storage in the \nWorkbenches\n section\non the \nDetails\n page for the project.\n3.4.2. Updating cluster storage\nIf your data science work requires you to change the identifying information of a project’s cluster\nstorage or the workbench that the storage is connected to, you can update your project’s cluster\nstorage to change these properties.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project that contains cluster storage.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project whose storage you want to update.\nThe \nDetails\n page for the project opens.\n3\n. \nClick the action menu (\n⋮\n) beside the storage that you want to update in the \nCluster storage\nsection and click \nEdit storage\n.\nThe \nEdit storage\n page opens.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n22",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 26
      },
      "page_content": "4\n. \nUpdate the storage’s properties.\na\n. \nUpdate the \nname\n for the storage, if applicable.\nb\n. \nUpdate the \ndescription\n for the storage, if applicable.\nc\n. \nIncrease the \nPersistent storage size\n for the storage, if applicable.\nNote that you can only increase the storage size. Updating the storage size restarts the\nworkbench and makes it unavailable for a period of time that is usually proportional to the\nsize change.\nd\n. \nUpdate the \nworkbench\n that the storage is connected to, if applicable.\ne\n. \nIf you selected a new workbench to connect the storage to, enter the storage directory in\nthe \nMount folder\n field.\n5\n. \nClick \nUpdate storage\n.\nIf you increased the storage size, the workbench restarts and is unavailable for a period of time that is\nusually proportional to the size change.\nVerification\nThe storage that you updated appears in the \nCluster storage\n section on the \nDetails\n page for\nthe project.\n3.4.3. Deleting cluster storage from a data science project\nYou can delete cluster storage from your data science projects to help you free up resources and delete\nunwanted storage space.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project with cluster storage.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to delete the storage from.\nA project details page opens.\n3\n. \nIn the \nCluster storage\n section, click the action menu (\n⋮\n) beside the storage that you want to\ndelete and then click \nDelete storage\n.\nThe \nDelete storage\n dialog opens.\n4\n. \nEnter the name of the storage in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete storage\n.\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n23",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 27
      },
      "page_content": "Verification\nThe storage that you deleted is no longer displayed in the \nCluster storage\n section on the\nproject \nDetails\n page.\nThe persistent volume (PV) and persistent volume claim (PVC) associated with the cluster\nstorage are both permanently deleted. This data is not recoverable.\n3.5. CONFIGURING DATA SCIENCE PIPELINES\n3.5.1. Configuring a pipeline server\nBefore you can successfully create a pipeline in OpenShift AI, you must configure a pipeline server. This\nincludes configuring where your pipeline artifacts and data are stored.\nNOTE\nAfter the pipeline server is created, the \n/metadata\n and \n/artifacts\n folders are automatically\ncreated in the default \nroot\n folder. Therefore, you are not required to specify any storage\ndirectories when configuring a data connection for your pipeline server.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that you can add a pipeline server to.\nYou have an existing S3-compatible object storage bucket and you have configured write\naccess to your S3 bucket on your storage account.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to configure a pipeline server for.\nA project details page opens.\n3\n. \nIn the \nPipelines\n section, click \nConfigure a pipeline server\n.\nThe \nConfigure pipeline server\n dialog appears.\n4\n. \nIn the \nObject storage connection\n section, provide values for the mandatory fields:\na\n. \nIn the \nAccess key\n field, enter the access key ID for the S3-compatible object storage\nprovider.\nb\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object storage\naccount that you specified.\nc\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage bucket.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n24",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 28
      },
      "page_content": "d\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nIMPORTANT\nIf you specify incorrect data connection settings, you cannot update these\nsettings on the same pipeline server. Therefore, you must delete the pipeline\nserver and configure another one.\n5\n. \nIn the \nDatabase\n section, click \nShow advanced database options\n to specify the database to\nstore your pipeline data and select one of the following sets of actions:\nSelect \nUse default database stored on your cluster\n to deploy a MariaDB database in your\nproject.\nSelect \nConnect to external MySQL database\n to add a new connection to an external\ndatabase that your pipeline server can access.\ni\n. \nIn the \nHost\n field, enter the database’s host name.\nii\n. \nIn the \nPort\n field, enter the database’s port.\niii\n. \nIn the \nUsername\n field, enter the default user name that is connected to the database.\niv\n. \nIn the \nPassword\n field, enter the password for the default user account.\nv\n. \nIn the \nDatabase\n field, enter the database name.\n6\n. \nClick \nConfigure\n.\nVerification\nThe pipeline server that you configured is displayed in the \nPipelines\n section on the project\ndetails page.\nThe \nImport pipeline\n button is available in the \nPipelines\n section on the project details page.\n3.5.2. Defining a pipeline\nThe Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use\nthe Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your\npipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). After\ndefining the pipeline, you can import the YAML file to the OpenShift AI dashboard to enable you to\nconfigure its execution settings. For more information about installing and using Kubeflow Pipelines SDK\nfor Tetkon, see \nKubeflow Pipelines SDK for Tekton\n.\nYou can also use the Elyra JupyterLab extension to create and run data science pipelines within\nJupyterLab. For more information on creating pipelines in JupyterLab, see \nWorking with pipelines in\nJupyterLab\n. For more information on the Elyra JupyterLab extension, see \nElyra Documentation\n.\nAdditional resources\nKubeflow Pipelines SDK for Tekton\nKFP Tekton samples and compiler samples\nKubeflow Pipelines v1 Documentation\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n25",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 29
      },
      "page_content": "Elyra Documentation\n3.5.3. Importing a data science pipeline\nTo help you begin working with data science pipelines in OpenShift AI, you can import a YAML file\ncontaining your pipeline’s code to an active pipeline server. This file contains a Kubeflow pipeline\ncompiled with the Tekton compiler. After you have imported the pipeline to a pipeline server, you can\nexecute the pipeline by creating a pipeline run.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to import a pipeline to.\n3\n. \nClick \nImport pipeline\n.\nThe \nImport pipeline\n dialog opens.\n4\n. \nEnter the details for the pipeline that you are importing.\na\n. \nIn the \nPipeline name\n field, enter a name for the pipeline that you are importing.\nb\n. \nIn the \nPipeline description\n field, enter a description for the pipeline that you are importing.\nc\n. \nClick \nUpload\n. Alternatively, drag the file from your local machine’s file system and drop it in\nthe designated area in the \nImport pipeline\n dialog.\nA file browser opens.\nd\n. \nNavigate to the file containing the pipeline code and click \nSelect\n.\ne\n. \nClick \nImport pipeline\n.\nVerification\nThe pipeline that you imported is displayed on the \nPipelines\n page.\nFor more information about using pipelines in OpenShift AI, see \nWorking with data science pipelines\n.\n3.6. CONFIGURING ACCESS TO DATA SCIENCE PROJECTS\n3.6.1. Configuring access to data science projects\nTo enable you to work collaboratively on your data science projects with other users, you can share\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n26",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 30
      },
      "page_content": "To enable you to work collaboratively on your data science projects with other users, you can share\naccess to your project. After creating your project, you can then set the appropriate access permissions\nfrom the OpenShift AI user interface.\nYou can assign the following access permission levels to your data science projects:\nAdmin - Users can modify all areas of a project, including its details (project name and\ndescription), components, and access permissions.\nEdit - Users can modify a project’s components, such as its workbench, but they cannot edit a\nproject’s access permissions or its details (project name and description).\n3.6.2. Sharing access to a data science project\nTo enable your organization to work collaboratively, you can share access to your data science project\nwith other users and groups.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nFrom the list of data science projects, click the name of the data science project that you want\nto share access to.\nA project details page opens.\n3\n. \nClick the \nPermissions\n tab.\nThe \nPermissions\n page for the project opens.\n4\n. \nProvide one or more users with access to the project.\na\n. \nIn the \nUsers\n section, click \nAdd user\n.\nb\n. \nIn the \nName\n field, enter the user name of the user whom you want to provide access to the\nproject.\nc\n. \nFrom the \nPermissions\n list, select one of the following access permission levels:\nAdmin: Users with this access level can edit project details and manage access to the\nproject.\nEdit: Users with this access level can view and edit project components, such as its\nworkbenches, data connections, and storage.\nd\n. \nTo confirm your entry, click \nConfirm\n ( \n \n).\ne\n. \nOptional: To add an additional user, click \nAdd user\n and repeat the process.\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n27",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 31
      },
      "page_content": "5\n. \nProvide one or more OpenShift groups with access to the project.\na\n. \nIn the \nGroups\n section, click \nAdd group\n.\nb\n. \nFrom the \nName\n list, select a group to provide access to the project.\nNOTE\nIf you do not have \ncluster-admin\n permissions, the \nName\n list is not visible.\nInstead, an input field is displayed enabling you to configure group\npermissions.\nc\n. \nFrom the \nPermissions\n list, select one of the following access permission levels:\nAdmin: Groups with this access permission level can edit project details and manage\naccess to the project.\nEdit: Groups with this access permission level can view and edit project components,\nsuch as its workbenches, data connections, and storage.\nd\n. \nTo confirm your entry, click \nConfirm\n ( \n \n).\ne\n. \nOptional: To add an additional group, click \nAdd group\n and repeat the process.\nVerification\nUsers to whom you provided access to the project can perform only the actions permitted by\ntheir access permission level.\nThe \nUsers\n and \nGroups\n sections on the \nPermissions\n tab show the respective users and groups\nthat you provided with access to the project.\n3.6.3. Updating access to a data science project\nTo change the level of collaboration on your data science project, you can update the access\npermissions of users and groups who have access to your project.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project.\nYou have previously shared access to your project with other users or groups.\nYou have administrator permissions or you are the project owner.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n28",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 32
      },
      "page_content": "2\n. \nClick the name of the project that you want to change the access permissions of.\nA project details page opens.\n3\n. \nClick the \nPermissions\n tab.\nThe \nPermissions\n page for the project opens.\n4\n. \nUpdate the user access permissions to the project.\na\n. \nIn the \nName\n field, update the user name of the user whom you want to provide access to\nthe project.\nb\n. \nFrom the \nPermissions\n list, update the user access permissions by selecting one of the\nfollowing:\nAdmin: Users with this access level can edit project details and manage access to the\nproject.\nEdit: Users with this access level can view and edit project components, such as its\nworkbenches, data connections, and storage.\nc\n. \nTo confirm the update to the entry, click \nConfirm\n ( \n \n).\n5\n. \nUpdate the OpenShift groups access permissions to the project.\na\n. \nFrom the \nName\n list, update the group that has access to the project by selecting another\ngroup from the list.\nNOTE\nIf you do not have \ncluster-admin\n permissions, the \nName\n list is not visible.\nInstead, you can configure group permissions in the input field that appears.\nb\n. \nFrom the \nPermissions\n list, update the group access permissions by selecting one of the\nfollowing:\nAdmin: Groups with this access permission level can edit project details and manage\naccess to the project.\nEdit: Groups with this access permission level can view and edit project components,\nsuch as its workbenches, data connections, and storage.\nc\n. \nTo confirm the update to the entry, click \nConfirm\n ( \n \n).\nVerification\nThe \nUsers\n and \nGroups\n sections on the \nPermissions\n tab show the respective users and groups\nwhose project access permissions you changed.\n3.6.4. Removing access to a data science project\nIf you no longer want to work collaboratively on your data science project, you can restrict access to your\nproject by removing users and groups that you previously provided access to your project.\nPrerequisites\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n29",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 33
      },
      "page_content": "You have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project.\nYou have previously shared access to your project with other users or groups.\nYou have administrator permissions or you are the project owner.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to change the access permissions of.\nA project details page opens.\n3\n. \nClick the \nPermissions\n tab.\nThe \nPermissions\n page for the project opens.\n4\n. \nClick the action menu (\n⋮\n) beside the user or group whose access permissions you want to\nrevoke and click \nDelete\n.\nVerification\nUsers whose access you have revoked can no longer perform the actions that were permitted\nby their access permission level.\n3.7. VIEWING PYTHON PACKAGES INSTALLED ON YOUR NOTEBOOK\nSERVER\nYou can check which Python packages are installed on your notebook server and which version of the\npackage you have by running the \npip\n tool in a notebook cell.\nPrerequisites\nLog in to Jupyter and open a notebook.\nProcedure\n1\n. \nEnter the following in a new cell in your notebook:\n!pip list\n2\n. \nRun the cell.\nVerification\nThe output shows an alphabetical list of all installed Python packages and their versions. For\nexample, if you use this command immediately after creating a notebook server that uses the\nMinimal\n image, the first packages shown are similar to the following:\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n30",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 34
      },
      "page_content": "Package                           Version\n--------------------------------- ----------\naiohttp                           3.7.3\nalembic                           1.5.2\nappdirs                           1.4.4\nargo-workflows                    3.6.1\nargon2-cffi                       20.1.0\nasync-generator                   1.10\nasync-timeout                     3.0.1\nattrdict                          2.0.1\nattrs                             20.3.0\nbackcall                          0.2.0\nAdditional resources\nInstalling Python packages on your notebook server\n3.8. INSTALLING PYTHON PACKAGES ON YOUR NOTEBOOK SERVER\nYou can install Python packages that are not part of the default notebook server image by adding the\npackage and the version to a \nrequirements.txt\n file and then running the \npip install\n command in a\nnotebook cell.\nNOTE\nYou can also install packages directly, but Red Hat recommends using a \nrequirements.txt\nfile so that the packages stated in the file can be easily re-used across different\nnotebooks. In addition, using a \nrequirements.txt\n file is also useful when using a S2I build\nto deploy a model.\nPrerequisites\nLog in to Jupyter and open a notebook.\nProcedure\n1\n. \nCreate a new text file using one of the following methods:\nClick \n+\n to open a new launcher and click \nText file\n.\nClick \nFile\n \n→\n \nNew\n \n→\n \nText File\n.\n2\n. \nRename the text file to \nrequirements.txt\n.\na\n. \nRight-click on the name of the file and click \nRename Text\n. The \nRename File\n dialog opens.\nb\n. \nEnter \nrequirements.txt\n in the \nNew Name\n field and click \nRename\n.\n3\n. \nAdd the packages to install to the \nrequirements.txt\n file.\naltair\nYou can specify the exact version to install by using the \n==\n (equal to) operator, for example:\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n31",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 35
      },
      "page_content": "altair==4.1.0\nNOTE\nRed Hat recommends specifying exact package versions to enhance the stability\nof your notebook server over time. New package versions can introduce\nundesirable or unexpected changes in your environment’s behavior.\nTo install multiple packages at the same time, place each package on a separate line.\n4\n. \nInstall the packages in \nrequirements.txt\n to your server using a notebook cell.\na\n. \nCreate a new cell in your notebook and enter the following command:\n!pip install -r requirements.txt\nb\n. \nRun the cell by pressing Shift and Enter.\nIMPORTANT\nThis command installs the package on your notebook server, but you must still\nrun the \nimport\n directive in a code cell to use the package in your code.\nimport altair\nVerification\nConfirm that the packages in \nrequirements.txt\n appear in the list of packages installed on the\nnotebook server. See \nViewing Python packages installed on your notebook server\n for details.\n3.9. UPDATING NOTEBOOK SERVER SETTINGS BY RESTARTING\nYOUR SERVER\nYou can update the settings on your notebook server by stopping and relaunching the notebook server.\nFor example, if your server runs out of memory, you can restart the server to make the container size\nlarger.\nPrerequisites\nA running notebook server.\nLog in to Jupyter.\nProcedure\n1\n. \nClick \nFile\n \n→\n \nHub Control Panel\n.\nThe \nNotebook server control panel\n opens.\n2\n. \nClick the \nStop notebook server\n button.\nThe \nStop server\n dialog opens.\n3\n. \nClick \nStop server\n to confirm your decision.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n32",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 36
      },
      "page_content": "The \nStart a notebook server\n page opens.\n4\n. \nUpdate the relevant notebook server settings and click \nStart server\n.\nVerification\nThe notebook server starts and contains your updated settings.\nAdditional resources\nLaunching Jupyter and starting a notebook server\nCHAPTER 3. WORKING ON DATA SCIENCE PROJECTS\n33",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 37
      },
      "page_content": "CHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\nAs a data scientist, you can enhance your data science projects on OpenShift AI by building portable\nmachine learning (ML) workflows with data science pipelines, using Docker containers. This enables you\nto standardize and automate machine learning workflows to enable you to develop and deploy your data\nscience models.\nFor example, the steps in a machine learning workflow might include items such as data extraction, data\nprocessing, feature extraction, model training, model validation, and model serving. Automating these\nactivities enables your organization to develop a continuous process of retraining and updating a model\nbased on newly received data. This can help address challenges related to building an integrated\nmachine learning deployment and continuously operating it in production.\nYou can also use the Elyra JupyterLab extension to create and run data science pipelines within\nJupyterLab. For more information, see \nWorking with pipelines in JupyterLab\n.\nA data science pipeline in OpenShift AI consists of the following components:\nPipeline server: A server that is attached to your data science project and hosts your data\nscience pipeline.\nPipeline: A pipeline defines the configuration of your machine learning workflow and the\nrelationship between each component in the workflow.\nPipeline code: A definition of your pipeline in a Tekton-formatted YAML file.\nPipeline graph: A graphical illustration of the steps executed in a pipeline run and the\nrelationship between them.\nPipeline run: An execution of your pipeline.\nTriggered run: A previously executed pipeline run.\nScheduled run: A pipeline run scheduled to execute at least once.\nThis feature is based on Kubeflow Pipelines v1. Use the Kubeflow Pipelines SDK to build your data\nscience pipeline in Python code. After you have built your pipeline, compile it into Tekton-formatted\nYAML code using kfp-tekton SDK (version 1.5.x only). The OpenShift AI user interface enables you to\ntrack and manage pipelines and pipeline runs. You can manage incremental changes to pipelines in\nOpenShift AI by using versioning. This allows you to develop and deploy pipelines iteratively, preserving a\nrecord of your changes.\nBefore you can use data science pipelines, you must install the OpenShift Pipelines operator. For more\ninformation about installing a compatible version of the OpenShift Pipelines operator, see \nRed Hat\nOpenShift Pipelines release notes\n and \nRed Hat OpenShift AI: Supported Configurations\n.\nYou can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not\nconsume local storage. To do this, you must first configure write access to your S3 bucket on your\nstorage account.\n4.1. MANAGING DATA SCIENCE PIPELINES\n4.1.1. Configuring a pipeline server\nBefore you can successfully create a pipeline in OpenShift AI, you must configure a pipeline server. This\nincludes configuring where your pipeline artifacts and data are stored.\nNOTE\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n34",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 38
      },
      "page_content": "NOTE\nAfter the pipeline server is created, the \n/metadata\n and \n/artifacts\n folders are automatically\ncreated in the default \nroot\n folder. Therefore, you are not required to specify any storage\ndirectories when configuring a data connection for your pipeline server.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that you can add a pipeline server to.\nYou have an existing S3-compatible object storage bucket and you have configured write\naccess to your S3 bucket on your storage account.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Projects\n.\nThe \nData science projects\n page opens.\n2\n. \nClick the name of the project that you want to configure a pipeline server for.\nA project details page opens.\n3\n. \nIn the \nPipelines\n section, click \nConfigure a pipeline server\n.\nThe \nConfigure pipeline server\n dialog appears.\n4\n. \nIn the \nObject storage connection\n section, provide values for the mandatory fields:\na\n. \nIn the \nAccess key\n field, enter the access key ID for the S3-compatible object storage\nprovider.\nb\n. \nIn the \nSecret key\n field, enter the secret access key for the S3-compatible object storage\naccount that you specified.\nc\n. \nIn the \nEndpoint\n field, enter the endpoint of your S3-compatible object storage bucket.\nd\n. \nIn the \nBucket\n field, enter the name of your S3-compatible object storage bucket.\nIMPORTANT\nIf you specify incorrect data connection settings, you cannot update these\nsettings on the same pipeline server. Therefore, you must delete the pipeline\nserver and configure another one.\n5\n. \nIn the \nDatabase\n section, click \nShow advanced database options\n to specify the database to\nstore your pipeline data and select one of the following sets of actions:\nSelect \nUse default database stored on your cluster\n to deploy a MariaDB database in your\nproject.\nSelect \nConnect to external MySQL database\n to add a new connection to an external\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n35",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 39
      },
      "page_content": "Select \nConnect to external MySQL database\n to add a new connection to an external\ndatabase that your pipeline server can access.\ni\n. \nIn the \nHost\n field, enter the database’s host name.\nii\n. \nIn the \nPort\n field, enter the database’s port.\niii\n. \nIn the \nUsername\n field, enter the default user name that is connected to the database.\niv\n. \nIn the \nPassword\n field, enter the password for the default user account.\nv\n. \nIn the \nDatabase\n field, enter the database name.\n6\n. \nClick \nConfigure\n.\nVerification\nThe pipeline server that you configured is displayed in the \nPipelines\n section on the project\ndetails page.\nThe \nImport pipeline\n button is available in the \nPipelines\n section on the project details page.\n4.1.2. Defining a pipeline\nThe Kubeflow Pipelines SDK enables you to define end-to-end machine learning and data pipelines. Use\nthe Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your\npipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). After\ndefining the pipeline, you can import the YAML file to the OpenShift AI dashboard to enable you to\nconfigure its execution settings. For more information about installing and using Kubeflow Pipelines SDK\nfor Tetkon, see \nKubeflow Pipelines SDK for Tekton\n.\nYou can also use the Elyra JupyterLab extension to create and run data science pipelines within\nJupyterLab. For more information on creating pipelines in JupyterLab, see \nWorking with pipelines in\nJupyterLab\n. For more information on the Elyra JupyterLab extension, see \nElyra Documentation\n.\nAdditional resources\nKubeflow Pipelines SDK for Tekton\nKFP Tekton samples and compiler samples\nKubeflow Pipelines v1 Documentation\nElyra Documentation\n4.1.3. Importing a data science pipeline\nTo help you begin working with data science pipelines in OpenShift AI, you can import a YAML file\ncontaining your pipeline’s code to an active pipeline server. This file contains a Kubeflow pipeline\ncompiled with the Tekton compiler. After you have imported the pipeline to a pipeline server, you can\nexecute the pipeline by creating a pipeline run.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n36",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 40
      },
      "page_content": "You have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to import a pipeline to.\n3\n. \nClick \nImport pipeline\n.\nThe \nImport pipeline\n dialog opens.\n4\n. \nEnter the details for the pipeline that you are importing.\na\n. \nIn the \nPipeline name\n field, enter a name for the pipeline that you are importing.\nb\n. \nIn the \nPipeline description\n field, enter a description for the pipeline that you are importing.\nc\n. \nClick \nUpload\n. Alternatively, drag the file from your local machine’s file system and drop it in\nthe designated area in the \nImport pipeline\n dialog.\nA file browser opens.\nd\n. \nNavigate to the file containing the pipeline code and click \nSelect\n.\ne\n. \nClick \nImport pipeline\n.\nVerification\nThe pipeline that you imported is displayed on the \nPipelines\n page.\n4.1.4. Downloading a data science pipeline\nTo make further changes to a data science pipeline that you previously uploaded to OpenShift AI, you\ncan download the pipeline’s code from the user interface.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nYou have created and imported a pipeline to an active pipeline server that is available to\ndownload.\nProcedure\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n37",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 41
      },
      "page_content": "Procedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project whose pipeline that you want to download.\n3\n. \nIn the \nPipeline name\n column, click the name of the pipeline that you want to download.\nThe \nPipeline details\n page opens displaying the \nGraph\n tab.\n4\n. \nClick the \nYAML\n tab.\nThe page reloads to display an embedded YAML editor showing the pipeline code.\n5\n. \nClick the \nDownload\n button ( \n \n) to download the YAML file containing your pipeline’s code\nto your local machine.\nVerification\nThe pipeline code is downloaded to your browser’s default directory for downloaded files.\n4.1.5. Deleting a data science pipeline\nYou can delete data science pipelines so that they do not appear on the \nData Science Pipelines\n page.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nThere are active pipelines available on the \nPipelines\n page.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that contains the pipeline that you want to delete.\n3\n. \nClick the action menu (\n⋮\n) beside the pipeline that you want to delete and click \nDelete pipeline\n.\nThe \nDelete pipeline\n dialog opens.\n4\n. \nEnter the pipeline name in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete pipeline\n.\nVerification\nThe data science pipeline that you deleted is no longer displayed on the \nPipelines\n page.\n4.1.6. Deleting a pipeline server\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n38",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 42
      },
      "page_content": "After you have finished running your data science pipelines, you can delete the pipeline server. Deleting a\npipeline server automatically deletes all of its associated pipelines, pipeline versions, and runs. If your\npipeline data is stored in a database, the database is also deleted along with its meta-data. In addition,\nafter deleting a pipeline server, you cannot create new pipelines or pipeline runs until you create another\npipeline server.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project whose pipeline server you want to delete.\n3\n. \nFrom the \nPipeline server actions\n list, select \nDelete pipeline server\n.\nThe \nDelete pipeline server\n dialog opens.\n4\n. \nEnter the pipeline server’s name in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete\n.\nVerification\nPipelines previously assigned to the deleted pipeline server are no longer displayed on the\nPipelines\n page for the relevant data science project.\nPipeline runs previously assigned to the deleted pipeline server are no longer displayed on the\nRuns\n page for the relevant data science project.\n4.1.7. Viewing the details of a pipeline server\nYou can view the details of pipeline servers configured in OpenShift AI, such as the pipeline’s data\nconnection details and where its data is stored.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nYou have previously created a data science project that contains an active and available pipeline\nserver.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n39",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 43
      },
      "page_content": "Procedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project whose pipeline server you want to view.\n3\n. \nFrom the \nPipeline server actions\n list, select \nView pipeline server configuration\n.\n4\n. \nWhen you have finished inspecting the pipeline server’s details, click \nDone\n.\nVerification\nYou can view the relevant pipeline server’s details in the \nView pipeline server\n dialog.\n4.1.8. Viewing existing pipelines\nYou can view the details of pipelines that you have imported to Red Hat OpenShift AI, such as the\npipeline’s last run, when it was created, and the pipeline’s executed runs.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have imported a pipeline to an active and available pipeline server.\nThe pipeline you imported is available, or there are other previously imported pipelines available\nto view.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the relevant project whose pipelines you want to view.\n3\n. \nStudy the pipelines on the list.\n4\n. \nOptional: Click \nExpand\n ( \n \n) on the relevant row to view the pipeline’s executed runs. If the\npipeline does not contain any runs, click \nCreate run\n to create one.\nVerification\nA list of previously created data science pipelines is displayed on the \nPipelines\n page.\n4.1.9. Overview of pipeline versions\nYou can manage incremental changes to pipelines in OpenShift AI by using versioning. This allows you to\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n40",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 44
      },
      "page_content": "You can manage incremental changes to pipelines in OpenShift AI by using versioning. This allows you to\ndevelop and deploy pipelines iteratively, preserving a record of your changes. You can track and manage\nyour changes on the OpenShift AI dashboard, allowing you to schedule and execute runs against all\navailable versions of your pipeline.\n4.1.10. Uploading a pipeline version\nYou can upload a YAML file to an active pipeline server that contains the latest version of your pipeline.\nThis file consists of a Kubeflow pipeline compiled with the Tekton compiler. After you upload a pipeline\nversion to a pipeline server, you can execute it by creating a pipeline run.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nYou have a pipeline version available and ready to upload.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to upload a pipeline version to.\n3\n. \nClick the \nImport pipeline\n dropdown list and select \nUpload new version\n.\nThe \nUpload new version\n dialog opens.\n4\n. \nEnter the details for the pipeline version that you are uploading.\na\n. \nFrom the \nPipeline\n list, select the pipeline that you want to upload your pipeline version to.\nb\n. \nIn the \nPipeline version name\n field, confirm the name for the pipeline version, and change it\nif necessary.\nc\n. \nIn the \nPipeline version description\n field, enter a description for the pipeline version.\nd\n. \nClick \nUpload\n. Alternatively, drag the file from your local machine’s file system and drop it in\nthe designated area in the \nUpload new version\n dialog.\nA file browser opens.\ne\n. \nNavigate to the file containing the pipeline version code and click \nSelect\n.\nf\n. \nClick \nUpload\n.\nVerification\nThe pipeline version that you uploaded is displayed on the \nPipelines\n page. Click \nExpand\n ( \n \n)\non the row containing the pipeline to view its versions.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n41",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 45
      },
      "page_content": "4.1.11. Deleting a pipeline version\nYou can delete specific versions of a pipeline when you no longer require them. Deleting a default\npipeline version automatically changes the default pipeline version to the next most recent version. If no\npipeline versions exist, the pipeline persists without a default version.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have imported a pipeline to an active and available pipeline server.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that contains a version of a pipeline that you want to\ndelete.\n3\n. \nOn the row containing the pipeline, click \nExpand\n ( \n \n).\n4\n. \nOn the row containing the pipeline version that you want to delete, select the checkbox.\n5\n. \nClick the action menu (\n⋮\n) next to the \nImport pipeline\n dropdown and select \nDelete selected\nfrom the list.\nThe \nDelete pipeline version\n dialog opens.\n6\n. \nEnter the name of the pipeline version in the text field to confirm that you intend to delete it.\n7\n. \nClick \nDelete\n.\nVerification\nThe pipeline version that you deleted no longer appears on the \nPipelines\n page.\n4.1.12. Viewing pipeline versions\nYou can view all versions for a pipeline on the \nPipelines\n page.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n42",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 46
      },
      "page_content": "If you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have a pipeline available on an active and available pipeline server.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project containing the pipeline versions that you want to view.\n3\n. \nClick \nExpand\n ( \n \n) on the row containing the pipeline that you want to view versions for.\nVerification\nYou can view the versions of the pipeline on the \nPipelines\n page.\n4.1.13. Viewing the details of a pipeline version\nYou can view the details of a pipeline version that you have uploaded to Red Hat OpenShift AI, such as\nits graph and YAML code.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have a pipeline available on an active and available pipeline server.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project containing the pipeline versions that you want to view\ndetails for.\n3\n. \nClick \nExpand\n ( \n \n) on the row containing the pipeline that you want to view versions for.\n4\n. \nClick the pipeline version that you want to view the details of.\nThe \nPipeline details\n page opens, displaying the \nGraph\n and \nYAML\n tabs.\nVerification\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n43",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 47
      },
      "page_content": "On the \nPipeline details\n page, you can view the pipeline graph and YAML code.\n4.2. MANAGING PIPELINE RUNS\n4.2.1. Overview of pipeline runs\nA pipeline run is a single execution of a data science pipeline. As data scientist, you can use OpenShift AI\nto define, manage, and track executions of a data science pipeline. You can view a record of your data\nscience project’s previously executed and scheduled runs from the \nRuns\n page in the OpenShift AI user\ninterface.\nRuns are intended for portability. Therefore, you can clone your pipeline runs to reproduce and scale\nthem accordingly, or delete them when you longer require them. You can configure a run to execute only\nonce immediately after creation or on a recurring basis. Recurring runs consist of a copy of a pipeline\nwith all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes.\nYou can define the following run triggers:\nPeriodic: used for scheduling runs to execute in intervals.\nCron: used for scheduling runs as a cron job.\nWhen executed, you can track the run’s progress from the run’s \nDetails\n page on the OpenShift AI user\ninterface. From here, you can view the run’s graph, and output artifacts.\nA pipeline run can be classified as the following:\nScheduled run: A pipeline run scheduled to execute at least once\nTriggered run: A previously executed pipeline run.\nYou can review and analyze logs for each step in a triggered pipeline run. With the log viewer, you can\nsearch for specific log messages, view the log for each step, and download the step logs to your local\nmachine.\n4.2.2. Scheduling a pipeline run using a cron job\nYou can use a cron job to schedule a pipeline run to execute at a specific time. Cron jobs are useful for\ncreating periodic and recurring tasks, and can also schedule individual tasks for a specific time, such as if\nyou want to schedule a run for a low activity period. To successfully execute runs in OpenShift AI, you\nmust use the supported format. See \nCron Expression Format\n for more information.\nThe following examples show the correct format:\nRun occurrence\nCron format\nEvery five minutes\n@every 5m\nEvery 10 minutes\n0 */10 * * * *\nDaily at 16:16 UTC\n0 16 16 * * *\nDaily every quarter of the hour\n0 0,15,30,45 * * * *\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n44",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 48
      },
      "page_content": "On Monday and Tuesday at 15:40 UTC\n0 40 15 * * MON,TUE\nRun occurrence\nCron format\nAdditional resources\nCron Expression Format\n4.2.3. Scheduling a pipeline run\nYou can instantiate a single execution of a pipeline by scheduling a pipeline run. In OpenShift AI, you can\nschedule runs to occur at specific times or execute them immediately after creation.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nYou have imported a pipeline to an active pipeline server.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to create a run for.\n3\n. \nClick the action menu (\n⋮\n) beside the relevant pipeline and click \nCreate run\n.\nThe \nCreate run\n page opens.\n4\n. \nIn the \nName\n field, enter a name for the run.\n5\n. \nIn the \nDescription\n field, enter a description for the run.\n6\n. \nFrom the \nPipeline\n list, select the pipeline that you want to create a run for. Alternatively, to\ncreate a new pipeline, click \nCreate new pipeline\n and complete the relevant fields in the \nImport\npipeline\n dialog.\n7\n. \nFrom the \nPipeline version\n list, select the pipeline version to create a run for. Alternatively, to\nupload a new version, click \nUpload new version\n and complete the relevant fields in the \nUpload\nnew version\n dialog.\n8\n. \nConfigure the run type by performing one of the following sets of actions:\nSelect \nRun once immediately after creation\n to specify the run executes once, and\nimmediately after its creation.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n45",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 49
      },
      "page_content": "Select \nSchedule recurring run\n to schedule the run to recur.\ni\n. \nConfigure the run’s trigger type.\nA\n. \nSelect \nPeriodic\n to specify an execution frequency. Enter a numerical value and\nselect an execution frequency from the list.\nB\n. \nSelect \nCron\n to specify the execution schedule in \ncron\n format. This creates a cron\njob to execute the run. Click the \nCopy\n button ( \n \n) to copy the cron job schedule\nto the clipboard. The field furthest to the left represents seconds. For more\ninformation about scheduling tasks using the supported \ncron\n format, see \nCron\nExpression Format\n.\nii\n. \nConfigure the run’s duration.\nA\n. \nSelect the \nStart date\n check box to specify a start date for the run. Select the run’s\nstart date using the \nCalendar\n and the start time from the list of times.\nB\n. \nSelect the \nEnd date\n check box to specify an end date for the run. Select the run’s\nend date using the \nCalendar\n and the end time from the list of times.\n9\n. \nConfigure the input parameters for the run by selecting the parameters from the list.\n10\n. \nClick \nCreate\n.\nVerification\nThe pipeline run that you created is shown in the \nScheduled\n tab on the \nRuns\n page.\n4.2.4. Cloning a scheduled pipeline run\nTo make it easier to schedule runs to execute as part of your pipeline configuration, you can duplicate\nexisting scheduled runs by cloning them.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nYou have imported a pipeline to an active pipeline server.\nYou have previously scheduled a run that is available to clone.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nRuns\n.\nThe \nRuns\n page opens.\n2\n. \nClick the action menu (\n⋮\n) beside the relevant run and click \nClone\n.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n46",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 50
      },
      "page_content": "The \nClone\n page opens.\n3\n. \nFrom the \nProject\n list, select the project that contains the pipeline whose run that you want to\nclone.\n4\n. \nIn the \nName\n field, enter a name for the run that you want to clone.\n5\n. \nIn the \nDescription\n field, enter a description for the run that you want to clone.\n6\n. \nFrom the \nPipeline\n list, select the pipeline containing the run that you want to clone.\n7\n. \nTo configure the run type for the run that you are cloning, in the \nRun type\n section, perform one\nof the following sets of actions:\nSelect \nRun once immediately after create\n to specify the run that you are cloning executes\nonce, and immediately after its creation. If you selected this option, skip to step 10.\nSelect \nSchedule recurring run\n to schedule the run that you are cloning to recur.\n8\n. \nIf you selected \nSchedule recurring run\n in the previous step, to configure the trigger type for\nthe run, perform one of the following actions:\nSelect \nPeriodic\n and select the execution frequency from the \nRun every\n list.\nSelect \nCron\n to specify the execution schedule in \ncron\n format. This creates a cron job to\nexecute the run. Click the \nCopy\n button ( \n \n) to copy the cron job schedule to the\nclipboard. The field furthest to the left represents seconds. For more information about\nscheduling tasks using the supported \ncron\n format, see \nCron Expression Format\n.\n9\n. \nIf you selected \nSchedule recurring run\n in step 7, configure the duration for the run that you are\ncloning.\na\n. \nSelect the \nStart date\n check box to specify a start date for the run. Select the start date\nusing the calendar tool and the start time from the list of times.\nb\n. \nSelect the \nEnd date\n check box to specify an end date for the run. Select the end date using\nthe calendar tool and the end time from the list of times.\n10\n. \nIn the \nParameters\n section, configure the input parameters for the run that you are cloning by\nselecting the appropriate parameters from the list.\n11\n. \nClick \nCreate\n.\nVerification\nThe pipeline run that you cloned is shown in the \nScheduled\n tab on the \nRuns\n page.\n4.2.5. Stopping a triggered pipeline run\nIf you no longer require a triggered pipeline run to continue executing, you can stop the run before its\ndefined end date.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n47",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 51
      },
      "page_content": "You have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nThere is a previously created data science project available that contains a pipeline server.\nYou have imported a pipeline to an active and available pipeline server.\nYou have previously triggered a pipeline run.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nRuns\n.\nThe \nRuns\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that contains the pipeline whose triggered run you want\nto stop.\nThe page refreshes to show the pipeline’s triggered runs on the \nTriggered\n tab.\n3\n. \nClick the action menu (\n⋮\n) beside the triggered run that you want to delete and click \nStop\n.\nThere might be a short delay while the run stops.\nVerification\nA list of previously triggered runs are displayed in the \nTriggered\n tab on the \nRuns\n page.\n4.2.6. Deleting a scheduled pipeline run\nTo discard pipeline runs that you previously scheduled, but no longer require, you can delete them so\nthat they do not appear on the \nRuns\n page.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nYou have imported a pipeline to an active pipeline server.\nYou have previously scheduled a run that is available to delete.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nRuns\n.\nThe \nRuns\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that contains the pipeline whose scheduled run you\nwant to delete.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n48",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 52
      },
      "page_content": "The page refreshes to show the pipeline’s scheduled runs on the \nScheduled\n tab.\n3\n. \nClick the action menu (\n⋮\n) beside the scheduled run that you want to delete and click \nDelete\n.\nThe \nDelete scheduled run\n dialog opens.\n4\n. \nEnter the run’s name in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete scheduled run\n.\nVerification\nThe run that you deleted is no longer displayed on the \nScheduled\n tab.\n4.2.7. Deleting a triggered pipeline run\nTo discard pipeline runs that you previously executed, but no longer require a record of, you can delete\nthem so that they do not appear on the \nTriggered\n tab on the \nRuns\n page.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a configured\npipeline server.\nYou have imported a pipeline to an active pipeline server.\nYou have previously executed a run that is available to delete.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nRuns\n.\nThe \nRuns\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that contains the pipeline whose triggered run you want\nto delete.\nThe page refreshes to show the pipeline’s triggered runs on the \nTriggered\n tab.\n3\n. \nClick the action menu (\n⋮\n) beside the triggered run that you want to delete and click \nDelete\n.\nThe \nDelete triggered run\n dialog opens.\n4\n. \nEnter the run’s name in the text field to confirm that you intend to delete it.\n5\n. \nClick \nDelete triggered run\n.\nVerification\nThe run that you deleted is no longer displayed on the \nTriggered\n tab.\n4.2.8. Viewing scheduled pipeline runs\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n49",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 53
      },
      "page_content": "You can view a list of pipeline runs that are scheduled for execution in OpenShift AI. From this list, you\ncan view details relating to your pipeline runs, such as the pipeline version that the run belongs to. You\ncan also view the run status, execution frequency, and schedule.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou have installed the OpenShift Pipelines operator.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have imported a pipeline to an active and available pipeline server.\nYou have created and scheduled a pipeline run.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nRuns\n.\nThe \nRuns\n page opens.\n2\n. \nFrom the \nProject\n list, select the project whose scheduled pipeline runs you want to view.\n3\n. \nClick the \nScheduled\n tab.\n4\n. \nStudy the table showing a list of scheduled runs.\nAfter a run has been scheduled, the run’s status is displayed in the \nStatus\n column in the table,\nindicating whether the run is ready for execution or unavailable for execution. To enable or\ndisable a previously imported notebook image, on the row containing the relevant notebook\nimage, click the toggle in the \nEnabled\n column.\nVerification\nA list of scheduled runs are displayed in the \nScheduled\n tab on the \nRuns\n page.\n4.2.9. Viewing triggered pipeline runs\nYou can view a list of pipeline runs that were previously executed in OpenShift AI. From this list, you can\nview details relating to your pipeline runs, such as the pipeline version that the run belongs to, along with\nthe run status, duration, and execution start time.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou have installed the OpenShift Pipelines operator.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have previously created a data science project that is available and has a pipeline server.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n50",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 54
      },
      "page_content": "You have imported a pipeline to an active and available pipeline server.\nYou have previously triggered a pipeline run.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nRuns\n.\nThe \nRuns\n page opens.\n2\n. \nFrom the \nProject\n list, select the project whose previously executed pipeline runs you want to\nview.\nThe \nRun details\n page opens.\n3\n. \nClick the \nTriggered\n tab.\nA table opens that shows list of triggered runs. After a run has completed its execution, the run’s\nstatus is displayed in the \nStatus\n column in the table, indicating whether the run has succeeded\nor failed.\nVerification\nA list of previously triggered runs are displayed in the \nTriggered\n tab on the \nRuns\n page.\n4.2.10. Viewing the details of a pipeline run\nTo gain a clearer understanding of your pipeline runs, you can view the details of a previously triggered\npipeline run, such as its graph, execution details, and run output.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have imported a pipeline to an active and available pipeline server.\nYou have previously triggered a pipeline run.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to view run details for.\n3\n. \nFor a pipeline that you want to view run details for, click \nExpand\n ( \n \n).\n4\n. \nClick \nView runs\n on the row containing the project version that you want to view run details for.\nThe \nRuns\n page opens displaying a list of previously executed pipeline runs.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n51",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 55
      },
      "page_content": "5\n. \nClick the name of the run that you want to view the details of.\nThe \nRun details\n page opens.\nVerification\nOn the \nRun details\n page, you can view the run’s graph, execution details, input parameters, step\nlogs, and run output.\n4.2.11. About pipeline logs\nYou can review and analyze step logs for each step in a triggered pipeline run.\nTo help you troubleshoot and audit your pipelines, you can review and analyze these step logs by using\nthe log viewer in the OpenShift AI dashboard. From here, you can search for specific log messages, view\nthe log for each step, and download the step logs to your local machine.\nIf the step log file exceeds its capacity, a warning appears above the log viewer stating that the log\nwindow displays partial content. Expanding the warning displays further information, such as how the log\nviewer refreshes every three seconds, and that each step log displays the last 500 lines of log messages\nreceived. In addition, you can click \ndownload all step logs\n to download all step logs to your local\nmachine.\nEach step has a set of container logs. You can view these container logs by selecting a container from\nthe \nSteps\n list in the log viewer. The \nStep-main\n container log consists of the log output for the step. The\nstep-copy-artifact\n container log consists of output relating to artifact data sent to s3-compatible\nstorage. If the data transferred between the steps in your pipeline is larger than 3 KB, five container logs\nare typically available. These logs contain output relating to data transferred between your persistent\nvolume claims (PVCs).\n4.2.12. Viewing pipeline step logs\nTo help you troubleshoot and audit your pipelines, you can review and analyze the log of each pipeline\nstep using the log viewer. From here, you can search for specific log messages and download the logs\nfor each step in your pipeline. If the pipeline is running, you can also pause and resume the log from the\nlog viewer.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have imported a pipeline to an active and available pipeline server.\nYou have previously triggered a pipeline run.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n52",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 56
      },
      "page_content": "The \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to view logs for.\n3\n. \nFor the pipeline that you want to view logs for, click \nExpand\n ( \n \n).\n4\n. \nClick \nView runs\n on the row containing the pipeline version that you want to view logs for.\nThe \nRuns\n page opens displaying a list of previously executed pipeline runs.\n5\n. \nClick the name of the run that you want to view logs for.\nThe \nRun details\n page opens.\n6\n. \nOn the graph, click the pipeline step that you want to view logs for.\nA pane opens displaying information about the pipeline step.\n7\n. \nClick the \nLogs\n tab.\nThe log viewer opens.\n8\n. \nTo view the logs of another pipeline step, from the \nSteps\n list, select the step that you want to\nview logs for.\n9\n. \nAnalyze the log using the log viewer.\nTo search for a specific log message, enter at least part of the message in the search bar.\nTo view the full log in a separate browser window, click the action menu (\n⋮\n) and select\nView raw logs\n. Alternatively, to expand the size of the log viewer, click the action menu (\n⋮\n)\nand select \nExpand\n.\nVerification\nYou can view the logs for each step in your pipeline.\n4.2.13. Downloading pipeline step logs\nInstead of viewing the step logs of a pipeline run using the log viewer on the OpenShift AI dashboard,\nyou can download them for further analysis. You can choose to download the logs belonging to all steps\nin your pipeline, or you can download the log only for the step log displayed in the log viewer.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have previously created a data science project that is available and contains a pipeline\nserver.\nYou have imported a pipeline to an active and available pipeline server.\nYou have previously triggered a pipeline run.\nProcedure\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n53",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 57
      },
      "page_content": "1\n. \nFrom the OpenShift AI dashboard, click \nData Science Pipelines\n \n→\n \nPipelines\n.\nThe \nPipelines\n page opens.\n2\n. \nFrom the \nProject\n list, select the project that you want to download logs for.\n3\n. \nFor the pipeline that you want to download logs for, click \nExpand\n ( \n \n).\n4\n. \nClick \nView runs\n on the row containing the pipeline version that you want to download logs for.\nThe \nRuns\n page opens displaying a list of previously executed pipeline runs.\n5\n. \nClick the name of the run that you want to download logs for.\nThe \nRun details\n page opens.\n6\n. \nOn the graph, click the pipeline step that you want to download logs for.\nA pane opens displaying information about the pipeline step.\n7\n. \nClick the \nLogs\n tab.\nThe log viewer opens.\n8\n. \nClick the \nDownload\n button ( \n \n).\na\n. \nSelect \nDownload current stop log\n to download the log for the current pipeline step.\nb\n. \nSelect \nDownload all step logs\n to download the logs for all steps in your pipeline run.\nVerification\nThe step logs download to your browser’s default directory for downloaded files.\n4.3. WORKING WITH PIPELINES IN JUPYTERLAB\n4.3.1. Overview of pipelines in JupyterLab\nYou can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for\nJupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can be executed in\nOpenShift AI.\nBefore you can work with pipelines in JupyterLab, you must install the OpenShift Pipelines operator. For\nmore information about installing a compatible version of the OpenShift Pipelines operator, see\nRed Hat OpenShift Pipelines release notes\n and \nRed Hat OpenShift AI: Supported Configurations\n.\nYou can access the Elyra extension within JupyterLab when you create the most recent version of one\nof the following notebook images:\nStandard Data Science\nPyTorch\nTensorFlow\nTrustyAI\nAs you can use the Pipeline Editor to visually design your pipelines, minimal coding is required to create\nand run pipelines. For more information about Elyra, see \nElyra Documentation\n. For more information on\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n54",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 58
      },
      "page_content": "the Pipeline Editor, see \nVisual Pipeline Editor\n. After you have created your pipeline, you can run it locally\nin JupyterLab, or remotely using data science pipelines in OpenShift AI.\nThe pipeline creation process consists of the following tasks:\nCreate a data science project that contains a workbench.\nCreate a pipeline server.\nCreate a new pipeline in the Pipeline Editor in JupyterLab.\nDevelop your pipeline by adding Python notebooks or Python scripts and defining their runtime\nproperties.\nDefine execution dependencies.\nRun or export your pipeline.\nBefore you can run a pipeline in JupyterLab, your pipeline instance must contain a runtime\nconfiguration. A runtime configuration defines connectivity information for your pipeline instance and\nS3-compatible cloud storage.\nIf you create a workbench as part of a data science project, a default runtime configuration is created\nautomatically. However, if you create a notebook from the Jupyter tile in the OpenShift AI dashboard,\nyou must create a runtime configuration before you can run your pipeline in JupyterLab. For more\ninformation about runtime configurations, see \nRuntime Configuration\n. As a prerequisite, before you\ncreate a workbench, ensure that you have created and configured a pipeline server within the same data\nscience project as your workbench.\nYou can use S3-compatible cloud storage to make data available to your notebooks and scripts while\nthey are executed. Your cloud storage must be accessible from the machine in your deployment that\nruns JupyterLab and from the cluster that hosts Data Science Pipelines. Before you create and run\npipelines in JupyterLab, ensure that you have your s3-compatible storage credentials readily available.\nAdditional resources\nElyra Documentation\nVisual Pipeline Editor\nRuntime Configuration\n.\n4.3.2. Accessing the pipeline editor\nYou can use Elyra to create visual end-to-end pipeline workflows in JupyterLab. Elyra is an extension for\nJupyterLab that provides you with a Pipeline Editor to create pipeline workflows that can execute in\nOpenShift AI.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n55",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 59
      },
      "page_content": "You have created a data science project.\nYou have created a workbench with the \nStandard Data Science\n notebook image.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nYou have access to S3-compatible storage.\nProcedure\n1\n. \nAfter you open JupyterLab, confirm that the JupyterLab launcher is automatically displayed.\n2\n. \nIn the \nElyra\n section of the JupyterLab launcher, click the \nPipeline Editor\n tile.\nThe Pipeline Editor opens.\nVerification\nYou can view the Pipeline Editor in JupyterLab.\n4.3.3. Creating a runtime configuration\nIf you create a workbench as part of a data science project, a default runtime configuration is created\nautomatically. However, if you create a notebook from the Jupyter tile in the OpenShift AI dashboard,\nyou must create a runtime configuration before you can run your pipeline in JupyterLab. This enables\nyou to specify connectivity information for your pipeline instance and S3-compatible cloud storage.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have access to S3-compatible cloud storage.\nYou have created a data science project that contains a workbench.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nProcedure\n1\n. \nIn the left sidebar of JupyterLab, click \nRuntimes\n ( \n \n).\n2\n. \nClick the \nCreate new runtime configuration\n button ( \n \n).\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n56",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 60
      },
      "page_content": "The \nAdd new Data Science Pipelines runtime configuration\n page opens.\n3\n. \nComplete the relevant fields to define your runtime configuration.\na\n. \nIn the \nDisplay Name\n field, enter a name for your runtime configuration.\nb\n. \nOptional: In the \nDescription\n field, enter a description to define your runtime configuration.\nc\n. \nOptional: In the \nTags\n field, click \nAdd Tag\n to define a category for your pipeline instance.\nEnter a name for the tag and press Enter.\nd\n. \nDefine the credentials of your data science pipeline:\ni\n. \nIn the \nData Science Pipelines API Endpoint\n field, enter the API endpoint of your data\nscience pipeline. Do not specify the pipelines namespace in this field.\nii\n. \nIn the \nPublic Data Science Pipelines API Endpoint\n field, enter the public API endpoint\nof your data science pipeline.\nIMPORTANT\nYou can obtain the Data Science Pipelines API endpoint from the \nData\nScience Pipelines\n \n→\n \nRuns\n page in the dashboard. Copy the relevant end\npoint and enter it in the \nPublic Data Science Pipelines API Endpoint\nfield.\niii\n. \nOptional: In the \nData Science Pipelines User Namespace\n field, enter the relevant user\nnamespace to run pipelines.\niv\n. \nFrom the \nData Science Pipelines engine\n list, select \nTekton\n.\nv\n. \nFrom the \nAuthentication Type\n list, select the authentication type required to\nauthenticate your pipeline.\nIMPORTANT\nIf you created a notebook directly from the Jupyter tile on the\ndashboard, select \nEXISTING_BEARER_TOKEN\n from the\nAuthentication Type\n list.\nvi\n. \nIn the \nData Science Pipelines API Endpoint Username\n field, enter the user name\nrequired for the authentication type.\nvii\n. \nIn the \nData Science Pipelines API Endpoint Password Or Token\n, enter the password\nor token required for the authentication type.\nIMPORTANT\nTo obtain the Data Science Pipelines API endpoint token, in the upper-\nright corner of the OpenShift web console, click your user name and\nselect \nCopy login command\n. After you have logged in, click \nDisplay\ntoken\n and copy the value of \n--token=\n from the \nLog in with this token\ncommand.\ne\n. \nDefine the connectivity information of your S3-compatible storage:\ni\n. \nIn the \nCloud Object Storage Endpoint\n field, enter the endpoint of your S3-compatible\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n57",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 61
      },
      "page_content": "i\n. \nIn the \nCloud Object Storage Endpoint\n field, enter the endpoint of your S3-compatible\nstorage. For more information about Amazon s3 endpoints, see \nAmazon Simple Storage\nService endpoints and quotas\n.\nii\n. \nOptional: In the \nPublic Cloud Object Storage Endpoint\n field, enter the URL of your\nS3-compatible storage.\niii\n. \nIn the \nCloud Object Storage Bucket Name\n field, enter the name of the bucket where\nyour pipeline artifacts are stored. If the bucket name does not exist, it is created\nautomatically.\niv\n. \nFrom the \nCloud Object Storage Authentication Type\n list, select the authentication\ntype required to access to your S3-compatible cloud storage. If you use AWS S3\nbuckets, select \nKUBERNETES_SECRET\n from the list.\nv\n. \nIn the \nCloud Object Storage Credentials Secret\n field, enter the secret that contains\nthe storage user name and password. This secret is defined in the relevant user\nnamespace, if applicable. In addition, it must be stored on the cluster that hosts your\npipeline runtime.\nvi\n. \nIn the \nCloud Object Storage Username\n field, enter the user name to connect to your\nS3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS\nSecret Access Key ID.\nvii\n. \nIn the \nCloud Object Storage Password\n field, enter the password to connect to your\nS3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS\nSecret Access Key.\nf\n. \nClick \nSave & Close\n.\nVerification\nThe runtime configuration that you created is shown in the \nRuntimes\n tab ( \n \n) in the left\nsidebar of JupyterLab.\n4.3.4. Updating a runtime configuration\nTo ensure that your runtime configuration is accurate and updated, you can change the settings of an\nexisting runtime configuration.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have access to S3-compatible storage.\nYou have created a data science project that contains a workbench.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n58",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 62
      },
      "page_content": "A previously created runtime configuration is available in the JupyterLab interface.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nProcedure\n1\n. \nIn the left sidebar of JupyterLab, click \nRuntimes\n ( \n \n).\n2\n. \nHover the cursor over the runtime configuration that you want to update and click the \nEdit\nbutton ( \n \n).\nThe \nData Science Pipelines runtime configuration\n page opens.\n3\n. \nFill in the relevant fields to update your runtime configuration.\na\n. \nIn the \nDisplay Name\n field, update name for your runtime configuration, if applicable.\nb\n. \nOptional: In the \nDescription\n field, update the description of your runtime configuration, if\napplicable.\nc\n. \nOptional: In the \nTags\n field, click \nAdd Tag\n to define a category for your pipeline instance.\nEnter a name for the tag and press Enter.\nd\n. \nDefine the credentials of your data science pipeline:\ni\n. \nIn the \nData Science Pipelines API Endpoint\n field, update the API endpoint of your\ndata science pipeline, if applicable. Do not specify the pipelines namespace in this field.\nii\n. \nIn the \nPublic Data Science Pipelines API Endpoint\n field, update the API endpoint of\nyour data science pipeline, if applicable.\niii\n. \nOptional: In the \nData Science Pipelines User Namespace\n field, update the relevant\nuser namespace to run pipelines, if applicable.\niv\n. \nFrom the \nData Science Pipelines engine\n list, select \nTekton\n.\nv\n. \nFrom the \nAuthentication Type\n list, select a new authentication type required to\nauthenticate your pipeline, if applicable.\nIMPORTANT\nIf you created a notebook directly from the Jupyter tile on the\ndashboard, select \nEXISTING_BEARER_TOKEN\n from the\nAuthentication Type\n list.\nvi\n. \nIn the \nData Science Pipelines API Endpoint Username\n field, update the user name\nrequired for the authentication type, if applicable.\nvii\n. \nIn the \nData Science Pipelines API Endpoint Password Or Token\n, update the password\nor token required for the authentication type, if applicable.\nIMPORTANT\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n59",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 63
      },
      "page_content": "IMPORTANT\nTo obtain the Data Science Pipelines API endpoint token, in the upper-\nright corner of the OpenShift web console, click your user name and\nselect \nCopy login command\n. After you have logged in, click \nDisplay\ntoken\n and copy the value of \n--token=\n from the \nLog in with this token\ncommand.\ne\n. \nDefine the connectivity information of your S3-compatible storage:\ni\n. \nIn the \nCloud Object Storage Endpoint\n field, update the endpoint of your S3-\ncompatible storage, if applicable. For more information about Amazon s3 endpoints,\nsee \nAmazon Simple Storage Service endpoints and quotas\n.\nii\n. \nOptional: In the \nPublic Cloud Object Storage Endpoint\n field, update the URL of your\nS3-compatible storage, if applicable.\niii\n. \nIn the \nCloud Object Storage Bucket Name\n field, update the name of the bucket where\nyour pipeline artifacts are stored, if applicable. If the bucket name does not exist, it is\ncreated automatically.\niv\n. \nFrom the \nCloud Object Storage Authentication Type\n list, update the authentication\ntype required to access to your S3-compatible cloud storage, if applicable. If you use\nAWS S3 buckets, you must select \nUSER_CREDENTIALS\n from the list.\nv\n. \nOptional: In the \nCloud Object Storage Credentials Secret\n field, update the secret that\ncontains the storage user name and password, if applicable. This secret is defined in the\nrelevant user namespace. You must save the secret on the cluster that hosts your\npipeline runtime.\nvi\n. \nOptional: In the \nCloud Object Storage Username\n field, update the user name to\nconnect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets,\nupdate your AWS Secret Access Key ID.\nvii\n. \nOptional: In the \nCloud Object Storage Password\n field, update the password to connect\nto your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, update\nyour AWS Secret Access Key.\nf\n. \nClick \nSave & Close\n.\nVerification\nThe runtime configuration that you updated is shown in the \nRuntimes\n tab ( \n \n) in the left\nsidebar of JupyterLab.\n4.3.5. Deleting a runtime configuration\nAfter you have finished using your runtime configuration, you can delete it from the JupyterLab\ninterface. After deleting a runtime configuration, you cannot run pipelines in JupyterLab until you create\nanother runtime configuration.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n60",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 64
      },
      "page_content": "You have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have created a data science project that contains a workbench.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nA previously created runtime configuration is visible in the JupyterLab interface.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nProcedure\n1\n. \nIn the left sidebar of JupyterLab, click \nRuntimes\n ( \n \n).\n2\n. \nHover the cursor over the runtime configuration that you want to delete and click the \nDelete\nItem\n button ( \n \n).\nA dialog box appears prompting you to confirm the deletion of your runtime configuration.\n3\n. \nClick \nOK\n.\nVerification\nThe runtime configuration that you deleted is no longer shown in the \nRuntimes\n tab ( \n \n) in\nthe left sidebar of JupyterLab.\n4.3.6. Duplicating a runtime configuration\nTo prevent you from re-creating runtime configurations with similar values in their entirety, you can\nduplicate an existing runtime configuration in the JupyterLab interface.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that contains a workbench.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nA previously created runtime configuration is visible in the JupyterLab interface.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n61",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 65
      },
      "page_content": "Procedure\n1\n. \nIn the left sidebar of JupyterLab, click \nRuntimes\n ( \n \n).\n2\n. \nHover the cursor over the runtime configuration that you want to duplicate and click the\nDuplicate\n button ( \n \n).\nVerification\nThe runtime configuration that you duplicated is shown in the \nRuntimes\n tab ( \n \n) in the left\nsidebar of JupyterLab.\n4.3.7. Running a pipeline in JupyterLab\nYou can run pipelines that you have created in JupyterLab from the Pipeline Editor user interface.\nBefore you can run a pipeline, you must create a data science project and a pipeline server. After you\ncreate a pipeline server, you must create a workbench within the same project as your pipeline server.\nYour pipeline instance in JupyterLab must contain a runtime configuration. If you create a workbench as\npart of a data science project, a default runtime configuration is created automatically. However, if you\ncreate a notebook from the Jupyter tile in the OpenShift AI dashboard, you must create a runtime\nconfiguration before you can run your pipeline in JupyterLab. A runtime configuration defines\nconnectivity information for your pipeline instance and S3-compatible cloud storage.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n) in OpenShift.\nYou have access to S3-compatible storage.\nYou have created a pipeline in JupyterLab.\nYou have opened your pipeline in the Pipeline Editor in JupyterLab.\nYour pipeline instance contains a runtime configuration.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nProcedure\n1\n. \nIn the Pipeline Editor user interface, click \nRun Pipeline\n ( \n \n).\nThe \nRun Pipeline\n dialog appears. The \nPipeline Name\n field is automatically populated with the\npipeline file name.\nIMPORTANT\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n62",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 66
      },
      "page_content": "IMPORTANT\nYou must enter a unique pipeline name. The pipeline name that you enter must\nnot match the name of any previously executed pipelines.\n2\n. \nDefine the settings for your pipeline run.\na\n. \nFrom the \nRuntime Configuration\n list, select the relevant runtime configuration to run your\npipeline.\nb\n. \nOptional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes\nthat reference pipeline parameters, you can change the default parameter values. If a\nparameter is required and has no default value, you must enter a value.\n3\n. \nClick \nOK\n.\nVerification\nYou can view the output artifacts of your pipeline run. The artifacts are stored in your\ndesignated object storage bucket.\n4.3.8. Exporting a pipeline in JupyterLab\nYou can export pipelines that you have created in JupyterLab. When you export a pipeline, the pipeline\nis prepared for later execution, but is not uploaded or executed immediately. During the export process,\nany package dependencies are uploaded to S3-compatible storage. Also, pipeline code is generated for\nthe target runtime.\nBefore you can export a pipeline, you must create a data science project and a pipeline server. After you\ncreate a pipeline server, you must create a workbench within the same project as your pipeline server. In\naddition, your pipeline instance in JupyterLab must contain a runtime configuration. If you create a\nworkbench as part of a data science project, a default runtime configuration is created automatically.\nHowever, if you create a notebook from the Jupyter tile in the OpenShift AI dashboard, you must create\na runtime configuration before you can export your pipeline in JupyterLab. A runtime configuration\ndefines connectivity information for your pipeline instance and S3-compatible cloud storage.\nPrerequisites\nYou have installed the OpenShift Pipelines operator.\nYou have logged in to Red Hat OpenShift AI.\nIf you are using specialized OpenShift AI groups, you are part of the user group or admin group\n(for example, \nrhoai-users\n or \nrhoai-admins\n ) in OpenShift.\nYou have created a data science project that contains a workbench.\nYou have created and configured a pipeline server within the data science project that contains\nyour workbench.\nYou have access to S3-compatible storage.\nYou have a created a pipeline in JupyterLab.\nYou have opened your pipeline in the Pipeline Editor in JupyterLab.\nCHAPTER 4. WORKING WITH DATA SCIENCE PIPELINES\n63",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 67
      },
      "page_content": "Your pipeline instance contains a runtime configuration.\nYou have created and launched a Jupyter server from a notebook image that contains the Elyra\nextension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI).\nProcedure\n1\n. \nIn the Pipeline Editor user interface, click \nExport Pipeline\n ( \n \n).\nThe \nExport Pipeline\n dialog appears. The \nPipeline Name\n field is automatically populated with\nthe pipeline file name.\n2\n. \nDefine the settings to export your pipeline.\na\n. \nFrom the \nRuntime Configuration\n list, select the relevant runtime configuration to export\nyour pipeline.\nb\n. \nFrom the \nExport Pipeline as\n select an appropriate file format\nc\n. \nIn the \nExport Filename\n field, enter a file name for the exported pipeline.\nd\n. \nSelect the \nReplace if file already exists\n check box to replace an existing file of the same\nname as the pipeline you are exporting.\ne\n. \nOptional: Configure your pipeline parameters, if applicable. If your pipeline contains nodes\nthat reference pipeline parameters, you can change the default parameter values. If a\nparameter is required and has no default value, you must enter a value.\n3\n. \nClick \nOK\n.\nVerification\nYou can view the file containing the pipeline that you exported in your designated object\nstorage bucket.\n4.4. ADDITIONAL RESOURCES\nKubeflow Pipelines v1 Documentation\nWorking with pipelines in JupyterLab\n.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n64",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 68
      },
      "page_content": "CHAPTER 5. WORKING WITH ACCELERATORS\nUse accelerators, such as NVIDIA GPUs and Habana Gaudi devices, to optimize the performance of your\nend-to-end data science workflows.\n5.1. OVERVIEW OF ACCELERATORS\nIf you work with large data sets, you can use accelerators to optimize the performance of your data\nscience models in OpenShift AI. With accelerators, you can scale your work, reduce latency, and increase\nproductivity. You can use accelerators in OpenShift AI to assist your data scientists in the following\ntasks:\nNatural language processing (NLP)\nInference\nTraining deep neural networks\nData cleansing and data processing\nOpenShift AI supports the following accelerators:\nNVIDIA graphics processing units (GPUs)\nTo use compute-heavy workloads in your models, you can enable NVIDIA graphics\nprocessing units (GPUs) in OpenShift AI.\nTo enable GPUs on OpenShift, you must install the \nNVIDIA GPU Operator\n.\nHabana Gaudi devices (HPUs)\nHabana, an Intel company, provides hardware accelerators intended for deep learning\nworkloads. You can use the Habana libraries and software associated with Habana Gaudi\ndevices available from your notebook.\nBefore you can successfully enable Habana Gaudi devices on OpenShift AI, you must install\nthe necessary dependencies and version 1.10 of the HabanaAI Operator. For more\ninformation about how to enable your OpenShift environment for Habana Gaudi devices,\nsee \nHabanaAI Operator for OpenShift\n.\nYou can enable Habana Gaudi devices on-premises or with AWS DL1 compute nodes on an\nAWS instance.\nBefore you can use an accelerator in OpenShift AI, your OpenShift instance must contain an associated\naccelerator profile. For accelerators that are new to your deployment, you must configure an accelerator\nprofile for the accelerator in context. You can create an accelerator profile from the \nSettings\n \n→\nAccelerator profiles\n page on the OpenShift AI dashboard. If your deployment contains existing\naccelerators that had associated accelerator profiles already configured, an accelerator profile is\nautomatically created after you upgrade to the latest version of OpenShift AI.\nAdditional resources\nHabanaAI Operator for OpenShift\nHabana, an Intel Company\nCHAPTER 5. WORKING WITH ACCELERATORS\n65",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 69
      },
      "page_content": "Amazon EC2 DL1 Instances\nlspci(8) - Linux man page\n5.2. WORKING WITH ACCELERATOR PROFILES\nTo configure accelerators for your data scientists to use in OpenShift AI, you must create an associated\naccelerator profile. An accelerator profile is a custom resource definition (CRD) on OpenShift that has\nan AcceleratorProfile resource, and defines the specification of the accelerator. You can create and\nmanage accelerator profiles by selecting \nSettings\n \n→\n \nAccelerator profiles\n on the OpenShift AI\ndashboard.\nFor accelerators that are new to your deployment, you must manually configure an accelerator profile\nfor each accelerator. If your deployment contains an accelerator before you upgrade, the associated\naccelerator profile remains after the upgrade. You can manage the accelerators that appear to your\ndata scientists by assigning specific accelerator profiles to your custom notebook images. This example\nshows the code for a Habana Gaudi 1 accelerator profile:\nThe accelerator profile code appears on the \nInstances\n tab on the details page for the \nAcceleratorProfile\n custom resource definition (CRD). For more information about accelerator profile\nattributes, see the following table:\nTable 5.1. Accelerator profile attributes\nAttribute\nType\nRequired\nDescription\ndisplayNa\nme\nString\nRequired\nThe display name of the accelerator profile.\ndescriptio\nn\nString\nOptional\nDescriptive text defining the accelerator profile.\nidentifier\nString\nRequired\nA unique identifier defining the accelerator resource.\nenabled\nBoolean\nRequired\nDetermines if the accelerator is visible in OpenShift AI.\n---\napiVersion:\n dashboard.opendatahub.io/v1alpha\nkind:\n AcceleratorProfile\nmetadata:\n  name:\n hpu-profile-first-gen-gaudi\nspec:\n  displayName:\n Habana HPU - \n1\nst Gen Gaudi\n  description:\n First Generation Habana Gaudi device\n  enabled:\n \ntrue\n  identifier:\n habana.ai/gaudi\n  tolerations:\n    - effect:\n NoSchedule\n      key:\n habana.ai/gaudi\n      operator:\n Exists\n---\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n66",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 70
      },
      "page_content": "tolerations\nArray\nOptional\nThe tolerations that can apply to notebooks and serving runtimes\nthat use the accelerator. For more information about the\ntoleration attributes that OpenShift AI supports, see \nToleration v1\ncore\n.\nAttribute\nType\nRequired\nDescription\nAdditional resources\nToleration v1 core\nUnderstanding taints and tolerations\nManaging resources from custom resource definitions\n5.2.1. Viewing accelerator profiles\nIf you have defined accelerator profiles for OpenShift AI, you can view, enable, and disable them from\nthe \nAccelerator profiles\n page.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou are assigned the \ncluster-admin\n role in OpenShift Container Platform.\nYour deployment contains existing accelerator profiles.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n \n→\n \nAccelerator profiles\n.\nThe \nAccelerator profiles\n page appears, displaying existing accelerator profiles.\n2\n. \nInspect the list of accelerator profiles. To enable or disable an accelerator profile, on the row\ncontaining the accelerator profile, click the toggle in the \nEnable\n column.\nVerification\nThe \nAccelerator profiles\n page appears appears, displaying existing accelerator profiles.\n5.2.2. Creating an accelerator profile\nTo configure accelerators for your data scientists to use in OpenShift AI, you must create an associated\naccelerator profile.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou are assigned the \ncluster-admin\n role in OpenShift Container Platform.\nCHAPTER 5. WORKING WITH ACCELERATORS\n67",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 71
      },
      "page_content": "Procedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n \n→\n \nAccelerator profiles\n.\nThe \nAccelerator profiles\n page appears, displaying existing accelerator profiles. To enable or\ndisable an existing accelerator profile, on the row containing the relevant accelerator profile,\nclick the toggle in the \nEnable\n column.\n2\n. \nClick \nCreate accelerator profile\n.\nThe \nCreate accelerator profile\n dialog appears.\n3\n. \nIn the \nName\n field, enter a name for the accelerator profile.\n4\n. \nIn the \nIdentifier\n field, enter a unique string that identifies the hardware accelerator associated\nwith the accelerator profile.\n5\n. \nOptional: In the \nDescription\n field, enter a description for the accelerator profile.\n6\n. \nTo enable or disable the accelerator profile immediately after creation, click the toggle in the\nEnable\n column.\n7\n. \nOptional: Add a toleration to schedule pods with matching taints.\na\n. \nClick \nAdd toleration\n.\nThe \nAdd toleration\n dialog opens.\nb\n. \nFrom the \nOperator\n list, select one of the following options:\nEqual\n - The \nkey/value/effect\n parameters must match. This is the default.\nExists\n - The \nkey/effect\n parameters must match. You must leave a blank value\nparameter, which matches any.\nc\n. \nFrom the \nEffect\n list, select one of the following options:\nNone\nNoSchedule\n - New pods that do not match the taint are not scheduled onto that node.\nExisting pods on the node remain.\nPreferNoSchedule\n - New pods that do not match the taint might be scheduled onto\nthat node, but the scheduler tries not to. Existing pods on the node remain.\nNoExecute\n - New pods that do not match the taint cannot be scheduled onto that\nnode. Existing pods on the node that do not have a matching toleration are removed.\nd\n. \nIn the \nKey\n field, enter a toleration key. The key is any string, up to 253 characters. The key\nmust begin with a letter or number, and may contain letters, numbers, hyphens, dots, and\nunderscores.\ne\n. \nIn the \nValue\n field, enter a toleration value. The value is any string, up to 63 characters. The\nvalue must begin with a letter or number, and may contain letters, numbers, hyphens, dots,\nand underscores.\nf\n. \nIn the \nToleration Seconds\n section, select one of the following options to specify how long a\npod stays bound to a node that has a node condition.\nForever\n - Pods stays permanently bound to a node.\nCustom value\n - Enter a value, in seconds, to define how long pods stay bound to a\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n68",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 72
      },
      "page_content": "Custom value\n - Enter a value, in seconds, to define how long pods stay bound to a\nnode that has a node condition.\ng\n. \nClick \nAdd\n.\n8\n. \nClick \nCreate accelerator profile\n.\nVerification\nThe accelerator profile appears on the \nAccelerator profiles\n page.\nThe \nAccelerator\n list appears on the \nStart a notebook server\n page. After you select an\naccelerator, the \nNumber of accelerators\n field appears, which you can use to choose the\nnumber of accelerators for your notebook server.\nThe accelerator profile appears on the \nInstances\n tab on the details page for the \nAcceleratorProfile\n custom resource definition (CRD).\nAdditional resources\nToleration v1 core\nUnderstanding taints and tolerations\nManaging resources from custom resource definitions\n5.2.3. Updating an accelerator profile\nYou can update the existing accelerator profiles in your deployment. You might want to change\nimportant identifying information, such as the display name, the identifier, or the description.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou are assigned the \ncluster-admin\n role in OpenShift Container Platform.\nThe accelerator profile exists in your deployment.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n \n→\n \nNotebook images\n.\nThe \nNotebook images\n page appears. Previously imported notebook images are displayed. To\nenable or disable a previously imported notebook image, on the row containing the relevant\nnotebook image, click the toggle in the \nEnable\n column.\n2\n. \nClick the action menu (\n⋮\n) and select \nEdit\n from the list.\nThe \nEdit accelerator profile\n dialog opens.\n3\n. \nIn the \nName\n field, update the accelerator profile name.\n4\n. \nIn the \nIdentifier\n field, update the unique string that identifies the hardware accelerator\nassociated with the accelerator profile, if applicable.\n5\n. \nOptional: In the \nDescription\n field, update the accelerator profile.\n6\n. \nTo enable or disable the accelerator profile immediately after creation, click the toggle in the\nCHAPTER 5. WORKING WITH ACCELERATORS\n69",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 73
      },
      "page_content": "6\n. \nTo enable or disable the accelerator profile immediately after creation, click the toggle in the\nEnable\n column.\n7\n. \nOptional: Add a toleration to schedule pods with matching taints.\na\n. \nClick \nAdd toleration\n.\nThe \nAdd toleration\n dialog opens.\nb\n. \nFrom the \nOperator\n list, select one of the following options:\nEqual\n - The \nkey/value/effect\n parameters must match. This is the default.\nExists\n - The \nkey/effect\n parameters must match. You must leave a blank value\nparameter, which matches any.\nc\n. \nFrom the \nEffect\n list, select one of the following options:\nNone\nNoSchedule\n - New pods that do not match the taint are not scheduled onto that node.\nExisting pods on the node remain.\nPreferNoSchedule\n - New pods that do not match the taint might be scheduled onto\nthat node, but the scheduler tries not to. Existing pods on the node remain.\nNoExecute\n - New pods that do not match the taint cannot be scheduled onto that\nnode. Existing pods on the node that do not have a matching toleration are removed.\nd\n. \nIn the \nKey\n field, enter a toleration key. The key is any string, up to 253 characters. The key\nmust begin with a letter or number, and may contain letters, numbers, hyphens, dots, and\nunderscores.\ne\n. \nIn the \nValue\n field, enter a toleration value. The value is any string, up to 63 characters. The\nvalue must begin with a letter or number, and may contain letters, numbers, hyphens, dots,\nand underscores.\nf\n. \nIn the \nToleration Seconds\n section, select one of the following options to specify how long a\npod stays bound to a node that has a node condition.\nForever\n - Pods stays permanently bound to a node.\nCustom value\n - Enter a value, in seconds, to define how long pods stay bound to a\nnode that has a node condition.\ng\n. \nClick \nAdd\n.\n8\n. \nIf your accelerator profile contains existing tolerations, you can edit them.\na\n. \nClick the action menu (\n⋮\n) on the row containing the toleration that you want to edit and\nselect \nEdit\n from the list.\nb\n. \nComplete the applicable fields to update the details of the toleration.\nc\n. \nClick \nUpdate\n.\n9\n. \nClick \nUpdate accelerator profile\n.\nVerification\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n70",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 74
      },
      "page_content": "If your accelerator profile has new identifying information, this information appears in the\nAccelerator\n list on the \nStart a notebook server\n page.\nAdditional resources\nToleration v1 core\nUnderstanding taints and tolerations\nManaging resources from custom resource definitions\n5.2.4. Deleting an accelerator profile\nTo discard accelerator profiles that you no longer require, you can delete them so that they do not\nappear on the dashboard.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nYou are assigned the \ncluster-admin\n role in OpenShift Container Platform.\nThe accelerator profile that you want to delete exists in your deployment.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n \n→\n \nAccelerator profiles\n.\nThe \nAccelerator profiles\n page appears, displaying existing accelerator profiles.\n2\n. \nClick the action menu (\n⋮\n) beside the accelerator profile that you want to delete and click\nDelete\n.\nThe \nDelete accelerator profile\n dialog opens.\n3\n. \nEnter the name of the accelerator profile in the text field to confirm that you intend to delete it.\n4\n. \nClick \nDelete\n.\nVerification\nThe accelerator profile no longer appears on the \nAccelerator profiles\n page.\nAdditional resources\nToleration v1 core\nUnderstanding taints and tolerations\nManaging resources from custom resource definitions\n5.2.5. Configuring a recommended accelerator for notebook images\nTo help you indicate the most suitable accelerators to your data scientists, you can configure a\nrecommended tag to appear on the dashboard.\nCHAPTER 5. WORKING WITH ACCELERATORS\n71",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 75
      },
      "page_content": "Prerequisites\nYou have logged in to OpenShift Container Platform.\nYou have the \ncluster-admin\n role in OpenShift Container Platform.\nYou have existing notebook images in your deployment.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n \n→\n \nNotebook images\n.\nThe \nNotebook images\n page appears. Previously imported notebook images are displayed.\n2\n. \nClick the action menu (\n⋮\n) and select \nEdit\n from the list.\nThe \nUpdate notebook image\n dialog opens.\n3\n. \nFrom the \nAccelerator identifier\n list, select an identifier to set its accelerator as recommended\nwith the notebook image. If the notebook image contains only one accelerator identifier, the\nidentifier name displays by default.\n4\n. \nClick \nUpdate\n.\nNOTE\nIf you have already configured an accelerator identifier for a notebook image, you\ncan specify a recommended accelerator for the notebook image by creating an\nassociated accelerator profile. To do this, click \nCreate profile\n on the row\ncontaining the notebook image and complete the relevant fields. If the notebook\nimage does not contain an accelerator identifier, you must manually configure\none before creating an associated accelerator profile.\nVerification\nWhen your data scientists select an accelerator with a specific notebook image, a tag appears\nnext to the corresponding accelerator indicating its compatibility.\n5.2.6. Configuring a recommended accelerator for serving runtimes\nTo help you indicate the most suitable accelerators to your data scientists, you can configure a\nrecommended accelerator tag for your serving runtimes.\nPrerequisites\nYou have logged in to Red Hat OpenShift AI.\nIf you use specialized OpenShift AI groups, you are part of the admin group (for example, \n{oai-\nadmin-group}\n ) in OpenShift.\nProcedure\n1\n. \nFrom the OpenShift AI dashboard, click \nSettings\n > \nServing runtimes\n.\nThe \nServing runtimes\n page opens and shows the model-serving runtimes that are already\ninstalled and enabled in your OpenShift AI deployment. By default, the OpenVINO Model Server\nruntime is pre-installed and enabled in OpenShift AI.\n2\n. \nEdit your custom runtime that you want to add the recommended accelerator tag to, click the\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n72",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 76
      },
      "page_content": "2\n. \nEdit your custom runtime that you want to add the recommended accelerator tag to, click the\naction menu (\n⋮\n) and select \nEdit\n.\nA page with an embedded YAML editor opens.\nNOTE\nYou cannot directly edit the OpenVINO Model Server runtime that is included in\nOpenShift AI by default. However, you can \nclone\n this runtime and edit the cloned\nversion. You can then add the edited clone as a new, custom runtime. To do this,\nclick the action menu beside the OpenVINO Model Server and select \nDuplicate\n.\n3\n. \nIn the editor, enter the YAML code to apply the annotation \nopendatahub.io/recommended-\naccelerators\n. The excerpt in this example shows the annotation to set a recommended tag for\nan NVIDIA GPU accelerator:\n4\n. \nClick \nUpdate\n.\nVerification\nWhen your data scientists select an accelerator with a specific serving runtime, a tag appears\nnext to the corresponding accelerator indicating its compatibility.\n5.3. HABANA GAUDI INTEGRATION\nTo accelerate your high-performance deep learning (DL) models, you can integrate Habana Gaudi\ndevices in OpenShift AI. OpenShift AI also includes the HabanaAI notebook image. This notebook image\nis pre-built and ready for your data scientists to use after you install or upgrade OpenShift AI.\nBefore you can successfully enable Habana Gaudi devices in OpenShift AI, you must install the\nnecessary dependencies and install the HabanaAI Operator. This allows your data scientists to use\nHabana libraries and software associated with Habana Gaudi devices from their notebooks. For more\ninformation about how to enable your OpenShift environment for Habana Gaudi devices, see \nHabanaAI\nOperator for OpenShift\n.\nIMPORTANT\nCurrently, Habana Gaudi integration is only supported in OpenShift 4.12.\nYou can use Habana Gaudi accelerators on OpenShift AI with version 1.10.0 of the\nHabana Gaudi Operator. For information about the supported configurations for version\n1.10 of the Habana Gaudi Operator, see \nSupport Matrix v1.10.0\n.\nIn addition, the version of the HabanaAI Operator that you install must match the version\nof the HabanaAI notebook image in your deployment.\nYou can use Habana Gaudi devices in an Amazon EC2 DL1 instance on OpenShift. Therefore, your\nOpenShift platform must support EC2 DL1 instances. Habana Gaudi accelerators are available to your\ndata scientists when they create a workbench, serve a model, and create a notebook.\nTo identify the Habana Gaudi devices present in your deployment, use the \nlspci\n utility. For more\nmetadata:\n annotations:\n  opendatahub.io/recommended-accelerators: \n'[\"nvidia.com/gpu\"]'\nCHAPTER 5. WORKING WITH ACCELERATORS\n73",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 77
      },
      "page_content": "To identify the Habana Gaudi devices present in your deployment, use the \nlspci\n utility. For more\ninformation, see \nlspci(8) - Linux man page\n.\nIMPORTANT\nIf the \nlspci\n utility indicates that Habana Gaudi devices are present in your deployment, it\ndoes not necessarily mean that the devices are ready to use.\nBefore you can use your Habana Gaudi devices, you must enable them in your OpenShift\nenvironment and configure an accelerator profile for each device. For more information\nabout how to enable your OpenShift environment for Habana Gaudi devices, see\nHabanaAI Operator for OpenShift\n.\nAdditional resources\nHabanaAI Operator for OpenShift\nlspci(8) - Linux man page\nAmazon EC2 DL1 Instances\nSupport Matrix v1.10.0\nWhat version of the Kubernetes API is included with each OpenShift 4.x release?\n5.3.1. Enabling Habana Gaudi devices\nBefore you can use Habana Gaudi devices in OpenShift AI, you must install the necessary dependencies\nand deploy the HabanaAI Operator.\nPrerequisites\nYou have logged in to OpenShift Container Platform.\nYou have the \ncluster-admin\n role in OpenShift Container Platform.\nProcedure\n1\n. \nTo enable Habana Gaudi devices in OpenShift AI, follow the instructions at \nHabanaAI Operator\nfor OpenShift\n.\n2\n. \nFrom the OpenShift AI dashboard, click \nSettings\n \n→\n \nAccelerator profiles\n.\nThe \nAccelerator profiles\n page appears, displaying existing accelerator profiles. To enable or\ndisable an existing accelerator profile, on the row containing the relevant accelerator profile,\nclick the toggle in the \nEnable\n column.\n3\n. \nClick \nCreate accelerator profile\n.\nThe \nCreate accelerator profile\n dialog opens.\n4\n. \nIn the \nName\n field, enter a name for the Habana Gaudi device.\n5\n. \nIn the \nIdentifier\n field, enter a unique string that identifies the Habana Gaudi device, for\nexample, \nhabana.ai/gaudi\n.\n6\n. \nOptional: In the \nDescription\n field, enter a description for the Habana Gaudi device.\n7\n. \nTo enable or disable the accelerator profile for the Habana Gaudi device immediately after\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n74",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 78
      },
      "page_content": "7\n. \nTo enable or disable the accelerator profile for the Habana Gaudi device immediately after\ncreation, click the toggle in the \nEnable\n column.\n8\n. \nOptional: Add a toleration to schedule pods with matching taints.\na\n. \nClick \nAdd toleration\n.\nThe \nAdd toleration\n dialog opens.\nb\n. \nFrom the \nOperator\n list, select one of the following options:\nEqual\n - The \nkey/value/effect\n parameters must match. This is the default.\nExists\n - The \nkey/effect\n parameters must match. You must leave a blank value\nparameter, which matches any.\nc\n. \nFrom the \nEffect\n list, select one of the following options:\nNone\nNoSchedule\n - New pods that do not match the taint are not scheduled onto that node.\nExisting pods on the node remain.\nPreferNoSchedule\n - New pods that do not match the taint might be scheduled onto\nthat node, but the scheduler tries not to. Existing pods on the node remain.\nNoExecute\n - New pods that do not match the taint cannot be scheduled onto that\nnode. Existing pods on the node that do not have a matching toleration are removed.\nd\n. \nIn the \nKey\n field, enter the toleration key \nhabana.ai/gaudi\n. The key is any string, up to 253\ncharacters. The key must begin with a letter or number, and may contain letters, numbers,\nhyphens, dots, and underscores.\ne\n. \nIn the \nValue\n field, enter a toleration value. The value is any string, up to 63 characters. The\nvalue must begin with a letter or number, and may contain letters, numbers, hyphens, dots,\nand underscores.\nf\n. \nIn the \nToleration Seconds\n section, select one of the following options to specify how long a\npod stays bound to a node that has a node condition.\nForever\n - Pods stays permanently bound to a node.\nCustom value\n - Enter a value, in seconds, to define how long pods stay bound to a\nnode that has a node condition.\ng\n. \nClick \nAdd\n.\n9\n. \nClick \nCreate accelerator profile\n.\nVerification\nFrom the \nAdministrator\n perspective, the following Operators appear on the \nOperators\n \n→\nInstalled Operators\n page.\nHabanaAI\nNode Feature Discovery (NFD)\nKernel Module Management (KMM)\nCHAPTER 5. WORKING WITH ACCELERATORS\n75",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 79
      },
      "page_content": "The \nAccelerator\n list displays the Habana Gaudi accelerator on the \nStart a notebook server\npage. After you select an accelerator, the \nNumber of accelerators\n field appears, which you can\nuse to choose the number of accelerators for your notebook server.\nThe accelerator profile appears on the \nAccelerator profiles\n page\nThe accelerator profile appears on the \nInstances\n tab on the details page for the \nAcceleratorProfile\n custom resource definition (CRD).\nAdditional resources\nHabanaAI Operator for OpenShift\n.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n76",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 80
      },
      "page_content": "CHAPTER 6. WORKING WITH DISTRIBUTED WORKLOADS\nTo train complex machine-learning models or process data more quickly, data scientists can use the\ndistributed workloads feature to run their jobs on multiple OpenShift worker nodes in parallel. This\napproach significantly reduces the task completion time, and enables the use of larger datasets and\nmore complex models.\nIMPORTANT\nThe distributed workloads feature is currently available in Red Hat OpenShift AI 2.8 as a\nTechnology Preview feature. Technology Preview features are not supported with\nRed Hat production service level agreements (SLAs) and might not be functionally\ncomplete. Red Hat does not recommend using them in production. These features\nprovide early access to upcoming product features, enabling customers to test\nfunctionality and provide feedback during the development process.\nFor more information about the support scope of Red Hat Technology Preview features,\nsee \nTechnology Preview Features Support Scope\n.\n6.1. OVERVIEW OF DISTRIBUTED WORKLOADS\nYou can use the distributed workloads feature to queue, scale, and manage the resources required to\nrun data science workloads across multiple nodes in an OpenShift cluster simultaneously. Typically, data\nscience workloads include several types of artificial intelligence (AI) workloads, including machine\nlearning (ML) and Python workloads.\nDistributed workloads provide the following benefits:\nYou can iterate faster and experiment more frequently because of the reduced processing time.\nYou can use larger datasets, which can lead to more accurate models.\nYou can use complex models that could not be trained on a single node.\nThe distributed workloads infrastructure includes the following components:\nCodeFlare Operator\nManages the queuing of batch jobs\nCodeFlare SDK\nDefines and controls the remote distributed compute jobs and infrastructure for any Python-based\nenvironment\nKubeRay\nManages remote Ray clusters on OpenShift for running distributed compute workloads\nYou can run distributed data science workloads from data science pipelines or from notebooks.\n6.2. CONFIGURING DISTRIBUTED WORKLOADS\nTo configure the distributed workloads feature for your data scientists to use in OpenShift AI, you must\nenable several components.\nPrerequisites\nCHAPTER 6. WORKING WITH DISTRIBUTED WORKLOADS\n77",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 81
      },
      "page_content": "You have logged in to OpenShift Container Platform with the \ncluster-admin\n role.\nYou have sufficient resources. In addition to the base OpenShift AI resources, you need 1.1\nvCPU and 1.6 GB memory to deploy the distributed workloads infrastructure.\nYou have access to a Ray cluster image. For information about how to create a Ray cluster, see\nthe \nRay Clusters documentation\n.\nYou have removed any previously installed instances of the CodeFlare Operator, as described in\nthe Knowledgebase solution \nHow to migrate from a separately installed CodeFlare Operator in\nyour data science cluster\n.\nIf you want to use graphics processing units (GPUs), you have enabled GPU support in\nOpenShift AI. See \nEnabling GPU support in OpenShift AI\n.\nIf you want to use self-signed certificates, you have added them to a central Certificate\nAuthority (CA) bundle as described in \nWorking with certificates\n (for disconnected\nenvironments, see \nWorking with certificates\n). No additional configuration is necessary to use\nthose certificates with distributed workloads. The centrally configured self-signed certificates\nare automatically available in the workload pods at the following mount points:\nCluster-wide CA bundle:\nCustom CA bundle:\nProcedure\n1\n. \nIn the OpenShift Container Platform console, click \nOperators\n \n→\n \nInstalled Operators\n.\n2\n. \nSearch for the \nRed Hat OpenShift AI\n Operator, and then click the Operator name to open the\nOperator details page.\n3\n. \nClick the \nData Science Cluster\n tab.\n4\n. \nClick the default instance name to open the instance details page.\nNOTE\nStarting from Red Hat OpenShift AI 2.4, the default instance name for new\ninstallations is \ndefault-dsc\n. The default instance name for earlier installations,\nrhods\n, is preserved during upgrade.\n5\n. \nClick the \nYAML\n tab to show the instance specifications.\n6\n. \nIn the \nspec.components\n section, ensure that the \nmanagementState\n field is set correctly for\nthe required components depending on whether the distributed workload is run from a pipeline\nor notebook or both, as shown in the following table.\nTable 6.1. Components required for distributed workloads\n/etc/pki/tls/certs/odh-trusted-ca-bundle.crt\n/etc/ssl/certs/odh-trusted-ca-bundle.crt\n/etc/pki/tls/certs/odh-ca-bundle.crt\n/etc/ssl/certs/odh-ca-bundle.crt\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n78",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 82
      },
      "page_content": "Component\nPipelines only\nNotebooks only\nPipelines and\nnotebooks\ncodeflare\nManaged\nManaged\nManaged\ndashboard\nManaged\nManaged\nManaged\ndatasciencepipelines\nManaged\nRemoved\nManaged\nray\nManaged\nManaged\nManaged\nworkbenches\nRemoved\nManaged\nManaged\n7\n. \nClick \nSave\n. After a short time, the components with a \nManaged\n state are ready.\nVerification\nCheck the status of the \ncodeflare-operator-manager\n pod, as follows:\n1\n. \nIn the OpenShift Container Platform console, from the \nProject\n list, select \nredhat-ods-\napplications\n.\n2\n. \nClick \nWorkloads\n \n→\n \nDeployments\n.\n3\n. \nSearch for the \ncodeflare-operator-manager\n deployment, and click the deployment name to\nopen the deployment details page.\n4\n. \nClick the \nPods\n tab. When the status of the \ncodeflare-operator-manager-_<pod-id>_\n pod is \nRunning\n, the pod is ready to use. To see more information about the pod, click the pod name to\nopen the pod details page, and click the \nLogs\n tab.\n6.3. RUNNING DISTRIBUTED DATA SCIENCE WORKLOADS FROM\nNOTEBOOKS\nTo run a distributed data science workload from a notebook, you must first provide the link to your Ray\ncluster image.\nPrerequisites\nYou have access to a data science cluster that is configured to run distributed workloads as\ndescribed in \nConfiguring distributed workloads\n.\nYou have created a data science project that contains a workbench that is running one of the\ndefault notebook images, for example, the \nStandard Data Science\n notebook. See the table in\nNotebook images for data scientists\n for a complete list of default notebook images.\nYou have launched your notebook server and logged in to Jupyter.\nProcedure\n1\n. \nTo access the demo notebooks, clone the \ncodeflare-sdk\n repository as follows:\nCHAPTER 6. WORKING WITH DISTRIBUTED WORKLOADS\n79",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 83
      },
      "page_content": "a\n. \nIn the JupyterLab interface, click \nGit > Clone a Repository\n.\nb\n. \nIn the \"Clone a repo\" dialog, enter \nhttps://github.com/project-codeflare/codeflare-sdk.git\nand then click \nClone\n. The \ncodeflare-sdk\n repository is listed in the left navigation pane.\n2\n. \nRun a distributed workload job as shown in the following example:\na\n. \nIn the JupyterLab interface, in the left navigation pane, double-click \ncodeflare-sdk\n.\nb\n. \nDouble-click \ndemo-notebooks\n, and then double-click \nguided-demos\n.\nc\n. \nUpdate each example demo notebook as follows:\nReplace the links to the example community image with a link to your Ray cluster image.\nSet \ninstascale\n to \nFalse\n. InstaScale is not deployed in the Technology Preview version\nof the distributed workloads feature.\nd\n. \nRun the notebooks.\nVerification\nThe notebooks run to completion without errors. In the notebooks, the output from the \ncluster.status()\nfunction or \ncluster.details()\n function indicates that the Ray cluster is \nActive\n.\n6.4. RUNNING DISTRIBUTED DATA SCIENCE WORKLOADS FROM\nDATA SCIENCE PIPELINES\nTo run a distributed data science workload from a data science pipeline, you must first update the\npipeline to include a link to your Ray cluster image.\nPrerequisites\nYou have logged in to OpenShift Container Platform with the \ncluster-admin\n role.\nYou have access to a data science cluster that is configured to run distributed workloads as\ndescribed in \nConfiguring distributed workloads\n.\nYou have installed the Red Hat OpenShift Pipelines Operator, as described in \nInstalling\nOpenShift Pipelines\n.\nYou have access to S3-compatible object storage.\nYou have logged in to Red Hat OpenShift AI.\nYou have created a data science project.\nProcedure\n1\n. \nCreate a data connection to connect the object storage to your data science project, as\ndescribed in \nAdding a data connection to your data science project\n.\n2\n. \nConfigure a pipeline server to use the data connection, as described in \nConfiguring a pipeline\nserver\n.\n3\n. \nCreate the data science pipeline as follows:\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n80",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 84
      },
      "page_content": "a\n. \nInstall the \nkfp-tekton\n Python package, which is required for all pipelines:\nb\n. \nInstall any other dependencies that are required for your pipeline.\nc\n. \nBuild your data science pipeline in Python code. For example, create a file named \ncompile_example.py\n with the following content:\n$ pip install kfp-tekton\nfrom\n kfp \nimport\n components, dsl\ndef\n \nray_fn\n(openshift_server: str, openshift_token: str)\n -> int:\n \n1\n   \nimport\n ray\n   \nfrom\n codeflare_sdk.cluster.auth \nimport\n TokenAuthentication\n   \nfrom\n codeflare_sdk.cluster.cluster \nimport\n Cluster, ClusterConfiguration\n   auth = TokenAuthentication( \n2\n       token=openshift_token, server=openshift_server, skip_tls=\nTrue\n   )\n   auth_return = auth.login()\n   cluster = Cluster( \n3\n       ClusterConfiguration(\n           name=\n\"raytest\"\n,\n           \n# namespace must exist\n           namespace=\n\"pipeline-example\"\n,\n           num_workers=\n1\n,\n           head_cpus=\n\"500m\"\n,\n           min_memory=\n1\n,\n           max_memory=\n1\n,\n           num_gpus=\n0\n,\n           image=\n\"quay.io/project-codeflare/ray:latest-py39-cu118\"\n, \n4\n           instascale=\nFalse\n, \n5\n       )\n   )\n   print(cluster.status())\n   cluster.up() \n6\n   cluster.wait_ready() \n7\n   print(cluster.status())\n   print(cluster.details())\n   ray_dashboard_uri = cluster.cluster_dashboard_uri()\n   ray_cluster_uri = cluster.cluster_uri()\n   print(ray_dashboard_uri, ray_cluster_uri)\n   \n# Before proceeding, ensure that the cluster exists and that its URI contains a value\n   \nassert\n ray_cluster_uri, \n\"Ray cluster must be started and set before proceeding\"\n   ray.init(address=ray_cluster_uri)\nCHAPTER 6. WORKING WITH DISTRIBUTED WORKLOADS\n81",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 85
      },
      "page_content": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nImports from the CodeFlare SDK the packages that define the cluster functions\nAuthenticates with the cluster by using values that you specify when creating the\npipeline run\nSpecifies the Ray cluster resources: replace these example values with the values for\nyour Ray cluster\nSpecifies the location of the Ray cluster image: if using a disconnected environment,\nreplace the default value with the location for your environment\nInstaScale is not supported in this release\nCreates a Ray cluster using the specified image and configuration\nWaits for the Ray cluster to be ready before proceeding\nReplace the example details in this section with the details for your workload\nRemoves the Ray cluster when your workload is finished\nReplace the example name and description with the values for your workload\n   print(\n\"Ray cluster is up and running: \"\n, ray.is_initialized())\n   @ray.remote\n   \ndef\n \ntrain_fn\n()\n:\n \n8\n       \n# complex training function\n       \nreturn\n \n100\n   result = ray.get(train_fn.remote())\n   \nassert\n \n100\n == result\n   ray.shutdown()\n   cluster.down() \n9\n   auth.logout()\n   \nreturn\n result\n@dsl.pipeline( \n10\n   name=\n\"Ray Simple Example\"\n,\n   description=\n\"Ray Simple Example\"\n,\n)\ndef\n \nray_integration\n(openshift_server, openshift_token)\n:\n   ray_op = components.create_component_from_func(\n       ray_fn,\n       base_image=\n'registry.redhat.io/ubi8/python-39:latest'\n,\n       packages_to_install=[\n\"codeflare-sdk\"\n],\n   )\n   ray_op(openshift_server, openshift_token)\nif\n __name__ == \n'__main__'\n: \n11\n    \nfrom\n kfp_tekton.compiler \nimport\n TektonCompiler\n    TektonCompiler().compile(ray_integration, \n'compiled-example.yaml'\n)\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n82",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 86
      },
      "page_content": "11\nCompiles the Python code and saves the output in a YAML file\nd\n. \nCompile the Python file (in this example, the \ncompile_example.py\n file):\nThis command creates a YAML file (in this example, \ncompiled-example.yaml\n), which you\ncan import in the next step.\n4\n. \nImport your data science pipeline, as described in \nImporting a data science pipeline\n.\n5\n. \nSchedule the pipeline run, as described in \nScheduling a pipeline run\n.\n6\n. \nWhen the pipeline run is complete, confirm that it is included in the list of triggered pipeline runs,\nas described in \nViewing triggered pipeline runs\n.\nVerification\nThe YAML file is created and the pipeline run completes without errors. You can view the run details, as\ndescribed in \nViewing the details of a pipeline run\n.\nAdditional resources\nWorking with data science pipelines\nRay Clusters documentation\n6.5. RUNNING DISTRIBUTED DATA SCIENCE WORKLOADS IN A\nDISCONNECTED ENVIRONMENT\nTo run a distributed data science workload in a disconnected environment, you must be able to access a\nRay cluster image, and the data sets and Python dependencies used by the workload, from the\ndisconnected environment.\nPrerequisites\nYou have logged in to OpenShift Container Platform with the \ncluster-admin\n role.\nYou have access to the disconnected data science cluster.\nYou have installed Red Hat OpenShift AI and created a mirror image as described in \nInstalling\nand uninstalling OpenShift AI Self-Managed in a disconnected environment\n.\nYou can access the following software from the disconnected cluster:\nA Ray cluster image\nThe data sets and models to be used by the workload\nThe Python dependencies for the workload, either in a Ray image or in your own Python\nPackage Index (PyPI) server that is available from the disconnected cluster\nYou have logged in to Red Hat OpenShift AI.\nYou have created a data science project.\n$ python compile_example.py\nCHAPTER 6. WORKING WITH DISTRIBUTED WORKLOADS\n83",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 87
      },
      "page_content": "Procedure\n1\n. \nConfigure the disconnected data science cluster to run distributed workloads as described in\nConfiguring distributed workloads\n.\n2\n. \nIn the \nClusterConfiguration\n section of the notebook or pipeline, ensure that the \nimage\n value\nspecifies a Ray cluster image that can be accessed from the disconnected environment:\nNotebooks use the Ray cluster image to create a Ray cluster when running the notebook.\nPipelines use the Ray cluster image to create a Ray cluster during the pipeline run.\n3\n. \nIf any of the Python packages required by the workload are not available in the Ray cluster,\nconfigure the Ray cluster to download the Python packages from a private PyPI server.\nFor example, set the \nPIP_INDEX_URL\n and \nPIP_TRUSTED_HOST\n environment variables for\nthe Ray cluster, to specify the location of the Python dependencies, as shown in the following\nexample:\nPIP_INDEX_URL: https://pypi-notebook.apps.mylocation.com/simple\nPIP_TRUSTED_HOST: pypi-notebook.apps.mylocation.com\nwhere\nPIP_INDEX_URL\n specifies the base URL of your private PyPI server (the default value is\nhttps://pypi.org\n).\nPIP_TRUSTED_HOST\n configures Python to mark the specified host as trusted, regardless\nof whether that host has a valid SSL certificate or is using a secure channel.\n4\n. \nRun the distributed data science workload, as described in \nRunning distributed data science\nworkloads from notebooks\n or \nRunning distributed data science workloads from data science\npipelines\n.\nVerification\nThe notebook or pipeline run completes without errors:\nFor notebooks, the output from the \ncluster.status()\n function or \ncluster.details()\n function\nindicates that the Ray cluster is \nActive\n.\nFor pipeline runs, you can view the run details as described in \nViewing the details of a pipeline\nrun\n.\nAdditional resources\nInstalling and uninstalling Red Hat OpenShift AI in a disconnected environment\nRay Clusters documentation\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n84",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 88
      },
      "page_content": "CHAPTER 7. TROUBLESHOOTING COMMON PROBLEMS IN\nJUPYTER FOR ADMINISTRATORS\nIf your users are experiencing errors in Red Hat OpenShift AI relating to Jupyter, their notebooks, or\ntheir notebook server, read this section to understand what could be causing the problem, and how to\nresolve the problem.\nIf you cannot see the problem here or in the release notes, contact Red Hat Support.\n7.1. A USER RECEIVES A \n404: PAGE NOT FOUND\n ERROR WHEN\nLOGGING IN TO JUPYTER\nProblem\nIf you have configured specialized user groups for OpenShift AI, the user name might not be added to\nthe default user group for OpenShift AI.\nDiagnosis\nCheck whether the user is part of the default user group.\n1\n. \nFind the names of groups allowed access to Jupyter.\na\n. \nLog in to the OpenShift Container Platform web console.\nb\n. \nClick \nUser Management\n \n→\n \nGroups\n.\nc\n. \nClick the name of your user group, for example, \nrhoai-users\n.\nThe \nGroup details\n page for that group appears.\n2\n. \nClick the \nDetails\n tab for the group and confirm that the \nUsers\n section for the relevant group\ncontains the users who have permission to access Jupyter.\nResolution\nIf the user is not added to any of the groups with permission access to Jupyter, follow \nAdding\nusers\n to add them.\nIf the user is already added to a group with permission to access Jupyter, contact Red Hat\nSupport.\n7.2. A USER’S NOTEBOOK SERVER DOES NOT START\nProblem\nThe OpenShift Container Platform cluster that hosts the user’s notebook server might not have access\nto enough resources, or the Jupyter pod may have failed.\nDiagnosis\n1\n. \nLog in to the OpenShift Container Platform web console.\n2\n. \nDelete and restart the notebook server pod for this user.\na\n. \nClick \nWorkloads\n \n→\n \nPods\n and set the \nProject\n to \nrhods-notebooks\n.\nb\n. \nSearch for the notebook server pod that belongs to this user, for example, \njupyter-nb-\nCHAPTER 7. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR ADMINISTRATORS\n85",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 89
      },
      "page_content": "b\n. \nSearch for the notebook server pod that belongs to this user, for example, \njupyter-nb-\n<username>-*\n.\nIf the notebook server pod exists, an intermittent failure may have occurred in the notebook\nserver pod.\nIf the notebook server pod for the user does not exist, continue with diagnosis.\n3\n. \nCheck the resources currently available in the OpenShift Container Platform cluster against the\nresources required by the selected notebook server image.\nIf worker nodes with sufficient CPU and RAM are available for scheduling in the cluster, continue\nwith diagnosis.\n4\n. \nCheck the state of the Jupyter pod.\nResolution\nIf there was an intermittent failure of the notebook server pod:\na\n. \nDelete the notebook server pod that belongs to the user.\nb\n. \nAsk the user to start their notebook server again.\nIf the notebook server does not have sufficient resources to run the selected notebook server\nimage, either add more resources to the OpenShift Container Platform cluster, or choose a\nsmaller image size.\nIf the Jupyter pod is in a \nFAILED\n state:\na\n. \nRetrieve the logs for the \njupyter-nb-*\n pod and send them to Red Hat Support for further\nevaluation.\nb\n. \nDelete the \njupyter-nb-*\n pod.\nIf none of the previous resolutions apply, contact Red Hat Support.\n7.3. THE USER RECEIVES A \nDATABASE OR DISK IS FULL\n ERROR OR A\nNO SPACE LEFT ON DEVICE\n ERROR WHEN THEY RUN NOTEBOOK\nCELLS\nProblem\nThe user might have run out of storage space on their notebook server.\nDiagnosis\n1\n. \nLog in to Jupyter and start the notebook server that belongs to the user having problems. If the\nnotebook server does not start, follow these steps to check whether the user has run out of\nstorage space:\na\n. \nLog in to the OpenShift Container Platform web console.\nb\n. \nClick \nWorkloads\n \n→\n \nPods\n and set the \nProject\n to \nrhods-notebooks\n.\nc\n. \nClick the notebook server pod that belongs to this user, for example, \njupyter-nb-<idp>-\n<username>-*\n.\nd\n. \nClick \nLogs\n. The user has exceeded their available capacity if you see lines similar to the\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n86",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 90
      },
      "page_content": "d\n. \nClick \nLogs\n. The user has exceeded their available capacity if you see lines similar to the\nfollowing:\nUnexpected error while saving file: XXXX database or disk is full\nResolution\nIncrease the user’s available storage by expanding their persistent volume: \nExpanding persistent\nvolumes\nWork with the user to identify files that can be deleted from the \n/opt/app-root/src\n directory on\ntheir notebook server to free up their existing storage space.\nNOTE\nWhen you delete files using the JupyterLab file explorer, the files move to the hidden \n/opt/app-root/src/.local/share/Trash/files\n folder in the persistent storage for the\nnotebook. To free up storage space for notebooks, you must permanently delete these\nfiles.\nCHAPTER 7. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR ADMINISTRATORS\n87",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 91
      },
      "page_content": "CHAPTER 8. TROUBLESHOOTING COMMON PROBLEMS IN\nJUPYTER FOR USERS\nIf you are seeing errors in Red Hat OpenShift AI related to Jupyter, your notebooks, or your notebook\nserver, read this section to understand what could be causing the problem.\nIf you cannot see your problem here or in the release notes, contact Red Hat Support.\n8.1. I SEE A \n403: FORBIDDEN\n ERROR WHEN I LOG IN TO JUPYTER\nProblem\nIf your administrator has configured specialized user groups for OpenShift AI, your user name might not\nbe added to the default user group or the default administrator group for OpenShift AI.\nResolution\nContact your administrator so that they can add you to the correct group/s.\n8.2. MY NOTEBOOK SERVER DOES NOT START\nProblem\nThe OpenShift Container Platform cluster that hosts your notebook server might not have access to\nenough resources, or the Jupyter pod may have failed.\nResolution\nCheck the logs in the \nEvents\n section in OpenShift for error messages associated with the problem. For\nexample:\nServer requested\n2021-10-28T13:31:29.830991Z [Warning] 0/7 nodes are available: 2 Insufficient memory,\n2 node(s) had taint {node-role.kubernetes.io/infra: }, that the pod didn't tolerate, 3 node(s) had taint\n \n{node-role.kubernetes.io/master: },\nthat the pod didn't tolerate.\nContact your administrator with details of any relevant error messages so that they can perform further\nchecks.\n8.3. I SEE A \nDATABASE OR DISK IS FULL\n ERROR OR A \nNO SPACE LEFT\nON DEVICE\n ERROR WHEN I RUN MY NOTEBOOK CELLS\nProblem\nYou might have run out of storage space on your notebook server.\nResolution\nContact your administrator so that they can perform further checks.\nRed Hat OpenShift AI Self-Managed 2.8 Working on data science projects\n88",
      "type": "Document"
    }
  },
  {
    "lc": 1,
    "type": "constructor",
    "id": [
      "langchain",
      "schema",
      "document",
      "Document"
    ],
    "kwargs": {
      "metadata": {
        "source": "tmp_repo/source_repo/examples/notebooks/langchain/rhods-doc/red_hat_openshift_ai_self-managed-2.8-working_on_data_science_projects-en-us.pdf",
        "page": 92
      },
      "page_content": "CHAPTER 8. TROUBLESHOOTING COMMON PROBLEMS IN JUPYTER FOR USERS\n89",
      "type": "Document"
    }
  }
]